{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime \n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SQLite Database into Pandas DataFrames\n",
    "\n",
    "The following code connects to an SQLite database (`melee_player_database.db`) and converts each table within the database into a pandas DataFrame. The DataFrames will be stored in a dictionary, where each key corresponds to the table name with `_df` appended, and the values are the respective DataFrames.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Database Connection**: We use the `sqlite3` library to connect to the SQLite database file.\n",
    "2. **Retrieve Table Names**: A query retrieves all the table names in the database.\n",
    "3. **Convert Tables to DataFrames**: For each table:\n",
    "   - The table is loaded into a pandas DataFrame using `pd.read_sql()`.\n",
    "   - We check each column to see if any data is JSON-formatted (lists or dictionaries). If so, we convert these columns from strings into their corresponding Python objects using `json.loads()`.\n",
    "4. **Store DataFrames**: The DataFrames are stored in a dictionary, where the key is the table name with a `_df` suffix, and the value is the DataFrame.\n",
    "5. **Database Connection Closed**: Once all tables are loaded into DataFrames, the database connection is closed.\n",
    "\n",
    "### Example:\n",
    "If the database contains a table named `players`, the corresponding DataFrame will be stored in the dictionary with the key `players_df`, and can be accessed as:\n",
    "\n",
    "```python\n",
    "players_df = dfs['players_df']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the table names\n",
    "def get_table_names(conn):\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    return pd.read_sql(query, conn)['name'].tolist()\n",
    "\n",
    "# Function to load tables into DataFrames\n",
    "def load_tables_to_dfs(conn):\n",
    "    table_names = get_table_names(conn)\n",
    "    dataframes = {}\n",
    "    \n",
    "    for table in table_names:\n",
    "        # Load table into a DataFrame\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "        \n",
    "        # Detect and convert JSON formatted columns (if any)\n",
    "        for col in df.columns:\n",
    "            # Check if any entry in the column is a valid JSON (list or dictionary)\n",
    "            if df[col].apply(lambda x: isinstance(x, str)).all():\n",
    "                try:\n",
    "                    # Try parsing the column as JSON\n",
    "                    df[col] = df[col].apply(lambda x: json.loads(x) if pd.notnull(x) else x)\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    # If it fails, skip the column\n",
    "                    pass\n",
    "        \n",
    "        # Store the DataFrame with table name + '_df'\n",
    "        dataframes[f\"{table}_df\"] = df\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "if os.path.exists(data_path + 'dfs_dict.pkl'):\n",
    "    cell_has_run = True\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    with open(data_path + 'dfs_dict.pkl', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n",
    "# Check if the flag variable exists in the global scope so that this code does not run twice\n",
    "if 'cell_has_run' not in globals():\n",
    "    path = + data_path + \"melee_player_database.db\"\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(path)\n",
    "\n",
    "    # Convert each table into a DataFrame\n",
    "    dfs = load_tables_to_dfs(conn)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Now, you have a dictionary 'dfs' where each key is the table name with '_df' suffix and value is the corresponding DataFrame.\n",
    "    # For example, to access the DataFrame for a table called 'players':\n",
    "    # players_df = dfs['players_df']\n",
    "\n",
    "    dfs['tournament_info_df']['start'] = pd.to_datetime(dfs['tournament_info_df']['start'], unit='s')\n",
    "    dfs['tournament_info_df']['end'] = pd.to_datetime(dfs['tournament_info_df']['end'], unit='s')\n",
    "\n",
    "    \n",
    "    # Set the flag to indicate that the cell has been run\n",
    "    cell_has_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we adjust the data types of the dataframes so that they are the correct type. (This will be updated as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['sets_df']['best_of'] = dfs['sets_df']['best_of'].fillna(0).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the dictionary of DataFrames as a pickle\n",
    "# with open(data_path + 'dfs_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(dfs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we make dataframes that we will use and print the head.\n",
    "\n",
    "The integers in 'characters' count the number of games the player has played that character. (We verify this for Zain below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = dfs['players_df']\n",
    "players_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df = dfs['ranking_df']\n",
    "ranking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_seasons_df = dfs['ranking_seasons_df']\n",
    "ranking_seasons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_df = dfs['sets_df']\n",
    "print(f\"{sets_df[sets_df['game_data'].apply(lambda x: len(x) > 0)].shape[0] / sets_df.shape[0]:0.01%} percent of sets have some game data\")\n",
    "sets_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_info_df = dfs['tournament_info_df']\n",
    "tournament_info_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games played by character ##\n",
    "Count the games played by each character and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Filter out rows where 'characters' is not a dictionary\n",
    "valid_rows = players_df[players_df['characters'].apply(lambda x: isinstance(x, dict))]\n",
    "\n",
    "# Use apply and Counter to aggregate counts\n",
    "total_character_counts = Counter()\n",
    "\n",
    "# Sum up all character counts\n",
    "total_character_counts.update(valid_rows['characters'].apply(Counter).sum())\n",
    "\n",
    "# Sort by number of games played\n",
    "total_character_counts = dict(total_character_counts)\n",
    "sorted_characters = sorted(total_character_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_characters = dict([(char.split('/')[1], count) for char, count in sorted_characters])\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10,6))\n",
    "plot = sns.barplot(sorted_characters, orient='h', alpha = .8, capsize=2)\n",
    "plot.set_title('Games played by character')\n",
    "plot.set_xlabel('Games played')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate data for Zain\n",
    "We choose zain because he is in the best player in the head of the players_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zain_df = players_df.iloc[2]\n",
    "print('Zain has played in', len(zain_df['placings']), 'tournaments.')\n",
    "\n",
    "# Dataframe of tournaments that zain has played in.\n",
    "zain_tournament_keys = [tournament['key'] for tournament in zain_df['placings']]\n",
    "zain_tournament_info_df = tournament_info_df[tournament_info_df['key'].isin(zain_tournament_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zain_tournament_info_df.info())\n",
    "zain_tournament_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a dataframe containing all the sets that zain has played in and add some columns so that we can identify him more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter sets_df with Zain as a player and make a copy\n",
    "zain_sets_df = sets_df[(sets_df['p1_id'] == zain_df['player_id']) | (sets_df['p2_id'] == zain_df['player_id'])].copy()\n",
    "print('Zain has played', zain_sets_df.shape[0], 'sets.')\n",
    "\n",
    "# Assuming zain_df['player_id'] gives Zain's player ID\n",
    "zain_id = zain_df['player_id']\n",
    "\n",
    "# Add a 'zain_win' column using .loc\n",
    "zain_sets_df.loc[:, 'zain_win'] = ((zain_sets_df['p1_id'] == zain_id) & (zain_sets_df['p1_score'] > zain_sets_df['p2_score'])) | \\\n",
    "                                  ((zain_sets_df['p2_id'] == zain_id) & (zain_sets_df['p2_score'] > zain_sets_df['p1_score']))\n",
    "\n",
    "# Add an 'opponent' column using .loc\n",
    "zain_sets_df.loc[:, 'opponent'] = zain_sets_df.apply(\n",
    "    lambda row: row['p1_id'] if row['p2_id'] == zain_id else row['p2_id'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Add 'zain_score' and 'opponent_score' columns using .loc\n",
    "zain_sets_df.loc[:, 'zain_score'] = zain_sets_df.apply(\n",
    "    lambda row: row['p1_score'] if row['p1_id'] == zain_id else row['p2_score'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "zain_sets_df.loc[:, 'opponent_score'] = zain_sets_df.apply(\n",
    "    lambda row: row['p2_score'] if row['p1_id'] == zain_id else row['p1_score'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Filter the zain_sets_df for rows where 'game_data' is not an empty list\n",
    "game_data_zain_sets_df = zain_sets_df[zain_sets_df['game_data'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Display the result\n",
    "print('Zain has played', game_data_zain_sets_df.shape[0], 'sets with game_data.')\n",
    "print(game_data_zain_sets_df.iloc[1]['game_data'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check to see that zain_df['characters'] counts the number of games that zain has played that character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loser_char = [game_data_dict['loser_char'] for game_data_dict in game_data_zain_sets_df.iloc[1]['game_data']]\n",
    "winner_char = [game_data_dict['winner_char'] for game_data_dict in game_data_zain_sets_df.iloc[1]['game_data']]\n",
    "print(loser_char)\n",
    "print(winner_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure zain_id is a string to match the data in game_data\n",
    "zain_id = str(zain_id)\n",
    "\n",
    "# Initialize a dictionary to count Zain's characters\n",
    "zain_characters_count = defaultdict(int)\n",
    "\n",
    "# Loop through the 'game_data' for each set Zain played\n",
    "for game_data in game_data_zain_sets_df['game_data']:\n",
    "    for game in game_data:\n",
    "        # Check if Zain was the winner or loser and increment the count of the character he used\n",
    "        if str(game['winner_id']) == zain_id:  # Compare as strings\n",
    "            zain_characters_count[game['winner_char']] += 1\n",
    "        elif str(game['loser_id']) == zain_id:  # Compare as strings\n",
    "            zain_characters_count[game['loser_char']] += 1\n",
    "\n",
    "# Convert the defaultdict to a regular dict and display the result\n",
    "zain_characters_count = dict(zain_characters_count)\n",
    "# print(zain_characters_count)\n",
    "# print(zain_df['characters'])\n",
    "\n",
    "# Extract the characters dictionary from zain_df\n",
    "zain_characters_actual = zain_df['characters']\n",
    "\n",
    "# Find the keys that are common to both dictionaries\n",
    "common_keys = set(zain_characters_count.keys()).intersection(set(zain_characters_actual.keys()))\n",
    "\n",
    "# Compare the values for the keys that are common\n",
    "for key in common_keys:\n",
    "    if zain_characters_count[key] == zain_characters_actual[key]:\n",
    "        print(f\"{key}: Match - {zain_characters_count[key]} games\")\n",
    "    else:\n",
    "        print(f\"{key}: Mismatch - counted {zain_characters_count[key]} games, actual {zain_characters_actual[key]} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Glicko-2 Exploration ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see what happens if we only have one update to a player's glicko rating.  It can blow up. We simulate what happens when a player's first update contains 3 wins against players with glicko-2 rating i and  rd value of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glicko2 import Player\n",
    "player = Player()\n",
    "for i in range(0,3200, 200):\n",
    "    player = Player()\n",
    "    # player._tau = 1.2\n",
    "    # print(player._tau)\n",
    "    player.update_player([i, i, i],[100, 100, 100],[1, 1, 1])\n",
    "    print(f'{i} : {int(player.getRating())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highest Glicko every 6 months ##\n",
    "Here we investigate who has the highest Glicko-2 rating every 6 months of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "glicko2_df = pd.read_pickle(data_path + 'overall_players_ranking.pkl')\n",
    "melee_release_date = datetime.datetime(2001, 11, 21)\n",
    "glicko2_df['dates'] = glicko2_df['dates'].apply(lambda x: np.insert(x, 0, melee_release_date))\n",
    "glicko2_df['rating_history'] = glicko2_df['rating_history'].apply(lambda x: np.insert(x, 0, 1500))\n",
    "glicko2_df['rd_history'] = glicko2_df['rd_history'].apply(lambda x: np.insert(x, 0, 350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glicko2_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just look for the highest rating of the dataset, we get some nonsense (players we don't expect to see appear in the list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_index_of_date(dates, target_date):\n",
    "#     return np.searchsorted(dates, target_date) - 1\n",
    "\n",
    "# # filtered_glicko = glicko2_df.copy()\n",
    "\n",
    "\n",
    "# for year in range(2016, 2025):\n",
    "#     for month in [1, 6]:\n",
    "#         filtered_glicko = glicko2_df.copy()\n",
    "        \n",
    "#         target_date = datetime.datetime(year, month, 1)\n",
    "\n",
    "#         indices = filtered_glicko['dates'].apply(lambda x: get_index_of_date(x, target_date))\n",
    "        \n",
    "#         # Filter out the players that have not entered a tournament yet.\n",
    "#         # filtered_glicko = filtered_glicko[indices > 0]\n",
    "        \n",
    "#         # Extract ratings as a Series, ensuring correct data type\n",
    "#         ratings_on_date = filtered_glicko.apply(\n",
    "#             lambda row: row['rating_history'][indices[row.name]], axis=1\n",
    "#         )\n",
    "\n",
    "#         rd_on_date = filtered_glicko.apply(\n",
    "#             lambda row: row['rd_history'][indices[row.name]], axis=1\n",
    "#         )\n",
    "\n",
    "#         # Ensure ratings_on_date is a Series and sort it\n",
    "#         top_5 = ratings_on_date.sort_values(ascending=False)[:10]\n",
    "\n",
    "#         # Retrieve the player tags along with their ratings\n",
    "#         top_5_df = players_df[players_df['player_id'].isin(top_5.index)]\n",
    "#         top_5_df = top_5_df.set_index('player_id').loc[top_5.index]\n",
    "#         top_5_df['rating'] = top_5.values.astype(int)\n",
    "\n",
    "#         # Display the top 5 players sorted by rating\n",
    "#         print(f\"Date: {target_date.strftime('%Y-%m-%d')}\")\n",
    "#         print(top_5_df[['tag', 'rating']].to_string(index=False))\n",
    "#         print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that players with only one update to their rank can be very high (see above), we filter out players that have gone to fewer than 5 tournaments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_index_of_date(dates, target_date):\n",
    "#     return np.searchsorted(dates, target_date) - 1\n",
    "\n",
    "\n",
    "# for year in range(2016, 2025):\n",
    "#     for month in [1, 6]:\n",
    "#         # First filter out the players who have only a few updates (played in 10 tournaments)\n",
    "#         filtered_glicko = glicko2_df[glicko2_df['dates'].apply(len) > 9]\n",
    "        \n",
    "#         target_date = datetime.datetime(year, month, 1)\n",
    "\n",
    "#         indices = filtered_glicko['dates'].apply(lambda x: get_index_of_date(x, target_date))\n",
    "        \n",
    "#         # Filter out the players that have not entered a tournament yet.\n",
    "#         # filtered_glicko = filtered_glicko[indices > 0]\n",
    "\n",
    "#         # Extract ratings as a Series, ensuring correct data type\n",
    "#         ratings_on_date = filtered_glicko.apply(\n",
    "#             lambda row: row['rating_history'][indices[row.name]], axis=1\n",
    "#         )\n",
    "\n",
    "#         rd_on_date = filtered_glicko.apply(\n",
    "#             lambda row: row['rd_history'][indices[row.name]], axis=1\n",
    "#         )\n",
    "\n",
    "#         # Ensure ratings_on_date is a Series and sort it\n",
    "#         top_5 = ratings_on_date.sort_values(ascending=False)[:20]\n",
    "\n",
    "#         # Retrieve the player tags along with their ratings\n",
    "#         top_5_df = players_df[players_df['player_id'].isin(top_5.index)]\n",
    "#         top_5_df = top_5_df.set_index('player_id').loc[top_5.index]\n",
    "#         top_5_df['rating'] = top_5.values.astype(int)\n",
    "\n",
    "#         # Display the top 5 players sorted by rating\n",
    "#         print(f\"Date: {target_date.strftime('%Y-%m-%d')}\")\n",
    "#         print(top_5_df[['tag', 'rating']].to_string(index=False))\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Glicko-2 Prediction ##\n",
    "We see how often the player with the higher Glicko-2 rating wins. The baseline of 72.3% does not initially seem too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_set_sample = sets_df \n",
    "\n",
    "# # Merge with 'tournament_info_df' to get 'start' date\n",
    "# random_set_sample = random_set_sample.merge(\n",
    "#     tournament_info_df[['key', 'start']],\n",
    "#     left_on='tournament_key',\n",
    "#     right_on='key',\n",
    "#     how='left'\n",
    "# )\n",
    "\n",
    "# # Ensure date columns are datetime\n",
    "# random_set_sample['start'] = pd.to_datetime(random_set_sample['start'])\n",
    "\n",
    "# # Flatten 'glicko2_df'\n",
    "# glicko2_long = glicko2_df[['dates', 'rating_history']].reset_index()\n",
    "\n",
    "# glicko2_long = glicko2_long.explode(['dates', 'rating_history'])\n",
    "\n",
    "# glicko2_long.rename(columns={'dates': 'date', 'rating_history': 'rating'}, inplace=True)\n",
    "# glicko2_long['date'] = pd.to_datetime(glicko2_long['date'])\n",
    "\n",
    "# # For Player 1\n",
    "# p1_data = random_set_sample[['p1_id', 'start']].rename(columns={'p1_id': 'player_id'})\n",
    "# p1_data['key'] = p1_data.index\n",
    "\n",
    "# p1_ratings = pd.merge_asof(\n",
    "#     p1_data.sort_values('start'),\n",
    "#     glicko2_long.sort_values('date'),\n",
    "#     by='player_id',\n",
    "#     left_on='start',\n",
    "#     right_on='date',\n",
    "#     direction='backward'\n",
    "# ).set_index('key')\n",
    "\n",
    "# # For Player 2\n",
    "# p2_data = random_set_sample[['p2_id', 'start']].rename(columns={'p2_id': 'player_id'})\n",
    "# p2_data['key'] = p2_data.index\n",
    "\n",
    "# p2_ratings = pd.merge_asof(\n",
    "#     p2_data.sort_values('start'),\n",
    "#     glicko2_long.sort_values('date'),\n",
    "#     by='player_id',\n",
    "#     left_on='start',\n",
    "#     right_on='date',\n",
    "#     direction='backward'\n",
    "# ).set_index('key')\n",
    "\n",
    "# # Combine ratings\n",
    "# combined_ratings = pd.DataFrame({\n",
    "#     'p1_rating': p1_ratings['rating'],\n",
    "#     'p2_rating': p2_ratings['rating'],\n",
    "#     'winner_id': random_set_sample['winner_id'],\n",
    "#     'p1_id': random_set_sample['p1_id'],\n",
    "#     'p2_id': random_set_sample['p2_id']\n",
    "# })\n",
    "\n",
    "# # Drop missing ratings\n",
    "# combined_ratings.dropna(subset=['p1_rating', 'p2_rating'], inplace=True)\n",
    "\n",
    "# # Update number of sets\n",
    "# num_sets = combined_ratings.shape[0]\n",
    "\n",
    "# # Determine if the higher-rated player won\n",
    "# higher_p1_wins = (\n",
    "#     (combined_ratings['p1_rating'] > combined_ratings['p2_rating']) &\n",
    "#     (combined_ratings['winner_id'] == combined_ratings['p1_id'])\n",
    "# )\n",
    "# higher_p2_wins = (\n",
    "#     (combined_ratings['p2_rating'] > combined_ratings['p1_rating']) &\n",
    "#     (combined_ratings['winner_id'] == combined_ratings['p2_id'])\n",
    "# )\n",
    "# correct_predictions = (higher_p1_wins | higher_p2_wins).sum()\n",
    "\n",
    "# print(f'The higher Glicko-2 rating wins {correct_predictions / num_sets:0.1%} of the time')\n",
    "# print(f'There were {correct_predictions} correct predictions out of {num_sets} sets.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think that a single number tells the whole story. We make a bar chart that shows the accuracy of the prediction for different skill gaps in rating. We see that the pridictive accuracy of the Glicko-2 rating is barely better than a guess when the difference in rating is less than 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rating difference\n",
    "combined_ratings['rating_diff'] = abs(combined_ratings['p1_rating'] - combined_ratings['p2_rating'])\n",
    "\n",
    "# Determine if the higher-rated player won\n",
    "combined_ratings['higher_rated_won'] = (\n",
    "    ((combined_ratings['rating_diff'] > 0) & (combined_ratings['winner_id'] == combined_ratings['p1_id'])) |\n",
    "    ((combined_ratings['rating_diff'] < 0) & (combined_ratings['winner_id'] == combined_ratings['p2_id']))\n",
    ")\n",
    "\n",
    "# Bin the rating differences with bin sizes of 100\n",
    "bin_size = 100\n",
    "min_rating_diff = combined_ratings['rating_diff'].min()\n",
    "max_rating_diff = combined_ratings['rating_diff'].max()\n",
    "bins = np.arange(\n",
    "    np.floor(min_rating_diff / bin_size) * bin_size,\n",
    "    np.ceil(max_rating_diff / bin_size) * bin_size + bin_size,\n",
    "    bin_size, dtype=int\n",
    ")\n",
    "\n",
    "# Assign bins without specifying labels (default labels are intervals)\n",
    "combined_ratings['rating_diff_bin'] = pd.cut(combined_ratings['rating_diff'], bins)\n",
    "\n",
    "# Group by bins and compute the percentage of higher-rated player wins\n",
    "result = combined_ratings.groupby('rating_diff_bin', observed=False)['higher_rated_won'].agg(['mean', 'count'])\n",
    "result['mean'] = result['mean'] * 100  # Convert to percentage\n",
    "\n",
    "# Reset index to turn 'rating_diff_bin' into a column\n",
    "result = result.reset_index()\n",
    "\n",
    "# Convert interval labels to strings for x-axis labels\n",
    "result['rating_diff_bin_str'] = result['rating_diff_bin'].astype(str)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = result['mean'].plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "# Set x-axis labels to be the interval strings\n",
    "ax.set_xticklabels(result['rating_diff_bin_str'])\n",
    "\n",
    "plt.title('Percentage of Times the Higher Glicko-2 Rated Player Wins vs. Rating Difference')\n",
    "plt.xlabel('Rating Difference Interval')\n",
    "plt.ylabel('Percentage of Wins by Higher-Rated Player (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add percentage labels on top of each bar\n",
    "for p, value in zip(ax.patches, result['mean']):\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{value:.1f}%', \n",
    "                (p.get_x() + p.get_width() / 2, height), \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check to see how much of a problem this is, we see how many games are played between players of different rating gaps. As we see, most games are played by players whose rating are close. This will likely be a problem for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rating difference\n",
    "combined_ratings['rating_diff'] = abs(combined_ratings['p1_rating'] - combined_ratings['p2_rating'])\n",
    "\n",
    "# Bin the rating differences with bin sizes of 100\n",
    "bin_size = 100\n",
    "min_rating_diff = combined_ratings['rating_diff'].min()\n",
    "max_rating_diff = combined_ratings['rating_diff'].max()\n",
    "bins = np.arange(\n",
    "    np.floor(min_rating_diff / bin_size) * bin_size,\n",
    "    np.ceil(max_rating_diff / bin_size) * bin_size + bin_size,\n",
    "    bin_size, dtype=int\n",
    ")\n",
    "\n",
    "# Assign bins without specifying labels (default labels are intervals)\n",
    "combined_ratings['rating_diff_bin'] = pd.cut(combined_ratings['rating_diff'], bins)\n",
    "\n",
    "# Group by bins and compute the count of games played\n",
    "result = combined_ratings.groupby('rating_diff_bin', observed=False).size().reset_index(name='count')\n",
    "\n",
    "# Convert interval labels to strings for x-axis labels\n",
    "result['rating_diff_bin_str'] = result['rating_diff_bin'].astype(str)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = result['count'].plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "# Set x-axis labels to be the interval strings\n",
    "ax.set_xticklabels(result['rating_diff_bin_str'])\n",
    "\n",
    "plt.title('Number of Games Played vs. Rating Difference')\n",
    "plt.xlabel('Rating Difference Interval')\n",
    "plt.ylabel('Number of Games Played')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "for p, value in zip(ax.patches, result['count']):\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{value}', \n",
    "                (p.get_x() + p.get_width() / 2, height), \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute rating difference\n",
    "combined_ratings['rating_diff'] = abs(combined_ratings['p1_rating'] - combined_ratings['p2_rating'])\n",
    "\n",
    "# Bin the rating differences with bin sizes of 100\n",
    "bin_size = 100\n",
    "min_rating_diff = combined_ratings['rating_diff'].min()\n",
    "max_rating_diff = min(1200, combined_ratings['rating_diff'].max())  # Limit to 1200\n",
    "\n",
    "bins = np.arange(\n",
    "    np.floor(min_rating_diff / bin_size) * bin_size,\n",
    "    np.ceil(max_rating_diff / bin_size) * bin_size + bin_size,\n",
    "    bin_size, dtype=int\n",
    ")\n",
    "\n",
    "# Assign bins without labels\n",
    "combined_ratings['rating_diff_bin'] = pd.cut(combined_ratings['rating_diff'], bins)\n",
    "\n",
    "# Filter out intervals beyond 1200\n",
    "filtered_data = combined_ratings[combined_ratings['rating_diff'] <= 1200]\n",
    "\n",
    "# Determine if the higher-rated player won, using .loc to avoid SettingWithCopyWarning\n",
    "filtered_data = filtered_data.copy()\n",
    "filtered_data.loc[:, 'higher_rated_won'] = (\n",
    "    ((filtered_data['p1_rating'] > filtered_data['p2_rating']) & (filtered_data['winner_id'] == filtered_data['p1_id'])) |\n",
    "    ((filtered_data['p1_rating'] < filtered_data['p2_rating']) & (filtered_data['winner_id'] == filtered_data['p2_id']))\n",
    ")\n",
    "\n",
    "# Group by bins and compute count and win stats\n",
    "game_counts = filtered_data.groupby('rating_diff_bin', observed=False).size().reset_index(name='count')\n",
    "win_stats = filtered_data.groupby('rating_diff_bin', observed=False)['higher_rated_won'].agg(['mean', 'count'])\n",
    "win_stats['mean'] = win_stats['mean'] * 100  # Convert to percentage\n",
    "\n",
    "# Merge both dataframes\n",
    "result = pd.merge(game_counts, win_stats, on='rating_diff_bin')\n",
    "\n",
    "# Convert interval labels to strings for x-axis\n",
    "result['rating_diff_bin_str'] = result['rating_diff_bin'].astype(str)\n",
    "\n",
    "# Plot the data\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# X-axis positions for bars\n",
    "bar_width = 0.35  # Width of each bar\n",
    "x = np.arange(len(result))\n",
    "\n",
    "# Plot the number of games\n",
    "bars1 = ax1.bar(x - bar_width / 2, result['count_x'], bar_width, \n",
    "                label='Number of Games', color='skyblue', edgecolor='black')\n",
    "ax1.set_xlabel('Rating Difference Interval')\n",
    "ax1.set_ylabel('Number of Sets Played', color='skyblue')\n",
    "ax1.tick_params(axis='y', labelcolor='skyblue')\n",
    "\n",
    "# Create a twin y-axis for the win percentage\n",
    "ax2 = ax1.twinx()\n",
    "bars2 = ax2.bar(x + bar_width / 2, result['mean'], bar_width, \n",
    "                label='Win Percentage (Higher Rated)', color='lightgreen', edgecolor='black')\n",
    "ax2.set_ylabel('Win Percentage (%)', color='lightgreen')\n",
    "ax2.tick_params(axis='y', labelcolor='lightgreen')\n",
    "\n",
    "# Set x-axis labels and ticks\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(result['rating_diff_bin_str'], rotation=45, ha='right')\n",
    "\n",
    "# Add labels for win percentage bars\n",
    "for bar, value in zip(bars2, result['mean']):\n",
    "    height = bar.get_height()\n",
    "    ax2.annotate(f'{round(value)}%', \n",
    "                 (bar.get_x() + bar.get_width() / 2, height), \n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Add a title and ensure layout is tight\n",
    "plt.title('Comparison of Number of Sets Played and Win Percentage vs. Rating Difference')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import weekly updated Glicko-2 rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_ratings_df = pd.read_pickle(data_path + 'overall_players_ranking_new_weekly.pkl')\n",
    "print(player_ratings_df.shape)\n",
    "player_ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Glicko-2 updates\n",
    "Running total of number of updates to each players glicko-2 rating. We use numba njit and prange to speed up the loops in the function. We save the results so that we only need to run the calculation once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def previous_updates(array):\n",
    "    \"\"\" This funcion returns an array like array with the number of times the value above i,j entry of array has changed.\n",
    "    Args:\n",
    "        array (np): the array\n",
    "\n",
    "    Returns:\n",
    "        np: the number of times array has changed above the i,j entry\n",
    "    \"\"\"\n",
    "    previous_updates = np.zeros_like(array, dtype=np.int32)\n",
    "    \n",
    "    for i in range(1, array.shape[0]): # row i\n",
    "        for j in prange(array.shape[1]): # col j\n",
    "            col_above_i_j = array[:i,j]\n",
    "            values  = np.unique(col_above_i_j)\n",
    "            previous_updates[i,j] += int(values.shape[0] - 1)\n",
    "            # print(f\"row {i}, col {j}, {col_above_i_j}, num_updates {values.shape[0]-1}\")\n",
    "\n",
    "    return previous_updates\n",
    "\n",
    "## Testing array\n",
    "# array = np.array([\n",
    "#     [1, 1, 1],\n",
    "#     [1, 1, 2],\n",
    "#     [1, 2, 3],\n",
    "#     [1, 3, 4]])\n",
    "\n",
    "# print(array)\n",
    "# previous_updates(array)\n",
    "# print(num_previous_updates(array))\n",
    "\n",
    "# Do the calculation once.\n",
    "# player_ratings_np = player_ratings_df.to_numpy()\n",
    "# number_of_rating_updates_df = pd.DataFrame(columns=player_ratings_df.columns, index=player_ratings_df.index, data=previous_updates(player_ratings_np))\n",
    "# number_of_rating_updates_df.head()\n",
    "\n",
    "# # Save the results\n",
    "# number_of_rating_updates_df.to_pickle(data_path + 'number_of_rating_updates_df.pkl')\n",
    "\n",
    "## Load the results\n",
    "number_of_rating_updates_df = pd.read_pickle(data_path + 'number_of_rating_updates_df.pkl')\n",
    "number_of_rating_updates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add some columns to sets_df\n",
    "We add the start of the tournament, the player ratings at the start of the tournament, and the number of times the player's rating has been updated before the start of the tournament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a merge on 'key' and 'tournament_key' to bring 'start' dates into sets_df\n",
    "merged_df = sets_df.merge(tournament_info_df[['key', 'start']], left_on='tournament_key', right_on='key', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "# Function to get both Player 1 and Player 2 ratings and the number of rating updates\n",
    "def get_ratings_and_updates(row, player_ratings_df, number_of_rating_updates_df):\n",
    "    # Find the closest date in player_ratings_df that is <= 'start' date\n",
    "    closest_date = player_ratings_df.index[player_ratings_df.index <= row['start']].max()\n",
    "    \n",
    "    # If there's no valid date, return None for ratings and updates\n",
    "    if pd.isnull(closest_date):\n",
    "        return pd.Series([None, None, None, None], index=['p1_rating', 'p2_rating', 'p1_updates', 'p2_updates'])\n",
    "    \n",
    "    # Fetch Player 1's and Player 2's ratings on the closest date\n",
    "    p1_rating = player_ratings_df.loc[closest_date, row['p1_id']] if row['p1_id'] in player_ratings_df.columns else None\n",
    "    p2_rating = player_ratings_df.loc[closest_date, row['p2_id']] if row['p2_id'] in player_ratings_df.columns else None\n",
    "    \n",
    "    # Fetch Player 1's and Player 2's number of rating updates on the closest date\n",
    "    p1_updates = number_of_rating_updates_df.loc[closest_date, row['p1_id']] if row['p1_id'] in number_of_rating_updates_df.columns else None\n",
    "    p2_updates = number_of_rating_updates_df.loc[closest_date, row['p2_id']] if row['p2_id'] in number_of_rating_updates_df.columns else None\n",
    "    \n",
    "    # Return all values as a pandas Series\n",
    "    return pd.Series([p1_rating, p2_rating, p1_updates, p2_updates], \n",
    "                     index=['p1_rating', 'p2_rating', 'p1_updates', 'p2_updates'])\n",
    "\n",
    "## Apply the function to each row in merged_df\n",
    "# merged_df[['p1_rating', 'p2_rating', 'p1_updates', 'p2_updates']] = merged_df.progress_apply(\n",
    "#     get_ratings_and_updates, axis=1, \n",
    "#     player_ratings_df=player_ratings_df, \n",
    "#     number_of_rating_updates_df=number_of_rating_updates_df\n",
    "# )\n",
    "\n",
    "## Save\n",
    "# merged_df.to_pickle(data_path + 'augmented_sets_df.pkl')\n",
    "\n",
    "## Load\n",
    "augmented_sets_df = pd.read_pickle(data_path + 'augmented_sets_df.pkl')\n",
    "augmented_sets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 8 Locations\n",
    "Here we look for what the sets corresponding to the top 8 of a tournament are labeled as in the column 'location_names'. We do this by inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sets_df['location_names'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all top 8 sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vast majority of the top 8 games have these \"location_names\"\n",
    "top_8_locations = [\n",
    "        ['WSF', 'Winners Semis', 'Winners Semi-Final'],\n",
    "        ['WF', 'Winners Final', 'Winners Final'],\n",
    "        ['LSF', 'Losers Semis', 'Losers Semi-Final'],\n",
    "        ['LF', 'Losers Final', 'Losers Final'],\n",
    "        ['GF', 'Grand Final', 'Grand Final'],\n",
    "        ['GFR', 'GF Reset', 'Grand Final Reset']\n",
    "    ] \n",
    "\n",
    "top_8_sets_df = augmented_sets_df[augmented_sets_df[\"location_names\"].isin(top_8_locations)]\n",
    "top_8_tournament_keys = top_8_sets_df['tournament_key'].unique()\n",
    "print(f\"There are {len(top_8_tournament_keys)} tournaments with double elimination finals.\")\n",
    "top_8_sets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_sets_with_top_8_df = augmented_sets_df[augmented_sets_df['tournament_key'].isin(top_8_tournament_keys)]\n",
    "non_top_8_sets_df = tournament_sets_with_top_8_df[~ tournament_sets_with_top_8_df['location_names'].isin(top_8_locations)]\n",
    "non_top_8_sets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the top_8_locations to a single list of all location name variations\n",
    "top_8_flat_list = [location for sublist in top_8_locations for location in sublist]\n",
    "\n",
    "# Add a 'top_8' column based on whether 'location_names' matches any entry in the top_8_flat_list\n",
    "tournament_sets_with_top_8_df['top_8'] = tournament_sets_with_top_8_df['location_names'].apply(\n",
    "    lambda locations: any(location in top_8_flat_list for location in locations)\n",
    ")\n",
    "\n",
    "tournament_sets_with_top_8_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tournament_sets_with_top_8_df.groupby('top_8')['p1_rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_sets_with_top_8_df.groupby('top_8')['p2_rating'].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_sets_with_top_8_df.groupby('top_8')['p1_updates'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_sets_with_top_8_df.groupby('top_8')['p2_updates'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_sets_with_top_8_df.loc[:, 'rating_difference'] = np.abs(tournament_sets_with_top_8_df['p1_rating'] - tournament_sets_with_top_8_df['p2_rating'])\n",
    "tournament_sets_with_top_8_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=tournament_sets_with_top_8_df[tournament_sets_with_top_8_df['rating_difference'] > 0], x='rating_difference', col='top_8', kind='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a deep copy of the dataframe to avoid the warning\n",
    "tournament_sets_with_top_8_df = tournament_sets_with_top_8_df.copy()\n",
    "tournament_sets_with_top_8_df = tournament_sets_with_top_8_df[tournament_sets_with_top_8_df['rating_difference'] > 0]\n",
    "\n",
    "# Now safely create the 'higher_rated_won' column\n",
    "tournament_sets_with_top_8_df['higher_rated_won'] = (\n",
    "    ((tournament_sets_with_top_8_df['winner_id'] == tournament_sets_with_top_8_df['p1_id']) & \n",
    "     (tournament_sets_with_top_8_df['p1_rating'] > tournament_sets_with_top_8_df['p2_rating'])) |\n",
    "    ((tournament_sets_with_top_8_df['winner_id'] == tournament_sets_with_top_8_df['p2_id']) & \n",
    "     (tournament_sets_with_top_8_df['p2_rating'] > tournament_sets_with_top_8_df['p1_rating']))\n",
    ")\n",
    "print(tournament_sets_with_top_8_df.shape)\n",
    "# tournament_sets_with_top_8_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_8_df = tournament_sets_with_top_8_df[tournament_sets_with_top_8_df['top_8'] == True]\n",
    "print(f\"Top 8 rating baseline: {top_8_df['higher_rated_won'].sum() / top_8_df.shape[0]:.0%}\")\n",
    "non_top_8_df = tournament_sets_with_top_8_df[tournament_sets_with_top_8_df['top_8'] == False]\n",
    "print(f\"Non top 8 rating baseline: {non_top_8_df['higher_rated_won'].sum() / non_top_8_df.shape[0]:.0%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now safely create the 'higher_rated_won' column\n",
    "tournament_sets_with_top_8_df['more_updates_won'] = (\n",
    "    ((tournament_sets_with_top_8_df['winner_id'] == tournament_sets_with_top_8_df['p1_id']) & \n",
    "     (tournament_sets_with_top_8_df['p1_updates'] > tournament_sets_with_top_8_df['p2_updates'])) |\n",
    "    ((tournament_sets_with_top_8_df['winner_id'] == tournament_sets_with_top_8_df['p2_id']) & \n",
    "     (tournament_sets_with_top_8_df['p2_updates'] > tournament_sets_with_top_8_df['p1_updates']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_8_df = tournament_sets_with_top_8_df[tournament_sets_with_top_8_df['top_8'] == True]\n",
    "print(f\"Top 8 updates baseline: {top_8_df['more_updates_won'].sum() / top_8_df.shape[0]:.0%}\")\n",
    "non_top_8_df = tournament_sets_with_top_8_df[tournament_sets_with_top_8_df['top_8'] == False]\n",
    "print(f\"Non top 8 updates baseline: {non_top_8_df['more_updates_won'].sum() / non_top_8_df.shape[0]:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_of_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.minimum(top_8_df['p1_updates'], top_8_df['p2_updates']) >= updates) & (np.minimum(top_8_df['p2_updates'], top_8_df['p1_updates']) < updates + 5)\n",
    "    masked_top_8 = top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,500, 50)):\n",
    "        difference_mask = (masked_top_8['rating_difference'] >= difference) & (masked_top_8['rating_difference'] < difference + 50)\n",
    "        accuracy_of_top_8[j,i] = int(masked_top_8[difference_mask]['higher_rated_won'].sum() / masked_top_8[difference_mask].shape[0] * 100)\n",
    "\n",
    "accuracy_of_non_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.minimum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) >= updates) & (np.minimum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_non_top_8 = non_top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,500, 50)):\n",
    "        difference_mask = (masked_non_top_8['rating_difference'] >= difference) & (masked_non_top_8['rating_difference'] < difference + 50)\n",
    "        accuracy_of_non_top_8[j,i] = int(masked_non_top_8[difference_mask]['higher_rated_won'].sum() / masked_non_top_8[difference_mask].shape[0] * 100)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Heatmap for top 8\n",
    "cax1 = axs[0].imshow(accuracy_of_top_8, cmap='Blues', aspect='auto')\n",
    "axs[0].set_title('Accuracy of Top 8')\n",
    "\n",
    "# Annotating values in the cells for top 8\n",
    "for i in range(accuracy_of_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_top_8.shape[1]):\n",
    "        axs[0].text(j, i, f'{accuracy_of_top_8[i, j]}%', ha='center', va='center', color='black')\n",
    "\n",
    "# Heatmap for non-top 8\n",
    "cax2 = axs[1].imshow(accuracy_of_non_top_8, cmap='Blues', aspect='auto')\n",
    "axs[1].set_title('Accuracy of Non-Top 8')\n",
    "\n",
    "# Annotating values in the cells for non-top 8\n",
    "for i in range(accuracy_of_non_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_non_top_8.shape[1]):\n",
    "        axs[1].text(j, i, f'{accuracy_of_non_top_8[i, j]}%', ha='center', va='center', color='black')\n",
    "\n",
    "# Set labels and ticks for both heatmaps\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Min Updates to Rating')  # This is correct for the y-axis\n",
    "    ax.set_ylabel('Rating Difference')  # This should be the x-axis label\n",
    "    ax.set_xticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_yticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_xticklabels(np.arange(0, 50, 5))  # Matches your `updates` range\n",
    "    ax.set_yticklabels(np.arange(0, 500, 50))  # Matches your `rating_difference` range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_of_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.minimum(top_8_df['p1_updates'], top_8_df['p2_updates']) >= updates) & (np.minimum(top_8_df['p1_updates'], top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_top_8 = top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,100, 10)):\n",
    "        difference_mask = (masked_top_8['rating_difference'] >= difference) & (masked_top_8['rating_difference'] < difference + 10)\n",
    "        accuracy_of_top_8[j,i] = int(masked_top_8[difference_mask]['higher_rated_won'].sum() / masked_top_8[difference_mask].shape[0] * 100)\n",
    "\n",
    "accuracy_of_non_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.minimum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) >= updates) & (np.minimum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_non_top_8 = non_top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,100, 10)):\n",
    "        difference_mask = (masked_non_top_8['rating_difference'] >= difference) & (masked_non_top_8['rating_difference'] < difference + 10)\n",
    "        accuracy_of_non_top_8[j,i] = int(masked_non_top_8[difference_mask]['higher_rated_won'].sum() / masked_non_top_8[difference_mask].shape[0] * 100)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Heatmap for top 8\n",
    "cax1 = axs[0].imshow(accuracy_of_top_8, cmap='Blues', aspect='auto')\n",
    "axs[0].set_title('Accuracy of Top 8')\n",
    "\n",
    "# Annotating values in the cells for top 8\n",
    "for i in range(accuracy_of_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_top_8.shape[1]):\n",
    "        axs[0].text(j, i, f'{accuracy_of_top_8[i, j]}%', ha='center', va='center', color='black')\n",
    "\n",
    "# Heatmap for non-top 8\n",
    "cax2 = axs[1].imshow(accuracy_of_non_top_8, cmap='Blues', aspect='auto')\n",
    "axs[1].set_title('Accuracy of Non-Top 8')\n",
    "\n",
    "# Annotating values in the cells for non-top 8\n",
    "for i in range(accuracy_of_non_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_non_top_8.shape[1]):\n",
    "        axs[1].text(j, i, f'{accuracy_of_non_top_8[i, j]}%', ha='center', va='center', color='black')\n",
    "\n",
    "# Set labels and ticks for both heatmaps\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Min Updates to Rating')  # This is correct for the y-axis\n",
    "    ax.set_ylabel('Rating Difference')  # This should be the x-axis label\n",
    "    ax.set_xticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_yticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_xticklabels(np.arange(0, 50, 5))  # Matches your `updates` range\n",
    "    ax.set_yticklabels(np.arange(0, 100, 10))  # Matches your `rating_difference` range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_of_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.maximum(top_8_df['p1_updates'], top_8_df['p2_updates']) >= updates) & (np.maximum(top_8_df['p1_updates'], top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_top_8 = top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,100, 10)):\n",
    "        difference_mask = (masked_top_8['rating_difference'] >= difference) & (masked_top_8['rating_difference'] < difference + 10)\n",
    "        accuracy_of_top_8[j,i] = int(masked_top_8[difference_mask]['higher_rated_won'].sum() / masked_top_8[difference_mask].shape[0] * 100)\n",
    "\n",
    "accuracy_of_non_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.maximum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) >= updates) & (np.maximum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_non_top_8 = non_top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,100, 10)):\n",
    "        difference_mask = (masked_non_top_8['rating_difference'] >= difference) & (masked_non_top_8['rating_difference'] < difference + 10)\n",
    "        accuracy_of_non_top_8[j,i] = int(masked_non_top_8[difference_mask]['higher_rated_won'].sum() / masked_non_top_8[difference_mask].shape[0] * 100)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Heatmap for top 8\n",
    "cax1 = axs[0].imshow(accuracy_of_top_8, cmap='Blues', aspect='auto')\n",
    "axs[0].set_title('Accuracy of Top 8')\n",
    "\n",
    "# Annotating values in the cells for top 8\n",
    "for i in range(accuracy_of_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_top_8.shape[1]):\n",
    "        axs[0].text(j, i, f'{accuracy_of_top_8[i, j]}%', ha='center', va='center', color='black')\n",
    "\n",
    "# Heatmap for non-top 8\n",
    "cax2 = axs[1].imshow(accuracy_of_non_top_8, cmap='Blues', aspect='auto')\n",
    "axs[1].set_title('Accuracy of Non-Top 8')\n",
    "\n",
    "# Annotating values in the cells for non-top 8\n",
    "for i in range(accuracy_of_non_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_non_top_8.shape[1]):\n",
    "        axs[1].text(j, i, f'{accuracy_of_non_top_8[i, j]}%', ha='center', va='center', color='black')\n",
    "\n",
    "# Set labels and ticks for both heatmaps\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Max Updates to Rating')  # This is correct for the y-axis\n",
    "    ax.set_ylabel('Rating Difference')  # This should be the x-axis label\n",
    "    ax.set_xticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_yticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_xticklabels(np.arange(0, 50, 5))  # Matches your `updates` range\n",
    "    ax.set_yticklabels(np.arange(0, 100, 10))  # Matches your `rating_difference` range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_of_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.minimum(top_8_df['p1_updates'], top_8_df['p2_updates']) >= updates) & (np.minimum(top_8_df['p1_updates'], top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_top_8 = top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,500, 50)):\n",
    "        difference_mask = (masked_top_8['rating_difference'] >= difference) & (masked_top_8['rating_difference'] < difference + 50)\n",
    "        accuracy_of_top_8[j,i] = int(masked_top_8[difference_mask].shape[0])\n",
    "\n",
    "accuracy_of_non_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.minimum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) >= updates) & (np.minimum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_non_top_8 = non_top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,500, 50)):\n",
    "        difference_mask = (masked_non_top_8['rating_difference'] >= difference) & (masked_non_top_8['rating_difference'] < difference + 50)\n",
    "        accuracy_of_non_top_8[j,i] = int(masked_non_top_8[difference_mask].shape[0])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Heatmap for top 8\n",
    "cax1 = axs[0].imshow(accuracy_of_top_8, cmap='Blues', aspect='auto')\n",
    "axs[0].set_title('Set Count of Top 8')\n",
    "\n",
    "# Annotating values in the cells for top 8\n",
    "for i in range(accuracy_of_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_top_8.shape[1]):\n",
    "        axs[0].text(j, i, f'{accuracy_of_top_8[i, j]}', ha='center', va='center', color='black')\n",
    "\n",
    "# Heatmap for non-top 8\n",
    "cax2 = axs[1].imshow(accuracy_of_non_top_8, cmap='Blues', aspect='auto')\n",
    "axs[1].set_title('Set Count of Non-Top 8')\n",
    "\n",
    "# Annotating values in the cells for non-top 8\n",
    "for i in range(accuracy_of_non_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_non_top_8.shape[1]):\n",
    "        axs[1].text(j, i, f'{accuracy_of_non_top_8[i, j]}', ha='center', va='center', color='black')\n",
    "\n",
    "# Set labels and ticks for both heatmaps\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Min Updates to Rating')  # This is correct for the y-axis\n",
    "    ax.set_ylabel('Rating Difference')  # This should be the x-axis label\n",
    "    ax.set_xticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_yticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_xticklabels(np.arange(0, 50, 5))  # Matches your `updates` range\n",
    "    ax.set_yticklabels(np.arange(0, 500, 50))  # Matches your `rating_difference` range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_of_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.maximum(top_8_df['p1_updates'], top_8_df['p2_updates']) >= updates) & (np.maximum(top_8_df['p1_updates'], top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_top_8 = top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,500, 50)):\n",
    "        difference_mask = (masked_top_8['rating_difference'] >= difference) & (masked_top_8['rating_difference'] < difference + 50)\n",
    "        accuracy_of_top_8[j,i] = int(masked_top_8[difference_mask].shape[0])\n",
    "\n",
    "accuracy_of_non_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.maximum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) >= updates) & (np.maximum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_non_top_8 = non_top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,500, 50)):\n",
    "        difference_mask = (masked_non_top_8['rating_difference'] >= difference) & (masked_non_top_8['rating_difference'] < difference + 50)\n",
    "        accuracy_of_non_top_8[j,i] = int(masked_non_top_8[difference_mask].shape[0])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Heatmap for top 8\n",
    "cax1 = axs[0].imshow(accuracy_of_top_8, cmap='Blues', aspect='auto')\n",
    "axs[0].set_title('Set Count of Top 8')\n",
    "\n",
    "# Annotating values in the cells for top 8\n",
    "for i in range(accuracy_of_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_top_8.shape[1]):\n",
    "        axs[0].text(j, i, f'{accuracy_of_top_8[i, j]}', ha='center', va='center', color='black')\n",
    "\n",
    "# Heatmap for non-top 8\n",
    "cax2 = axs[1].imshow(accuracy_of_non_top_8, cmap='Blues', aspect='auto')\n",
    "axs[1].set_title('Set Count of Non-Top 8')\n",
    "\n",
    "# Annotating values in the cells for non-top 8\n",
    "for i in range(accuracy_of_non_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_non_top_8.shape[1]):\n",
    "        axs[1].text(j, i, f'{accuracy_of_non_top_8[i, j]}', ha='center', va='center', color='black')\n",
    "\n",
    "# Set labels and ticks for both heatmaps\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Max Updates to Rating')  # This is correct for the y-axis\n",
    "    ax.set_ylabel('Rating Difference')  # This should be the x-axis label\n",
    "    ax.set_xticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_yticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_xticklabels(np.arange(0, 50, 5))  # Matches your `updates` range\n",
    "    ax.set_yticklabels(np.arange(0, 500, 50))  # Matches your `rating_difference` range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_of_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.maximum(top_8_df['p1_updates'], top_8_df['p2_updates']) >= updates) & (np.maximum(top_8_df['p1_updates'], top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_top_8 = top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,100, 10)):\n",
    "        difference_mask = (masked_top_8['rating_difference'] >= difference) & (masked_top_8['rating_difference'] < difference + 10)\n",
    "        accuracy_of_top_8[j,i] = int(masked_top_8[difference_mask].shape[0])\n",
    "\n",
    "accuracy_of_non_top_8 = np.zeros((10,10), dtype=np.int32)\n",
    "\n",
    "for i, updates in enumerate(range(0, 50, 5)):\n",
    "    updates_mask = (np.maximum(non_top_8_df['p2_updates'], non_top_8_df['p2_updates']) >= updates) & (np.maximum(non_top_8_df['p1_updates'], non_top_8_df['p2_updates']) < updates + 5)\n",
    "    masked_non_top_8 = non_top_8_df[updates_mask]\n",
    "    for j, difference in enumerate(range(0,100, 10)):\n",
    "        difference_mask = (masked_non_top_8['rating_difference'] >= difference) & (masked_non_top_8['rating_difference'] < difference + 10)\n",
    "        accuracy_of_non_top_8[j,i] = int(masked_non_top_8[difference_mask].shape[0])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Heatmap for top 8\n",
    "cax1 = axs[0].imshow(accuracy_of_top_8, cmap='Blues', aspect='auto')\n",
    "axs[0].set_title('Set Count of Top 8')\n",
    "\n",
    "# Annotating values in the cells for top 8\n",
    "for i in range(accuracy_of_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_top_8.shape[1]):\n",
    "        axs[0].text(j, i, f'{accuracy_of_top_8[i, j]}', ha='center', va='center', color='black')\n",
    "\n",
    "# Heatmap for non-top 8\n",
    "cax2 = axs[1].imshow(accuracy_of_non_top_8, cmap='Blues', aspect='auto')\n",
    "axs[1].set_title('Set Count of Non-Top 8')\n",
    "\n",
    "# Annotating values in the cells for non-top 8\n",
    "for i in range(accuracy_of_non_top_8.shape[0]):\n",
    "    for j in range(accuracy_of_non_top_8.shape[1]):\n",
    "        axs[1].text(j, i, f'{accuracy_of_non_top_8[i, j]}', ha='center', va='center', color='black')\n",
    "\n",
    "# Set labels and ticks for both heatmaps\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Max Updates to Rating')  # This is correct for the y-axis\n",
    "    ax.set_ylabel('Rating Difference')  # This should be the x-axis label\n",
    "    ax.set_xticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_yticks(np.arange(0, 10))  # These ticks are fine\n",
    "    ax.set_xticklabels(np.arange(0, 50, 5))  # Matches your `updates` range\n",
    "    ax.set_yticklabels(np.arange(0, 100, 10))  # Matches your `rating_difference` range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glicko-2 updated weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_ratings_df = pd.read_pickle(data_path + 'overall_players_ranking_new_weekly.pkl')\n",
    "player_ratings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(player_ratings_df.max().sort_values()[-20:])\n",
    "print(player_ratings_df.max().describe())\n",
    "print(sum(player_ratings_df.max() == 1500))\n",
    "sns.violinplot(player_ratings_df.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matchup specfic Glicko-2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchup_glicko = pd.read_pickle(data_path + 'player_ratings_matchup_df.pkl')\n",
    "matchup_glicko.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aMSa v Cody rating ##\n",
    "Here we plot aMSa's vs Fox and Cody's vs Yoshi. Cody pretty much always beats aMSa and I would expect that his Fox vs Yoshi matchup rating to be higher than aMSa's Yoshi vs Fox rating. We need to understand why the ratings do not reflect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant row from the DataFrame\n",
    "player_data = matchup_glicko.loc[matchup_glicko['player_id'] == '1021', 'rating_history_dict'].values[0]\n",
    "date_value = matchup_glicko.loc[matchup_glicko['player_id'] == '1021', 'dates_dict'].values[0]\n",
    "# Access the specific nested data: melee/yoshi vs melee/fox\n",
    "yoshi_vs_fox_ratings = player_data['melee/yoshi']['melee/fox']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(date_value['melee/yoshi']['melee/fox'], player_data['melee/yoshi']['melee/fox'], label='aMSa')\n",
    "\n",
    "# Extract the relevant row from the DataFrame\n",
    "player_data = matchup_glicko.loc[matchup_glicko['player_id'] == '19554', 'rating_history_dict'].values[0]\n",
    "date_value = matchup_glicko.loc[matchup_glicko['player_id'] == '19554', 'dates_dict'].values[0]\n",
    "# Access the specific nested data: melee/yoshi vs melee/fox\n",
    "fox_vs_yoshi_ratings = player_data['melee/fox']['melee/yoshi']\n",
    "plt.title('aMSa v Cody')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Glicko-2 Rating')\n",
    "\n",
    "plt.plot(date_value['melee/fox']['melee/yoshi'], player_data['melee/fox']['melee/yoshi'], label='Cody')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchup_glicko.loc[matchup_glicko['player_id'] == '1021', 'game_count_dict'].values[0]['melee/yoshi']['melee/fox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchup_glicko.loc[matchup_glicko['player_id'] == '19554', 'game_count_dict'].values[0]['melee/fox']['melee/yoshi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the player and opponent characters\n",
    "# player_character = 'melee/fox'\n",
    "# opponent_character = 'melee/falco'\n",
    "\n",
    "\n",
    "\n",
    "# def get_matchup(matchup_glicko, player_character, opponent_character):\n",
    "#     # Safely access nested dictionaries with error handling\n",
    "#     def extract_dates(nested_dict):\n",
    "#         \"\"\"Safely extract the dates for a given player and opponent matchup.\"\"\"\n",
    "#         try:\n",
    "#             # Extract the dates if both keys are present\n",
    "#             return nested_dict.get(player_character, {}).get(opponent_character, [])\n",
    "#         except AttributeError:\n",
    "#             # Handle cases where the structure is not as expected\n",
    "#             return []  # Default fallback\n",
    "        \n",
    "#     # Copy the original DataFrame\n",
    "#     glicko2_matchup = matchup_glicko.copy()\n",
    "    \n",
    "#     # Apply the extraction function\n",
    "#     glicko2_matchup['dates'] = glicko2_matchup['dates_dict'].apply(extract_dates)\n",
    "#     glicko2_matchup['rating_history'] = glicko2_matchup['rating_history_dict'].apply(extract_dates)\n",
    "#     glicko2_matchup['rd_history'] = glicko2_matchup['rating_history_dict'].apply(extract_dates)\n",
    "#     # glicko2_matchup['extracted_dates'] = glicko2_matchup['dates_dict'].apply(extract_dates)\n",
    "\n",
    "#     # glicko2_matchup.\n",
    "\n",
    "#     # Check the first few rows to verify the result\n",
    "#     # print(glicko2_matchup[['player_id','dates_dict', 'extracted_dates','extracted_rating']].head())\n",
    "#     glicko2_matchup_filtered = glicko2_matchup[glicko2_matchup['dates'].apply(len) > 0][['player_id','dates', 'rating_history','rd_history']]\n",
    "#     glicko2_matchup_filtered.set_index(glicko2_matchup_filtered['player_id'], inplace=True)\n",
    "\n",
    "#     return glicko2_matchup_filtered[['dates', 'rating_history','rd_history']]\n",
    "\n",
    "# def get_index_of_date(dates, target_date):\n",
    "#     return np.searchsorted(dates, target_date) - 1\n",
    "\n",
    "\n",
    "# print(get_matchup(matchup_glicko,'melee/fox', 'melee/falco'))\n",
    "# # print(glicko2_df)\n",
    "\n",
    "# def print_top_players(glicko2_df, n_players = 5):\n",
    "#     for year in range(2016, 2025):\n",
    "#         for month in [1, 6]:\n",
    "#             # First filter out the players who have only a few updates (played in 5 tournaments)\n",
    "#             filtered_glicko = glicko2_df[glicko2_df['dates'].apply(len) > 4]\n",
    "            \n",
    "#             target_date = datetime.datetime(year, month, 1)\n",
    "\n",
    "#             indices = filtered_glicko['dates'].apply(lambda x: get_index_of_date(x, target_date))\n",
    "            \n",
    "#             # Filter out the players that have not entered a tournament yet.\n",
    "#             filtered_glicko = filtered_glicko[indices > 0]\n",
    "\n",
    "#             # Extract ratings as a Series, ensuring correct data type\n",
    "#             # print(indices)\n",
    "#             ratings_on_date = filtered_glicko.apply(\n",
    "#                 lambda row: row['rating_history'][indices[row.name]], axis=1\n",
    "#             )\n",
    "\n",
    "#             rd_on_date = filtered_glicko.apply(\n",
    "#                 lambda row: row['rd_history'][indices[row.name]], axis=1\n",
    "#             )\n",
    "\n",
    "#             # Ensure ratings_on_date is a Series and sort it\n",
    "#             top_5 = ratings_on_date.sort_values(ascending=False)[:n_players]\n",
    "\n",
    "#             # Retrieve the player tags along with their ratings\n",
    "#             top_5_df = players_df[players_df['player_id'].isin(top_5.index)]\n",
    "#             top_5_df = top_5_df.set_index('player_id').loc[top_5.index]\n",
    "\n",
    "#             top_5_df['rating'] = top_5.values.astype(int)\n",
    "\n",
    "#             # Display the top 5 players sorted by rating\n",
    "#             print(f\"Date: {target_date.strftime('%Y-%m-%d')}\")\n",
    "#             print(top_5_df[['tag', 'rating']].to_string(index=False))\n",
    "#             print()\n",
    "\n",
    "\n",
    "# # Example usage: print top players for Fox vs Falco\n",
    "# print_top_players(get_matchup(matchup_glicko, 'melee/fox', 'melee/falco'), 10)\n",
    "\n",
    "# # get_matchup(matchup_glicko,'melee/fox', 'melee/falco')\n",
    "\n",
    "# print_top_players(get_matchup(matchup_glicko,'melee/fox', 'melee/falco'), 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will visualize the effect of different values of tau in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matchup_glicko_3 = pd.read_pickle('../player_ratings_matchup_data/player_ratings_matchup_tau_3_df.pkl')\n",
    "# matchup_glicko_5 = pd.read_pickle('../player_ratings_matchup_data/player_ratings_matchup_tau_5_df.pkl')\n",
    "# matchup_glicko_7 = pd.read_pickle('../player_ratings_matchup_data/player_ratings_matchup_tau_7_df.pkl')\n",
    "# matchup_glicko_9 = pd.read_pickle('../player_ratings_matchup_data/player_ratings_matchup_tau_9_df.pkl')\n",
    "# matchup_glicko_11 = pd.read_pickle('../player_ratings_matchup_data/player_ratings_matchup_tau_11_df.pkl')\n",
    "# matchup_glicko_13 = pd.read_pickle('../player_ratings_matchup_data/player_ratings_matchup_tau_13_df.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the relevant row from the DataFrame\n",
    "# glicko_data_list = [matchup_glicko_3, matchup_glicko_5, matchup_glicko_7, matchup_glicko_9, matchup_glicko_11, matchup_glicko_13]\n",
    "# # glicko_data_list = [matchup_glicko_3, matchup_glicko_5]\n",
    "# titles  = ['tau = .3', 'tau = .5', 'tau = .7', 'tau = .9', 'tau = 1.1', 'tau = 1.3']\n",
    "\n",
    "# # Create subplots\n",
    "# fig, ax = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "# # Add a main title to the figure\n",
    "# fig.suptitle('aMSa and Cody matchup rating by tau', fontsize=16)\n",
    "\n",
    "\n",
    "# for i, data in enumerate(glicko_data_list):\n",
    "#     amsa_matchup = data.loc[data['player_id'] == '1021']\n",
    "#     cody_matchup = data.loc[data['player_id'] == '19554']\n",
    "    \n",
    "#     amsa_dates = amsa_matchup['dates_dict'].values[0]['melee/yoshi']['melee/fox']\n",
    "#     cody_dates = cody_matchup['dates_dict'].values[0]['melee/fox']['melee/yoshi']\n",
    "    \n",
    "#     amsa_ratings = amsa_matchup['rating_history_dict'].values[0]['melee/yoshi']['melee/fox']\n",
    "#     cody_ratings = cody_matchup['rating_history_dict'].values[0]['melee/fox']['melee/yoshi']\n",
    "    \n",
    "#     # Determine the subplot position in the 3x2 grid\n",
    "#     row, col = divmod(i, 2)\n",
    "    \n",
    "#     ax[row, col].plot(amsa_dates, amsa_ratings, label='aMSa')\n",
    "#     ax[row, col].plot(cody_dates, cody_ratings, label='Cody')\n",
    "#     ax[row, col].legend()\n",
    "#     ax[row, col].set_title(titles[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
