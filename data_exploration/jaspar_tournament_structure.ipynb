{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime \n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SQLite Database into Pandas DataFrames\n",
    "\n",
    "The following code connects to an SQLite database (`melee_player_database.db`) and converts each table within the database into a pandas DataFrame. The DataFrames will be stored in a dictionary, where each key corresponds to the table name with `_df` appended, and the values are the respective DataFrames.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Database Connection**: We use the `sqlite3` library to connect to the SQLite database file.\n",
    "2. **Retrieve Table Names**: A query retrieves all the table names in the database.\n",
    "3. **Convert Tables to DataFrames**: For each table:\n",
    "   - The table is loaded into a pandas DataFrame using `pd.read_sql()`.\n",
    "   - We check each column to see if any data is JSON-formatted (lists or dictionaries). If so, we convert these columns from strings into their corresponding Python objects using `json.loads()`.\n",
    "4. **Store DataFrames**: The DataFrames are stored in a dictionary, where the key is the table name with a `_df` suffix, and the value is the DataFrame.\n",
    "5. **Database Connection Closed**: Once all tables are loaded into DataFrames, the database connection is closed.\n",
    "\n",
    "### Example:\n",
    "If the database contains a table named `players`, the corresponding DataFrame will be stored in the dictionary with the key `players_df`, and can be accessed as:\n",
    "\n",
    "```python\n",
    "players_df = dfs['players_df']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the table names\n",
    "def get_table_names(conn):\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    return pd.read_sql(query, conn)['name'].tolist()\n",
    "\n",
    "# Function to load tables into DataFrames\n",
    "def load_tables_to_dfs(conn):\n",
    "    table_names = get_table_names(conn)\n",
    "    dataframes = {}\n",
    "    \n",
    "    for table in table_names:\n",
    "        # Load table into a DataFrame\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "        \n",
    "        # Detect and convert JSON formatted columns (if any)\n",
    "        for col in df.columns:\n",
    "            # Check if any entry in the column is a valid JSON (list or dictionary)\n",
    "            if df[col].apply(lambda x: isinstance(x, str)).all():\n",
    "                try:\n",
    "                    # Try parsing the column as JSON\n",
    "                    df[col] = df[col].apply(lambda x: json.loads(x) if pd.notnull(x) else x)\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    # If it fails, skip the column\n",
    "                    pass\n",
    "        \n",
    "        # Store the DataFrame with table name + '_df'\n",
    "        dataframes[f\"{table}_df\"] = df\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "if os.path.exists(data_path + 'dfs_dict.pkl'):\n",
    "    cell_has_run = True\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    with open(data_path + 'dfs_dict.pkl', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n",
    "# Check if the flag variable exists in the global scope so that this code does not run twice\n",
    "if 'cell_has_run' not in globals():\n",
    "    path = + data_path + \"melee_player_database.db\"\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(path)\n",
    "\n",
    "    # Convert each table into a DataFrame\n",
    "    dfs = load_tables_to_dfs(conn)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Now, you have a dictionary 'dfs' where each key is the table name with '_df' suffix and value is the corresponding DataFrame.\n",
    "    # For example, to access the DataFrame for a table called 'players':\n",
    "    # players_df = dfs['players_df']\n",
    "\n",
    "    dfs['tournament_info_df']['start'] = pd.to_datetime(dfs['tournament_info_df']['start'], unit='s')\n",
    "    dfs['tournament_info_df']['end'] = pd.to_datetime(dfs['tournament_info_df']['end'], unit='s')\n",
    "\n",
    "    \n",
    "    # Set the flag to indicate that the cell has been run\n",
    "    cell_has_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we adjust the data types of the dataframes so that they are the correct type. (This will be updated as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['sets_df']['best_of'] = dfs['sets_df']['best_of'].fillna(0).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the dictionary of DataFrames as a pickle\n",
    "# with open(data_path + 'dfs_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(dfs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we make dataframes that we will use and print the head.\n",
    "\n",
    "The integers in 'characters' count the number of games the player has played that character. (We verify this for Zain below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = dfs['players_df']\n",
    "players_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df = dfs['ranking_df']\n",
    "ranking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_seasons_df = dfs['ranking_seasons_df']\n",
    "ranking_seasons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_df = dfs['sets_df']\n",
    "print(f\"{sets_df[sets_df['game_data'].apply(lambda x: len(x) > 0)].shape[0] / sets_df.shape[0]:0.01%} percent of sets have some game data\")\n",
    "sets_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_info_df = dfs['tournament_info_df']\n",
    "print(tournament_info_df.shape)\n",
    "tournament_info_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Glicko-2 Exploration ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import weekly updated Glicko-2 rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_ratings_df = pd.read_pickle(data_path + 'overall_players_ranking_new_weekly.pkl')\n",
    "print(player_ratings_df.shape)\n",
    "player_ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Glicko-2 updates\n",
    "Running total of number of updates to each players glicko-2 rating. We use numba njit and prange to speed up the loops in the function. We save the results so that we only need to run the calculation once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def previous_updates(array):\n",
    "    \"\"\" This funcion returns an array like array with the number of times the value above i,j entry of array has changed.\n",
    "    Args:\n",
    "        array (np): the array\n",
    "\n",
    "    Returns:\n",
    "        np: the number of times array has changed above the i,j entry\n",
    "    \"\"\"\n",
    "    previous_updates = np.zeros_like(array, dtype=np.int32)\n",
    "    \n",
    "    for i in range(1, array.shape[0]-1): # row i\n",
    "        previous_row = array[i-1,:]\n",
    "        # print(previous_row)\n",
    "        current_row = array[i,:]\n",
    "        # print(current_row)\n",
    "        change = (previous_row != current_row).astype(np.int32)\n",
    "        change\n",
    "\n",
    "        previous_updates[i+1,:] = previous_updates[i,:] + change\n",
    "\n",
    "    return previous_updates\n",
    "\n",
    "## Testing array\n",
    "# array = np.array([\n",
    "#     [1, 1, 1],\n",
    "#     [1, 1, 2],\n",
    "#     [1, 2, 3],\n",
    "#     [1, 3, 4]])\n",
    "\n",
    "# print(array)\n",
    "# previous_updates(array)\n",
    "# print(previous_updates(array))\n",
    "\n",
    "# # Do the calculation once.\n",
    "# player_ratings_np = player_ratings_df.to_numpy()\n",
    "# start = time.time()\n",
    "# number_of_rating_updates_df = pd.DataFrame(columns=player_ratings_df.columns, index=player_ratings_df.index, data=previous_updates(player_ratings_np))\n",
    "# end = time.time()\n",
    "# print(f'time = {end-start:.2f}')\n",
    "# number_of_rating_updates_df.head()\n",
    "\n",
    "# # Save the results\n",
    "# number_of_rating_updates_df.to_pickle(data_path + 'number_of_rating_updates_df.pkl')\n",
    "\n",
    "## Load the results\n",
    "number_of_rating_updates_df = pd.read_pickle(data_path + 'number_of_rating_updates_df.pkl')\n",
    "number_of_rating_updates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add some columns to sets_df\n",
    "We add the start of the tournament, the player ratings at the start of the tournament, and the number of times the player's rating has been updated before the start of the tournament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a merge on 'key' and 'tournament_key' to bring 'start' dates into sets_df\n",
    "merged_df = sets_df.merge(tournament_info_df[['key', 'start']], left_on='tournament_key', right_on='key', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "import swifter\n",
    "\n",
    "# Function to get both Player 1 and Player 2 ratings and the number of rating updates\n",
    "def get_ratings_and_updates(row, player_ratings_df, number_of_rating_updates_df):\n",
    "    # Find the closest date in player_ratings_df that is <= 'start' date\n",
    "    closest_date = player_ratings_df.index[player_ratings_df.index <= row['start']].max()\n",
    "    \n",
    "    # If there's no valid date, return None for ratings and updates\n",
    "    if pd.isnull(closest_date):\n",
    "        return pd.Series([None, None, None, None], index=['p1_rating', 'p2_rating', 'p1_updates', 'p2_updates'])\n",
    "    \n",
    "    # Fetch Player 1's and Player 2's ratings on the closest date\n",
    "    p1_rating = player_ratings_df.loc[closest_date, row['p1_id']] if row['p1_id'] in player_ratings_df.columns else None\n",
    "    p2_rating = player_ratings_df.loc[closest_date, row['p2_id']] if row['p2_id'] in player_ratings_df.columns else None\n",
    "    \n",
    "    # Fetch Player 1's and Player 2's number of rating updates on the closest date\n",
    "    p1_updates = number_of_rating_updates_df.loc[closest_date, row['p1_id']] if row['p1_id'] in number_of_rating_updates_df.columns else None\n",
    "    p2_updates = number_of_rating_updates_df.loc[closest_date, row['p2_id']] if row['p2_id'] in number_of_rating_updates_df.columns else None\n",
    "    \n",
    "    # Return all values as a pandas Series\n",
    "    return pd.Series([p1_rating, p2_rating, p1_updates, p2_updates], \n",
    "                     index=['p1_rating', 'p2_rating', 'p1_updates', 'p2_updates'])\n",
    "\n",
    "# Apply the function to each row in merged_df\n",
    "# merged_df[['p1_rating', 'p2_rating', 'p1_updates', 'p2_updates']] = merged_df.progress_apply(\n",
    "#     get_ratings_and_updates, axis=1, \n",
    "#     player_ratings_df=player_ratings_df, \n",
    "#     number_of_rating_updates_df=number_of_rating_updates_df,\n",
    "# )\n",
    "\n",
    "# Save\n",
    "# merged_df.to_pickle(data_path + 'augmented_sets_df.pkl')\n",
    "\n",
    "# # Load\n",
    "augmented_sets_df = pd.read_pickle(data_path + 'tournament_sets_with_top_8_df.pkl')\n",
    "augmented_sets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 8 Locations\n",
    "We look for tournaments with a sets with 'location_names' = ['GF', 'Grand Final', 'Grand Final'].\n",
    "The assumption is that tournaments with that location name will have a consistent location name structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Filter the rows where 'location_names' exactly matches ['GF', 'Grand Final', 'Grand Final']\n",
    "# gf_sets_df = sets_df[augmented_sets_df['location_names'].apply(lambda x: x == ['GF', 'Grand Final', 'Grand Final'])]\n",
    "\n",
    "# # Extract the tournament keys for the Grand Finals\n",
    "# gf_tournament_keys = list(gf_sets_df['tournament_key'])\n",
    "\n",
    "# Filter the rows where 'location_names' exactly matches ['GF', 'Grand Final', 'Grand Final']\n",
    "gf_sets_df = augmented_sets_df[augmented_sets_df['location_names'].apply(lambda x: x == ['GF', 'Grand Final', 'Grand Final'])]\n",
    "\n",
    "# Extract the tournament keys for the Grand Finals\n",
    "gf_tournament_keys = list(gf_sets_df['tournament_key'])\n",
    "\n",
    "# Filter the sets_df to include only the sets from tournaments that had Grand Finals\n",
    "valid_tournament_sets_df = augmented_sets_df[augmented_sets_df['tournament_key'].isin(gf_tournament_keys)]\n",
    "\n",
    "# Display the result\n",
    "print(valid_tournament_sets_df.head(3))\n",
    "print(valid_tournament_sets_df['location_names'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location names of the top 8 games are the following:\n",
    "- [f\"L{n}\", f\"Losers {n}\", f\"Losers Round {n}], # Where n is the maximum n in all such location of the  tournament.  \n",
    "- ['WSF', 'Winners Semis', 'Winners Semi-Final'],\n",
    "- ['LQF', 'Losers Quarters', 'Losers Quarter-Final'],\n",
    "- ['WF', 'Winners Final', 'Winners Final'],\n",
    "- ['LSF', 'Losers Semis', 'Losers Semi-Final'],\n",
    "- ['LF', 'Losers Final', 'Losers Final'],\n",
    "- ['GF', 'Grand Final', 'Grand Final'],\n",
    "- ['GFR', 'GF Reset', 'Grand Final Reset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vast majority of the top 8 games have these \"location_names\"\n",
    "top_8_locations = [                                   \n",
    "        ['WSF', 'Winners Semis', 'Winners Semi-Final'],\n",
    "        ['LQF', 'Losers Quarters', 'Losers Quarter-Final'],\n",
    "        ['WF', 'Winners Final', 'Winners Final'],\n",
    "        ['LSF', 'Losers Semis', 'Losers Semi-Final'],\n",
    "        ['LF', 'Losers Final', 'Losers Final'],\n",
    "        ['GF', 'Grand Final', 'Grand Final'],\n",
    "        ['GFR', 'GF Reset', 'Grand Final Reset']\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_first_item(location_list):\n",
    "    return location_list[0]\n",
    "valid_tournament_sets_df['location_names'] = valid_tournament_sets_df['location_names'].progress_apply(take_first_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "\n",
    "\n",
    "# def max_losers_round_pattern(location_name):\n",
    "#     losers_round_pattern = r'^L(\\d+)$'\n",
    "#     return re.match(losers_round_pattern, location_name)\n",
    "        \n",
    "# def find_last_losers(tournament_key):\n",
    "#     tournament_sets_df = valid_tournament_sets_df[valid_tournament_sets_df['tournament_key'] == tournament_key]\n",
    "    \n",
    "#     mask = tournament_sets_df['location_names'].apply(max_losers_round_pattern)\n",
    "    \n",
    "#     losers_rounds = tournament_sets_df[mask]['location_names'][1:].to(int)\n",
    "#     return max(losers_rounds)\n",
    "\n",
    "\n",
    "\n",
    "# valid_tournaments_df = tournament_info_df[tournament_info_df['key'].isin(gf_tournament_keys)]\n",
    "\n",
    "# valid_tournaments_df['last_losers_round'] = valid_tournaments_df['key'].progress_apply(find_last_losers)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# import pandas as pd\n",
    "\n",
    "# Compile the regex pattern once\n",
    "losers_round_pattern = re.compile(r'^L(\\d+)$')\n",
    "\n",
    "# Function to extract the first item from the list in 'location_names'\n",
    "def take_first_item(location_list):\n",
    "    return location_list[0]\n",
    "\n",
    "# Function to extract the round number 'n' from location names like 'L{n}', returns None if no match\n",
    "def extract_losers_round(location_name):\n",
    "    match = losers_round_pattern.match(location_name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "# Extract the losers round numbers in one go\n",
    "valid_tournament_sets_df['losers_round_n'] = valid_tournament_sets_df['location_names'].apply(extract_losers_round)\n",
    "\n",
    "# Group by 'tournament_key' and find the maximum losers round for each tournament\n",
    "max_losers_round_by_tournament = valid_tournament_sets_df.groupby('tournament_key')['losers_round_n'].max().reset_index()\n",
    "\n",
    "# Filter tournaments based on 'gf_tournament_keys'\n",
    "valid_tournaments_df = tournament_info_df[tournament_info_df['key'].isin(gf_tournament_keys)]\n",
    "\n",
    "# Merge the max losers round with the tournaments DataFrame\n",
    "valid_tournaments_df = valid_tournaments_df.merge(max_losers_round_by_tournament, left_on='key', right_on='tournament_key', how='left')\n",
    "\n",
    "# Rename the column for clarity\n",
    "valid_tournaments_df.rename(columns={'losers_round_n': 'last_losers_round'}, inplace=True)\n",
    "\n",
    "# Fill NaN values with a default value (e.g., -1 or 0, depending on your requirements)\n",
    "valid_tournaments_df['last_losers_round'].fillna(0, inplace=True)\n",
    "\n",
    "# Convert the column to integer type\n",
    "valid_tournaments_df['last_losers_round'] = valid_tournaments_df['last_losers_round'].astype(int)\n",
    "\n",
    "# Display the result\n",
    "print(valid_tournaments_df['last_losers_round'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_8_locations = ['WSF']\n",
    "\n",
    "# Create a dictionary to map each tournament_key to its last_losers_round for faster lookups\n",
    "last_losers_round_dict = valid_tournaments_df.set_index('key')['last_losers_round'].to_dict()\n",
    "\n",
    "# Vectorized check if 'location_names' is in the top 8 predefined locations\n",
    "top_8_mask = valid_tournament_sets_df['location_names'].isin(top_8_locations)\n",
    "\n",
    "# Vectorized check if the 'location_names' is equal to the corresponding 'last_losers_round' for each tournament_key\n",
    "losers_round_mask = valid_tournament_sets_df.apply(\n",
    "    lambda row: row['location_names'] == f\"L{last_losers_round_dict.get(row['tournament_key'], '')}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine the two conditions using bitwise OR (|) to get the final mask\n",
    "top_8_combined_mask = top_8_mask | losers_round_mask\n",
    "\n",
    "# Filter the DataFrame based on the combined mask\n",
    "valid_top_8_sets_df = valid_tournament_sets_df[top_8_combined_mask]\n",
    "\n",
    "# Display the result\n",
    "valid_top_8_sets_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_top_8_sets_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'tournament_key' and check if any of the players in top 8 have updates == 0\n",
    "tournaments_with_zero_updates = valid_top_8_sets_df.groupby('tournament_key').filter(\n",
    "    lambda x: not ((x['p1_updates'] <= 1).any() or (x['p2_updates'] <= 1).any())\n",
    ")\n",
    "tournaments_with_zero_updates['higher_rated_won'].sum()/tournaments_with_zero_updates.shape[0]\n",
    "# Step 2: Display the filtered DataFrame\n",
    "# tournaments_with_zero_updates.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournaments_with_zero_updates['higher_rated_won'].sum()/tournaments_with_zero_updates.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournaments_with_zero_updates.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
