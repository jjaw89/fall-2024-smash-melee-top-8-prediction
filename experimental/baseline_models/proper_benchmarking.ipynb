{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "#import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from glicko2 import Player\n",
    "import multiprocessing\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Here, we assume that the ``sets_df`` file, potentially with player swapping, was saved as a separate pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_players = True\n",
    "\n",
    "sets_df = pd.read_pickle(data_path + ('sets_df_randomized.pkl' if swap_players else 'sets_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we ignore the L{n} location name.\n",
    "top_8_locations = [                                   \n",
    "        ['WSF', 'Winners Semis', 'Winners Semi-Final'],\n",
    "        ['LQF', 'Losers Quarters', 'Losers Quarter-Final'],\n",
    "        ['WF', 'Winners Final', 'Winners Final'],\n",
    "        ['LSF', 'Losers Semis', 'Losers Semi-Final'],\n",
    "        ['LF', 'Losers Final', 'Losers Final'],\n",
    "        ['GF', 'Grand Final', 'Grand Final'],\n",
    "        ['GFR', 'GF Reset', 'Grand Final Reset']\n",
    "    ] \n",
    "\n",
    "top_8 = sets_df['location_names'].isin(top_8_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mini_df = pd.read_pickle(data_path + ('dataset_mini_randomized.pkl' if swap_players else 'dataset_mini.pkl'))\n",
    "\n",
    "# Temporary bugfix, might have added stuff twice at some point\n",
    "# dataset_mini_df = dataset_mini_df.loc[:,~dataset_mini_df.columns.duplicated()].copy()\n",
    "\n",
    "#minier_cols = [x for x in dataset_mini_df.columns if \"m2\" not in x and \"_alt_\" not in x]\n",
    "minier_cols = [x for x in dataset_mini_df.columns if \"m2\" not in x and \"_alt_\" not in x]\n",
    "#minier_cols = [x for x in dataset_mini_df.columns if \"m2\" not in x]\n",
    "dataset_minier_df = dataset_mini_df[minier_cols]\n",
    "\n",
    "dataset_df = dataset_minier_df\n",
    "\n",
    "features_default_elo = ['p1_elo', 'p2_elo']\n",
    "features_all_elo = ['p1_elo', 'p2_elo', 'p1/m1/m1_elo', 'p2/m1/m1_elo', 'p1/m1_elo', 'p2/m1_elo']\n",
    "features_all_rd = [x.replace('elo', 'rd') for x in features_all_elo]\n",
    "features_all_updates = [x.replace('elo', 'updates') for x in features_all_elo]\n",
    "features_all_eru = features_all_elo + features_all_rd + features_all_updates\n",
    "features_all_everything = list(dataset_df.columns[:-1])\n",
    "\n",
    "# Filter by elos that actually have nontrivial data\n",
    "quality_filter = pd.Series(True, index=dataset_df.index)\n",
    "for update_col in features_all_updates:\n",
    "    quality_filter = quality_filter & (dataset_df[update_col] >= 10.0)\n",
    "\n",
    "low_quality_filter = pd.Series(True, index=dataset_df.index)\n",
    "for update_col in features_all_updates:\n",
    "    low_quality_filter = low_quality_filter & ((dataset_df[update_col] >= 2.0) & (dataset_df[update_col] <= 10.0))\n",
    "\n",
    "similar_default_elo_filter = (dataset_df['p1_elo'] - dataset_df['p2_elo']).abs() <= 20.0\n",
    "\n",
    "dataset_df[similar_default_elo_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An observation about the data\n",
    "\n",
    "First, we note the motivation for including ``quality_filter`` as an option for the data. With it, ELO scores appear to (mostly) follow a multivariate normal distribution, while lower-quality data tends to cluster far more around the default elo values of 1500. This suggests, at least in the case of high-quality data, that very simplistic linear models might actually yield the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elos = ['p1_elo', 'p2_elo', 'p1/m1/m1_elo', 'p2/m1/m1_elo', 'p1/m1_elo', 'p2/m1_elo']\n",
    "\n",
    "plt.title('default, alt2, alt3 ELOs for quality data')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.subplot(3,2,2*i+1)\n",
    "    plt.scatter(dataset_df[quality_filter & (dataset_df['winner'] == 1.0)][all_elos[2*i]],\n",
    "                dataset_df[quality_filter & (dataset_df['winner'] == 1.0)][all_elos[2*i+1]],\n",
    "                s=0.3, alpha=0.2, label='p1 wins')\n",
    "    plt.xlim(0, 3000)\n",
    "    plt.ylim(0, 3000)\n",
    "    \n",
    "    if i != 2:\n",
    "        plt.xticks([])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3,2,2*i+2)\n",
    "    plt.scatter(dataset_df[quality_filter & (dataset_df['winner'] == 0.0)][all_elos[2*i]],\n",
    "                dataset_df[quality_filter & (dataset_df['winner'] == 0.0)][all_elos[2*i+1]],\n",
    "                s=0.3, alpha=0.2, label='p2 wins')\n",
    "    plt.xlim(0, 3000)\n",
    "    plt.ylim(0, 3000)\n",
    "    plt.legend()\n",
    "\n",
    "    if i != 2:\n",
    "        plt.xticks([])\n",
    "\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing some basic models\n",
    "\n",
    "Before we begin, some important remarks are in order. In some sense, this is time series data, and in another sense, it is not. It somewhat showcases the evolution of player ELO ratings (and related features) over time, albeit without directly linking them to certain players. However, these ELO scores, in some sense, already take into account all of the player's past performance up to a certain point. Likewise, RD values are meant to indicate the \"uncertainty\" in any player's current ELO score, and at any one point in time it takes into account all of their previous games, and even how long it has been since they've last played.\n",
    "\n",
    "That being said, it is somewhat ill-advised to shuffle the data when training. Especially for more advanced models, it might potentially be able to recognize that a very precise ELO score has shown up in some future match, and conclude that it must have won some previous matches (note that we have updated ELO scores only once a week).\n",
    "\n",
    "Here, we begin by training some basic models that have the goal of predicting the outcome of individual sets, with no ability to look back on any past performance (single-set models). We first want to observe the impact of the following factors, and are not yet interested in serious hyperparameter tuning:\n",
    "\n",
    "* Is it better to only train on more recent data rather than all data up to a certain point (perhaps game meta, average ELO, etc... shifts over time)\n",
    "* Are we actually gaining anything by including all of our engineered ELO scores, rather than just the default ones?\n",
    "* What is the impact of considering only on \"high quality\" data which has received many updates to all ELO scores?\n",
    "\n",
    "We also note that for this single-set predictor, we train on data up to 2022 (and test on 2023) for cross-validation and hyperparameter tuning. The secondary model which takes tournament performance into account (which will use this single-set model), tuned on 2023 data and have its final performance tested on 2024 data. However, for the interest of obtaining a final performance score for the single-set predictor, it should be safely testable on 2024 data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models that only use the ELO scores\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2017, 2022+1)\n",
    "\n",
    "# These models work best with normally distributed data, which appears to be the case for the various ELOs\n",
    "models = {'lr': LogisticRegression(penalty=None, max_iter=10000),\n",
    "          'lda': LinearDiscriminantAnalysis(),\n",
    "          'qda': QuadraticDiscriminantAnalysis(),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "training_modes = ['past_year', 'all_years'] # Train on just the past year, or all years (starting from 2016).\n",
    "elo_modes = ['default_elo', 'all_elo'] # Only the default glicko2 elo scores, or all engineered ones\n",
    "data_modes = ['quality_data', 'all_data'] # Whether or not each elo has had at least 10 updates\n",
    "\n",
    "for training_mode in training_modes:\n",
    "    for elo_mode in elo_modes:\n",
    "        for data_mode in data_modes:\n",
    "            # Just using default ELOs\n",
    "            for i, y in enumerate(tqdm(years)):\n",
    "                # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "                # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "                dataset_train_df = dataset_df[(sets_df['start'] >= datetime.datetime(2016 if training_mode == 'all_years' else y, 1, 1)) &\n",
    "                                            (sets_df['end'] <= datetime.datetime(y,12,31)) &\n",
    "                                            (quality_filter if data_mode == 'quality_data' else True)]\n",
    "                dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                            (sets_df['end'] <= datetime.datetime(y+1,12,31)) &\n",
    "                                            (quality_filter if data_mode == 'quality_data' else True)]\n",
    "                \n",
    "                for j, name in enumerate(models):\n",
    "                    models[name].fit(dataset_train_df[features_default_elo if elo_mode == 'default_elo' else features_all_elo], dataset_train_df['winner'])\n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_default_elo if elo_mode == 'default_elo' else features_all_elo])\n",
    "                    y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "                    ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "                    acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "            print(\"Using \" + training_mode + \" and \" + elo_mode + \" and \" + data_mode)\n",
    "            print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "                        index=years,\n",
    "                        columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of particular interest are the latter few years, because the data is of substantially higher quality, and it will also more closely follow years 2023 and 2024.\n",
    "\n",
    "Without any need to run any statistical tests, a simple side-by-side comparison reveals that\n",
    "* In all instances, training on more recent data is slightly favourable for both linear models and XGBoost. We will stick with that from now on.\n",
    "* LDA appears favourable over QDA.\n",
    "* When dealing with higher-quality data, basic linear models substantially outperform XGBoost, at least without any hyperparameter tuning and training only on a very specific subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A closer look at linear models\n",
    "\n",
    "Here, we test the following linear models in the case of high-quality data, and explicitly low-quality data. The models we will be testing are LogisticRegression (classification), LDA, and a custom ErrorLDA model which should take into account the various RD values (roughly interpreted as a measurement error on the player ELOs, with the \"true\" ELOs being unknown values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced models\n",
    "import xgboost as xgb\n",
    "import errorlda\n",
    "import importlib\n",
    "\n",
    "# Just in case we make changes to this model.\n",
    "importlib.reload(errorlda)\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2020, 2022+1)\n",
    "\n",
    "models = {'lr': LogisticRegression(penalty=None, max_iter=10000),\n",
    "          'lda': LinearDiscriminantAnalysis(),\n",
    "          'errorlda': errorlda.ErrorLDA(),\n",
    "          #'errorlda_scaling': errorlda.ErrorLDA(),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "data_modes = ['low_quality', 'high_quality']\n",
    "\n",
    "for data_mode in data_modes:\n",
    "    for i, y in enumerate(tqdm(years)):\n",
    "        # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "        # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "        dataset_train_df = dataset_df[(sets_df['start'] >= datetime.datetime(y,1,1)) &\n",
    "                                    (sets_df['end'] <= datetime.datetime(y,12,31)) &\n",
    "                                    (low_quality_filter if data_mode == 'low_quality' else quality_filter)]\n",
    "        dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                    (sets_df['end'] <= datetime.datetime(y+1,12,31)) &\n",
    "                                    (low_quality_filter if data_mode == 'low_quality' else quality_filter)]\n",
    "        \n",
    "        for j, name in enumerate(models):\n",
    "            y_prob = None\n",
    "            y_pred = None\n",
    "\n",
    "            # Basically, all of these require slightly different syntax and restriction of features\n",
    "            # * ErrorLDA to include the variances (RD values)\n",
    "            # * LDA to just use the the ELO values without RD values (mainly to compare to ErrorLDA)\n",
    "            match name:\n",
    "                case 'lr' | 'lda':\n",
    "                    models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'])\n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_all_elo])\n",
    "\n",
    "                case 'errorlda' | 'errorlda_scaling':\n",
    "                    models[name] = errorlda.ErrorLDA() # Not sure if I've implemented .fit() to reset everything upon every new fit.\n",
    "\n",
    "                    # Experimental pre-scaling of the RD values\n",
    "                    pre_scaler = np.diag([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "                    models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'],\n",
    "                                    X_train_errors=dataset_train_df[features_all_rd].apply(lambda row: np.diag(row.to_numpy() ** 2) @ pre_scaler, axis=1),\n",
    "                                    error_scaling=(True if name == 'errorlda_scaling' else False)) # Note that error-scaling is slower by like a factor of 8, unfortunately\n",
    "                    \n",
    "                    # Literally just for numerical stability, in case some eigenvalues are near zero.\n",
    "                    # This will add 1 to each eigenvalue.\n",
    "                    models[name].variance += np.identity(len(features_all_rd))\n",
    "                    \n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_all_elo],\n",
    "                                                        X_error=dataset_test_df[features_all_rd].apply(lambda row: np.diag(row.to_numpy() ** 2) @ pre_scaler, axis=1))\n",
    "\n",
    "                case 'xgb': # Special syntax required here.\n",
    "                    models[name].fit(dataset_train_df[features_all_elo + features_all_rd], dataset_train_df['winner'])\n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_all_elo + features_all_rd])\n",
    "\n",
    "            # Rest of the prediction code is the same among all models    \n",
    "            y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "            ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "            acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "    print(\"Scores involving all ELOs and RDs in \" + data_mode + \" mode\")\n",
    "    print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "                index=years,\n",
    "                columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best linear model\n",
    "\n",
    "At least from the looks of things, it does not appear that there is any noticeable difference in accuracy and log loss with the ErrorLDA model, compared to the other two linear models. As such, considering the enormous lack of speed in training these models, we will just stick with LogisticRegression() in such special cases for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using separate depending on the quality of the data\n",
    "\n",
    "Here, we try another experimental approach, where we split the data into different \"quality classes\" (depending on the RD values of each elo), and train linear models on each class (and test if it is better than XGBoost in each case). We then combine all of the better models into one ensemble, and see if the quality of the predictions improve at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data according to the various amount of updates (\"no data/low quality data/high quality data\")\n",
    "\n",
    "def get_quality_encoding(row):\n",
    "    result = ''\n",
    "\n",
    "    for update_col in features_all_updates:\n",
    "        #if row[update_col] >= 20.0:  # High quality\n",
    "        #    result += '3'\n",
    "        if row[update_col] >= 10.0:  # High quality\n",
    "            result += '2'\n",
    "        elif row[update_col] >= 1.0: # Low quality\n",
    "            result += '1'\n",
    "        else:                        # None (or basically none)\n",
    "            result += '0'\n",
    "    \n",
    "    return result\n",
    "\n",
    "dataset_df = dataset_df.copy() # Fixes \"copy of a slice\" nonsense\n",
    "dataset_df['quality_class'] = dataset_df.apply(get_quality_encoding, axis=1)\n",
    "\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splits off the data into different classes and applies distinct models to each\n",
    "class SplitRegression:\n",
    "    # min_data is the minimum number of occurences of a class to split it off\n",
    "    def __init__(self, min_data=1000,\n",
    "                 default_model=xgb.XGBClassifier()):\n",
    "        self.features = None # Basically the list of columns of the training data\n",
    "        self.min_data = min_data\n",
    "\n",
    "        self.large_classes = None\n",
    "        self.small_classes = None\n",
    "        self.zero_classes = None\n",
    "\n",
    "        self.large_models = {} # A dictionary of models, one for each class with lots of data\n",
    "        self.large_models_reduced_features = {} # Keep track of whether to only pass reduced features to each large model\n",
    "\n",
    "        # TODO: Possibly get rid of small/zero separation        \n",
    "        self.small_model = default_model # Just bundle together all small classes (except zero) into one group and apply a more complex model to them\n",
    "        self.zero_model = default_model # Apply another (possibly) separate model to the \"basically no data\" group\n",
    "\n",
    "        self.default_model = default_model\n",
    "\n",
    "    # Figure out what features we actually need to pull from (and ignore the ones that have basically zero data)\n",
    "    def class_to_features(self, c):\n",
    "        return [self.features[i] for i,x in enumerate(c) if x != '0']\n",
    "    \n",
    "    # Jankiness, but it works. If there is *technically* only one class in whatever we've split off here,\n",
    "    # then we should make sure to add the other class. Ideally with fake data.\n",
    "    def patch_missing_outcome(self, X_train, y_train):\n",
    "        old_len = len(X_train.index)\n",
    "\n",
    "        y_unique = list(y_train.unique()) # Could very well be empty, or just one value. This handles both cases.\n",
    "        for y in [y for y in [0.0, 1.0] if y not in y_unique]:\n",
    "            X_train = pd.concat([X_train, pd.DataFrame([1500.0] * len(X_train.columns), index=X_train.columns).T], axis=0) # Most canonical fake data\n",
    "            y_train = pd.concat([y_train, pd.Series([y])])\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    # Kind of assumes these are all dataframes and series-es\n",
    "    # X_class corresponds to a unique id of the form n_1 ... n_k for the class\n",
    "    # A value of n_i = 0 means we will not actually use data from that class\n",
    "    def fit(self, X, y, X_class):\n",
    "        # First and foremost, don't forget to fit the default model to all the data\n",
    "        # (Subsequent models will be compared to it, and it will be used when they don't have better performance)\n",
    "        self.default_model.fit(X,y)\n",
    "\n",
    "        self.features = list(X.columns)\n",
    "\n",
    "        class_counts = X_class.value_counts()\n",
    "\n",
    "        # The zero classes, treated as separate classes. Realistically, there should only be at most one.\n",
    "        self.zero_classes = [c for c in class_counts.index if self.class_to_features(c) == []]\n",
    "        \n",
    "        self.large_classes = [x for x in class_counts[class_counts >= self.min_data].index if x not in self.zero_classes]\n",
    "        self.small_classes = [x for x in class_counts[class_counts < self.min_data].index if x not in self.zero_classes]\n",
    "\n",
    "        # As we've seen, LogisticRegression does quite well on classes with lots of data.\n",
    "        # We kind of assume each class has slightly different means, hence the need to train separate models,\n",
    "        # and also just skip out entirely on passing it info from features with basically no info.\n",
    "        #\n",
    "        # Train and test such a model, and very importantly, actually see if it does better than the default!\n",
    "        for c in self.large_classes:\n",
    "            class_features = self.class_to_features(c)\n",
    "\n",
    "            # Just in case the outcome is constant on this training subset.\n",
    "            X_train, y_train = self.patch_missing_outcome(X[X_class == c], y[X_class == c])\n",
    "\n",
    "            # Gonna assume some (possibly very slight) correlation between past and future entries, and so we avoid shuffling\n",
    "            X_train_tt, X_train_ho, y_train_tt, y_train_ho = train_test_split(X_train, y_train, shuffle=False)\n",
    "\n",
    "            default_ll = log_loss(y_train_ho, self.default_model.predict_proba(X_train_ho))\n",
    "\n",
    "            test_model = LogisticRegression(penalty=None, max_iter=10000)\n",
    "\n",
    "            test_model.fit(X_train_tt[class_features], y_train_tt)\n",
    "            test_ll = log_loss(y_train_ho, test_model.predict_proba(X_train_ho[class_features]))\n",
    "\n",
    "            # Depending on if we stick with the default or the new model,\n",
    "            # we will also need to keep track of whether or not the features need to be reduced or not.\n",
    "            if test_ll <= 0.95 * default_ll: # Just to account for RNG in some way\n",
    "                test_model.fit(X_train[class_features], y_train) # Extra training data - let's not throw out the holdout set!\n",
    "                self.large_models[c] = test_model\n",
    "                self.large_models_reduced_features[c] = True\n",
    "            else:\n",
    "                self.large_models[c] = self.default_model\n",
    "                self.large_models_reduced_features[c] = False\n",
    "\n",
    "        # Now the zero classes (realistically at most one of them) get lumped together and have a single model used.\n",
    "        # Same for the small classes.\n",
    "        # Might as well toss all features in there, just in case it feels like extracting *some* kind of info, somehow.\n",
    "        small_class_filter = X_class.apply(lambda c: c in self.small_classes)\n",
    "        zero_class_filter = X_class.apply(lambda c: c in self.zero_classes)\n",
    "        \n",
    "        # TODO: Tuning of hyperparameters\n",
    "        self.small_model = xgb.XGBClassifier()\n",
    "        self.zero_model = xgb.XGBClassifier()        \n",
    "\n",
    "        # These could technically be empty, or have just one outcome\n",
    "        # Small class\n",
    "        X_train, y_train = self.patch_missing_outcome(X[small_class_filter], y[small_class_filter])\n",
    "        self.small_model.fit(X_train, y_train)\n",
    "        # Zero class\n",
    "        X_train, y_train = self.patch_missing_outcome(X[zero_class_filter], y[zero_class_filter])\n",
    "        self.zero_model.fit(X_train, y_train)\n",
    "\n",
    "    def predict_proba(self, X, X_class):\n",
    "\n",
    "        merged_df = pd.concat([X, X_class], axis=1)\n",
    "        merged_df.columns = list(X.columns) + ['quality_class']\n",
    "\n",
    "        # It is substantially more efficient to figure out which model applies to which row,\n",
    "        # use a groupby(), and feed the entire block of data into the model,\n",
    "        # rather than doing this entire operation row by row.\n",
    "\n",
    "        # Also, this breaks if we try to put the models directly in the dataframe.\n",
    "        # Let's instead just assign them numeric values.\n",
    "        model_list = [self.large_models[c] for c in self.large_models] + [self.zero_model, self.small_model]\n",
    "        model_to_num_dict = {}\n",
    "\n",
    "        for i, model in enumerate(model_list):\n",
    "            model_to_num_dict[model] = i\n",
    "\n",
    "        def assign_model(row):\n",
    "            c = row['quality_class']\n",
    "\n",
    "            if c in self.large_classes:\n",
    "                return model_to_num_dict[self.large_models[c]]\n",
    "            \n",
    "            # Not a large model. Perhaps zero?\n",
    "            if self.class_to_features(c) == []:\n",
    "                return model_to_num_dict[self.zero_model]\n",
    "            \n",
    "            # Only possibility is the \"small\" model. Just lump it in with the rest of the data.\n",
    "            return model_to_num_dict[self.small_model]\n",
    "        \n",
    "        merged_df['model'] = merged_df.apply(assign_model, axis=1)\n",
    "        merged_df['model_copy'] = merged_df['model'] # include_groups=True deprecation nonsense\n",
    "\n",
    "        # Run predict_proba on entire blocks of data that use the same model,\n",
    "        # rather than running it row by row (slow)!\n",
    "        def block_proba(df):\n",
    "            model_num = df.iloc[0]['model_copy']\n",
    "            c = df.iloc[0]['quality_class']\n",
    "            c_features = df.columns[:-2] # Ignore 'quality_class' and 'model_copy' columns\n",
    "\n",
    "            # Large model, might need to restrict features\n",
    "            # (usually this is the case if it's not pointing to the default model)\n",
    "            if model_num < len(self.large_models) and self.large_models_reduced_features[c]:\n",
    "                c_features = self.class_to_features(c)\n",
    "            \n",
    "            model = model_list[model_num]\n",
    "            \n",
    "            return pd.DataFrame(model.predict_proba(df[c_features]), index=df.index)\n",
    "        \n",
    "        \n",
    "        result = merged_df.groupby('model').apply(block_proba, include_groups=False)\n",
    "\n",
    "        # Note that this will have a two-layered index now.\n",
    "        # One for the model number, and one for the original index.\n",
    "        # Let's remove it.\n",
    "        result = result.droplevel(0)\n",
    "\n",
    "        # NOTE: The index is NOT the original order anymore, because of the above! Let's fix that\n",
    "        result = result.loc[X.index]\n",
    "\n",
    "        return result.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced models\n",
    "import xgboost as xgb\n",
    "import errorlda\n",
    "import importlib\n",
    "\n",
    "# Just in case we make changes to this model.\n",
    "importlib.reload(errorlda)\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2018, 2022+1)\n",
    "\n",
    "models = {'split': SplitRegression(min_data=5000),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "\n",
    "\n",
    "for i, y in enumerate(tqdm(years)):\n",
    "    # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "    # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "    dataset_train_df = dataset_df[(sets_df['start'] >= datetime.datetime(y,1,1)) &\n",
    "                                (sets_df['end'] <= datetime.datetime(y,12,31))]\n",
    "    dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                (sets_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "    \n",
    "    for j, name in enumerate(models):\n",
    "        y_prob = None\n",
    "        y_pred = None\n",
    "\n",
    "        # Basically, all of these require slightly different syntax and restriction of features\n",
    "        # * ErrorLDA to include the variances (RD values)\n",
    "        # * LDA to just use the the ELO values without RD values (mainly to compare to ErrorLDA)\n",
    "        match name:\n",
    "            case 'split': # Special syntax required here.\n",
    "                models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'], dataset_train_df['quality_class'])\n",
    "                y_prob = models[name].predict_proba(dataset_test_df[features_all_elo], dataset_test_df['quality_class'])\n",
    "            case 'xgb':\n",
    "                models[name].fit(dataset_train_df[features_all_elo + features_all_rd], dataset_train_df['winner'])\n",
    "                y_prob = models[name].predict_proba(dataset_test_df[features_all_elo + features_all_rd])\n",
    "\n",
    "        # Rest of the prediction code is the same among all models    \n",
    "        y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "        ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "        acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "print(\"Scores involving all ELOs and RDs in \" + data_mode + \" mode\")\n",
    "print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "            index=years,\n",
    "            columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the above linear/XGBoost ensemble\n",
    "\n",
    "Unfortunately, it again appears as if the performance of this ensemble is comparable (possibly ever so slightly worse) than untuned XGBoost. The only exeption appears to be around years 2019/2020, but those suffer from poorer data quality. You'll have to take my word for it that this was tested on many different combinations of parameters and data test sets, and the results are remarkably consistent. Almost always, the ensemble has about a 1-2% higher log loss than untuned XGBoost on years 2021/2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "years = range(2021, 2022+1)\n",
    "\n",
    "# Actually partition the data ahead of time, to avoid doing it every time.\n",
    "# It will be the same, every single time.\n",
    "data_partitioned = {}\n",
    "\n",
    "for y in years: \n",
    "    dataset_train_df = dataset_df[(sets_df['start'] >= datetime.datetime(y,1,1)) &\n",
    "                                  (sets_df['end'] <= datetime.datetime(y,12,31))]\n",
    "    dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                 (sets_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "        \n",
    "    dtrain = xgb.DMatrix(dataset_train_df[features_all_elo + features_all_rd], label=dataset_train_df['winner'])\n",
    "    dvalid = xgb.DMatrix(dataset_test_df[features_all_elo + features_all_rd], label=dataset_test_df['winner'])\n",
    "    \n",
    "    data_partitioned[y] = (dataset_train_df, dataset_test_df, dtrain, dvalid)\n",
    "\n",
    "# Baseline results to compare to.\n",
    "xgb_baseline = xgb.XGBClassifier()\n",
    "results_baseline = np.zeros(len(years))\n",
    "\n",
    "for i,y in enumerate(years):\n",
    "    dataset_train_df = data_partitioned[y][0]\n",
    "    dataset_test_df = data_partitioned[y][1]\n",
    "\n",
    "    xgb_baseline.fit(dataset_train_df[features_all_elo + features_all_rd], dataset_train_df['winner'])    \n",
    "    y_prob = xgb_baseline.predict_proba(dataset_test_df[features_all_elo + features_all_rd])\n",
    "\n",
    "    results_baseline[i] = log_loss(dataset_test_df['winner'], y_prob)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Years to train on. We will train on one and test on the following.\n",
    "    # Three years for vaguely decent cross-validation.\n",
    "    results = np.zeros(len(years))\n",
    "\n",
    "    for i,y in enumerate(years):\n",
    "\n",
    "        # Far more efficient to compute these only once, as they don't actually change.\n",
    "        dataset_train_df = data_partitioned[y][0]\n",
    "        dataset_test_df = data_partitioned[y][1]\n",
    "        dtrain = data_partitioned[y][2]\n",
    "        dvalid = data_partitioned[y][3]\n",
    "\n",
    "        # After some testing, a lot of these options don't really do much if other than their default values.\n",
    "        param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, step=50),\n",
    "            \"verbosity\": 0,\n",
    "            \"objective\": \"binary:logistic\", # Actually make sure we are training a classifier that can spit out probabilities\n",
    "            # defines booster, gblinear for linear functions.\n",
    "            \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\"]),\n",
    "            # L2 regularization weight.\n",
    "            #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            # L1 regularization weight.\n",
    "            #\"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "            # sampling ratio for training data.\n",
    "            #\"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "            # sampling according to each tree.\n",
    "            #\"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        }\n",
    "\n",
    "        if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "            # maximum depth of the tree, signifies complexity of the tree.\n",
    "            param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 2, 6, step=1)\n",
    "            # minimum child weight, larger the term more conservative the tree.\n",
    "            #param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "            param[\"eta\"] = trial.suggest_float(\"eta\", 1e-5, 1.0, log=True)\n",
    "            # defines how selective algorithm is.\n",
    "            param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-5, 1.0, log=True)\n",
    "            param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "        if param[\"booster\"] == \"dart\":\n",
    "            param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "            param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "        bst = xgb.train(param, dtrain)\n",
    "\n",
    "        y_prob = bst.predict(dvalid) # Not like sklearn api. This can spit out probabilities, depending on objective.\n",
    "        results[i] = log_loss(dataset_test_df['winner'], y_prob)\n",
    "\n",
    "    # See how much we've improved over the baseline.\n",
    "    # Measured as the average fractional decrease in log loss\n",
    "\n",
    "    return (results_baseline / results).mean()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-test that exact model and spit out the exact results\n",
    "\n",
    "years = range(2021, 2022+1)\n",
    "\n",
    "models = {'xgb_default': xgb.XGBClassifier(),\n",
    "          'xgb_manual': xgb.XGBClassifier(n_estimators=300, max_depth=3),\n",
    "          #'xgb_tuned1': xgb.XGBClassifier(booster='gbtree', max_depth=7, min_child_weight=10, eta=0.336, gamma=0.01, grow_policy='depthwise'),\n",
    "          #'xgb_tuned2': xgb.XGBClassifier(booster='dart', max_depth=8, min_child_weight=8, eta=0.355, gamma=1.329e-6, grow_policy='lossguide', sample_type='weighted', normalize_type='forest', rate_drop=1.317e-5, skip_drop=0.0096)\n",
    "          }\n",
    "\n",
    "results = np.zeros(shape=(len(years),len(models))) # Hold both model results now\n",
    "\n",
    "for i,y in enumerate(years):\n",
    "    dataset_train_df = dataset_df[(sets_df['start'] >= datetime.datetime(y,1,1)) &\n",
    "                                  (sets_df['end'] <= datetime.datetime(y,12,31))]\n",
    "    dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                 (sets_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "\n",
    "    for j,name in enumerate(models):\n",
    "        models[name].fit(dataset_train_df[features_all_elo + features_all_rd], dataset_train_df['winner'])    \n",
    "        y_prob = models[name].predict_proba(dataset_test_df[features_all_elo + features_all_rd])\n",
    "        results[i,j] = accuracy_score(dataset_test_df['winner'], y_prob[:,1] >= 0.5)\n",
    "\n",
    "print(pd.DataFrame(results, index=years, columns=models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test = xgb.XGBClassifier()\n",
    "\n",
    "for i,y in enumerate(years):\n",
    "    dataset_train_df = dataset_df[(sets_df['start'] >= datetime.datetime(y,1,1)) &\n",
    "                                  (sets_df['end'] <= datetime.datetime(y,12,31))]\n",
    "    dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                 (sets_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "    \n",
    "    xgb_test.fit(dataset_train_df[features_default_elo], dataset_train_df['winner'])\n",
    "    y_prob = xgb_test.predict_proba(dataset_test_df[features_default_elo])\n",
    "    print(y, log_loss(dataset_test_df['winner'], y_prob), accuracy_score(dataset_test_df['winner'], y_prob[:,1] >= 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sets_df['p1_id'] == sets_df['winner_id']).astype(float).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
