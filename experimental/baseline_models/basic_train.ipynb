{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Trials For Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import datetime\n",
    "import os\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if os.path.exists('/workspace/data_2'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data_2/'\n",
    "else:\n",
    "    data_path = '../data/'\n",
    "    \n",
    "# if torch.cuda.is_available() == False:\n",
    "#     RuntimeError(\"GPU detected: False\")\n",
    "#     print(\"GPU detected: False\")\n",
    "# else:\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"The GPU is detected.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_df = pd.read_pickle(data_path + 'sets_with_results_df.pkl')\n",
    "dataset_mini_df = pd.read_pickle(data_path + 'dataset_mini.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_x</th>\n",
       "      <th>game</th>\n",
       "      <th>tournament_key</th>\n",
       "      <th>winner_id</th>\n",
       "      <th>loser_id</th>\n",
       "      <th>p1_id</th>\n",
       "      <th>p2_id</th>\n",
       "      <th>p1_score</th>\n",
       "      <th>p2_score</th>\n",
       "      <th>valid_score</th>\n",
       "      <th>...</th>\n",
       "      <th>result_1</th>\n",
       "      <th>result_2</th>\n",
       "      <th>result_3</th>\n",
       "      <th>result_4</th>\n",
       "      <th>result_5</th>\n",
       "      <th>result_6</th>\n",
       "      <th>result_7</th>\n",
       "      <th>result_8</th>\n",
       "      <th>result_9</th>\n",
       "      <th>result_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104675843</td>\n",
       "      <td>melee</td>\n",
       "      <td>mdva-invitational-2017-(challonge-mirror)</td>\n",
       "      <td>5620</td>\n",
       "      <td>Chillin</td>\n",
       "      <td>Chillin</td>\n",
       "      <td>5620</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104675844</td>\n",
       "      <td>melee</td>\n",
       "      <td>mdva-invitational-2017-(challonge-mirror)</td>\n",
       "      <td>Aglet</td>\n",
       "      <td>15634</td>\n",
       "      <td>Aglet</td>\n",
       "      <td>15634</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104675845</td>\n",
       "      <td>melee</td>\n",
       "      <td>mdva-invitational-2017-(challonge-mirror)</td>\n",
       "      <td>6126</td>\n",
       "      <td>1097</td>\n",
       "      <td>1097</td>\n",
       "      <td>6126</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104675846</td>\n",
       "      <td>melee</td>\n",
       "      <td>mdva-invitational-2017-(challonge-mirror)</td>\n",
       "      <td>1069</td>\n",
       "      <td>Chu</td>\n",
       "      <td>1069</td>\n",
       "      <td>Chu</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104675847</td>\n",
       "      <td>melee</td>\n",
       "      <td>mdva-invitational-2017-(challonge-mirror)</td>\n",
       "      <td>Rishi</td>\n",
       "      <td>Jerry</td>\n",
       "      <td>Jerry</td>\n",
       "      <td>Rishi</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       key_x   game                             tournament_key winner_id  \\\n",
       "0  104675843  melee  mdva-invitational-2017-(challonge-mirror)      5620   \n",
       "1  104675844  melee  mdva-invitational-2017-(challonge-mirror)     Aglet   \n",
       "2  104675845  melee  mdva-invitational-2017-(challonge-mirror)      6126   \n",
       "3  104675846  melee  mdva-invitational-2017-(challonge-mirror)      1069   \n",
       "4  104675847  melee  mdva-invitational-2017-(challonge-mirror)     Rishi   \n",
       "\n",
       "  loser_id    p1_id  p2_id  p1_score  p2_score  valid_score  ...  result_1  \\\n",
       "0  Chillin  Chillin   5620         1         3         True  ...       0.5   \n",
       "1    15634    Aglet  15634         3         2         True  ...       0.5   \n",
       "2     1097     1097   6126         0         3         True  ...       0.5   \n",
       "3      Chu     1069    Chu         3         0         True  ...       0.5   \n",
       "4    Jerry    Jerry  Rishi         1         3         True  ...       0.5   \n",
       "\n",
       "  result_2 result_3 result_4 result_5 result_6  result_7 result_8  result_9  \\\n",
       "0      0.5      0.5      0.5      0.5      0.5       0.5      0.5       0.5   \n",
       "1      0.5      0.5      0.5      0.5      0.5       0.5      0.5       0.5   \n",
       "2      0.5      0.5      0.5      0.5      0.5       0.5      0.5       0.5   \n",
       "3      0.5      0.5      0.5      0.5      0.5       0.5      0.5       0.5   \n",
       "4      0.5      0.5      0.5      0.5      0.5       0.5      0.5       0.5   \n",
       "\n",
       "  result_10  \n",
       "0       0.5  \n",
       "1       0.5  \n",
       "2       0.5  \n",
       "3       0.5  \n",
       "4       0.5  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_elo</th>\n",
       "      <th>p2_elo</th>\n",
       "      <th>p1_rd</th>\n",
       "      <th>p2_rd</th>\n",
       "      <th>p1_updates</th>\n",
       "      <th>p2_updates</th>\n",
       "      <th>p1_m1_usage</th>\n",
       "      <th>p1_m2_usage</th>\n",
       "      <th>p2_m1_usage</th>\n",
       "      <th>p2_m2_usage</th>\n",
       "      <th>...</th>\n",
       "      <th>p1/m2_elo</th>\n",
       "      <th>p1/m2_rd</th>\n",
       "      <th>p1/m2_updates</th>\n",
       "      <th>p2/m1_elo</th>\n",
       "      <th>p2/m1_rd</th>\n",
       "      <th>p2/m1_updates</th>\n",
       "      <th>p2/m2_elo</th>\n",
       "      <th>p2/m2_rd</th>\n",
       "      <th>p2/m2_updates</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1_elo  p2_elo  p1_rd  p2_rd  p1_updates  p2_updates  p1_m1_usage  \\\n",
       "681  1500.0  1500.0  350.0  350.0         0.0         0.0          0.0   \n",
       "682  1500.0  1500.0  350.0  350.0         0.0         0.0          0.0   \n",
       "683  1500.0  1500.0  350.0  350.0         0.0         0.0          0.0   \n",
       "684  1500.0  1500.0  350.0  350.0         0.0         0.0          0.0   \n",
       "685  1500.0  1500.0  350.0  350.0         0.0         0.0          0.0   \n",
       "\n",
       "     p1_m2_usage  p2_m1_usage  p2_m2_usage  ...  p1/m2_elo  p1/m2_rd  \\\n",
       "681          0.0          0.0          0.0  ...     1500.0     350.0   \n",
       "682          0.0          0.0          0.0  ...     1500.0     350.0   \n",
       "683          0.0          0.0          0.0  ...     1500.0     350.0   \n",
       "684          0.0          0.0          0.0  ...     1500.0     350.0   \n",
       "685          0.0          0.0          0.0  ...     1500.0     350.0   \n",
       "\n",
       "     p1/m2_updates  p2/m1_elo  p2/m1_rd  p2/m1_updates  p2/m2_elo  p2/m2_rd  \\\n",
       "681            0.0     1500.0     350.0            0.0     1500.0     350.0   \n",
       "682            0.0     1500.0     350.0            0.0     1500.0     350.0   \n",
       "683            0.0     1500.0     350.0            0.0     1500.0     350.0   \n",
       "684            0.0     1500.0     350.0            0.0     1500.0     350.0   \n",
       "685            0.0     1500.0     350.0            0.0     1500.0     350.0   \n",
       "\n",
       "     p2/m2_updates  winner  \n",
       "681            0.0     0.0  \n",
       "682            0.0     0.0  \n",
       "683            0.0     1.0  \n",
       "684            0.0     1.0  \n",
       "685            0.0     1.0  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mini_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_x</th>\n",
       "      <th>game</th>\n",
       "      <th>tournament_key</th>\n",
       "      <th>winner_id</th>\n",
       "      <th>loser_id</th>\n",
       "      <th>p1_id</th>\n",
       "      <th>p2_id</th>\n",
       "      <th>p1_score</th>\n",
       "      <th>p2_score</th>\n",
       "      <th>valid_score</th>\n",
       "      <th>...</th>\n",
       "      <th>p1/m2_elo</th>\n",
       "      <th>p1/m2_rd</th>\n",
       "      <th>p1/m2_updates</th>\n",
       "      <th>p2/m1_elo</th>\n",
       "      <th>p2/m1_rd</th>\n",
       "      <th>p2/m1_updates</th>\n",
       "      <th>p2/m2_elo</th>\n",
       "      <th>p2/m2_rd</th>\n",
       "      <th>p2/m2_updates</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104675843</td>\n",
       "      <td>melee</td>\n",
       "      <td>mdva-invitational-2017-(challonge-mirror)</td>\n",
       "      <td>5620</td>\n",
       "      <td>Chillin</td>\n",
       "      <td>Chillin</td>\n",
       "      <td>5620</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1613.001275</td>\n",
       "      <td>70.600790</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>216.482281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104675844</td>\n",
       "      <td>melee</td>\n",
       "      <td>mdva-invitational-2017-(challonge-mirror)</td>\n",
       "      <td>Aglet</td>\n",
       "      <td>15634</td>\n",
       "      <td>Aglet</td>\n",
       "      <td>15634</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1637.162591</td>\n",
       "      <td>77.931512</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1408.326897</td>\n",
       "      <td>271.489866</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104675845</td>\n",
       "      <td>melee</td>\n",
       "      <td>mdva-invitational-2017-(challonge-mirror)</td>\n",
       "      <td>6126</td>\n",
       "      <td>1097</td>\n",
       "      <td>1097</td>\n",
       "      <td>6126</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1618.921028</td>\n",
       "      <td>81.744391</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104675846</td>\n",
       "      <td>melee</td>\n",
       "      <td>mdva-invitational-2017-(challonge-mirror)</td>\n",
       "      <td>1069</td>\n",
       "      <td>Chu</td>\n",
       "      <td>1069</td>\n",
       "      <td>Chu</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1279.022079</td>\n",
       "      <td>284.579342</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104675847</td>\n",
       "      <td>melee</td>\n",
       "      <td>mdva-invitational-2017-(challonge-mirror)</td>\n",
       "      <td>Rishi</td>\n",
       "      <td>Jerry</td>\n",
       "      <td>Jerry</td>\n",
       "      <td>Rishi</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       key_x   game                             tournament_key winner_id  \\\n",
       "0  104675843  melee  mdva-invitational-2017-(challonge-mirror)      5620   \n",
       "1  104675844  melee  mdva-invitational-2017-(challonge-mirror)     Aglet   \n",
       "2  104675845  melee  mdva-invitational-2017-(challonge-mirror)      6126   \n",
       "3  104675846  melee  mdva-invitational-2017-(challonge-mirror)      1069   \n",
       "4  104675847  melee  mdva-invitational-2017-(challonge-mirror)     Rishi   \n",
       "\n",
       "  loser_id    p1_id  p2_id  p1_score  p2_score  valid_score  ...    p1/m2_elo  \\\n",
       "0  Chillin  Chillin   5620         1         3         True  ...  1500.000000   \n",
       "1    15634    Aglet  15634         3         2         True  ...  1500.000000   \n",
       "2     1097     1097   6126         0         3         True  ...  1500.000000   \n",
       "3      Chu     1069    Chu         3         0         True  ...  1279.022079   \n",
       "4    Jerry    Jerry  Rishi         1         3         True  ...  1500.000000   \n",
       "\n",
       "     p1/m2_rd p1/m2_updates    p2/m1_elo    p2/m1_rd p2/m1_updates  \\\n",
       "0  350.000000           0.0  1613.001275   70.600790           5.0   \n",
       "1  350.000000           0.0  1637.162591   77.931512           5.0   \n",
       "2  350.000000           0.0  1618.921028   81.744391           4.0   \n",
       "3  284.579342           1.0  1500.000000  350.000000           0.0   \n",
       "4  350.000000           0.0  1500.000000  350.000000           0.0   \n",
       "\n",
       "     p2/m2_elo    p2/m2_rd  p2/m2_updates winner  \n",
       "0  1500.000000  216.482281            0.0    0.0  \n",
       "1  1408.326897  271.489866            1.0    1.0  \n",
       "2  1500.000000  350.000000            0.0    0.0  \n",
       "3  1500.000000  350.000000            0.0    1.0  \n",
       "4  1500.000000  350.000000            0.0    0.0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.merge(sets_df, dataset_mini_df, how='left', left_index=True, right_index=True)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the score so that we can do a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['regression_score'] = None\n",
    "dataset_df.loc[(dataset_df['valid_score']==True),'regression_score'] = dataset_df['p1_score'] / (dataset_df['p1_score'] + dataset_df['p2_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6650847227319329"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset_df['valid_score']==True).sum()/dataset_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    1193953\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing values in specified columns\n",
    "train_df = dataset_df.dropna(subset=dataset_df.columns[36:])\n",
    "\n",
    "((train_df['regression_score'] > .5) == train_df['winner']).value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify columns for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 key_x\n",
      "1 game\n",
      "2 tournament_key\n",
      "3 winner_id\n",
      "4 loser_id\n",
      "5 p1_id\n",
      "6 p2_id\n",
      "7 p1_score\n",
      "8 p2_score\n",
      "9 valid_score\n",
      "10 best_of\n",
      "11 location_names\n",
      "12 bracket_name\n",
      "13 bracket_order\n",
      "14 set_order\n",
      "15 game_data\n",
      "16 top_8\n",
      "17 top_8_location_names\n",
      "18 valid_top_8_bracket\n",
      "19 top_8_bracket_location_names\n",
      "20 major\n",
      "21 key_y\n",
      "22 start\n",
      "23 end\n",
      "24 start_week\n",
      "25 p1_characters\n",
      "26 p2_characters\n",
      "27 p1_consistent\n",
      "28 p2_consistent\n",
      "29 matchup_strings\n",
      "30 end_week\n",
      "31 players_have_history\n",
      "32 (p1/p2)_sorted\n",
      "33 (p1/p2)_was_sorted\n",
      "34 results_sorted\n",
      "35 results\n",
      "36 result_1\n",
      "37 result_2\n",
      "38 result_3\n",
      "39 result_4\n",
      "40 result_5\n",
      "41 result_6\n",
      "42 result_7\n",
      "43 result_8\n",
      "44 result_9\n",
      "45 result_10\n",
      "46 p1_elo\n",
      "47 p2_elo\n",
      "48 p1_rd\n",
      "49 p2_rd\n",
      "50 p1_updates\n",
      "51 p2_updates\n",
      "52 p1_m1_usage\n",
      "53 p1_m2_usage\n",
      "54 p2_m1_usage\n",
      "55 p2_m2_usage\n",
      "56 p1/m1/m1_elo\n",
      "57 p1/m1/m1_rd\n",
      "58 p1/m1/m1_updates\n",
      "59 p1/m1/m2_elo\n",
      "60 p1/m1/m2_rd\n",
      "61 p1/m1/m2_updates\n",
      "62 p1/m2/m1_elo\n",
      "63 p1/m2/m1_rd\n",
      "64 p1/m2/m1_updates\n",
      "65 p1/m2/m2_elo\n",
      "66 p1/m2/m2_rd\n",
      "67 p1/m2/m2_updates\n",
      "68 p2/m1/m1_elo\n",
      "69 p2/m1/m1_rd\n",
      "70 p2/m1/m1_updates\n",
      "71 p2/m1/m2_elo\n",
      "72 p2/m1/m2_rd\n",
      "73 p2/m1/m2_updates\n",
      "74 p2/m2/m1_elo\n",
      "75 p2/m2/m1_rd\n",
      "76 p2/m2/m1_updates\n",
      "77 p2/m2/m2_elo\n",
      "78 p2/m2/m2_rd\n",
      "79 p2/m2/m2_updates\n",
      "80 p1/m1/m1_alt_elo\n",
      "81 p1/m1/m1_alt_updates\n",
      "82 p1/m1/m2_alt_elo\n",
      "83 p1/m1/m2_alt_updates\n",
      "84 p1/m2/m1_alt_elo\n",
      "85 p1/m2/m1_alt_updates\n",
      "86 p1/m2/m2_alt_elo\n",
      "87 p1/m2/m2_alt_updates\n",
      "88 p2/m1/m1_alt_elo\n",
      "89 p2/m1/m1_alt_updates\n",
      "90 p2/m1/m2_alt_elo\n",
      "91 p2/m1/m2_alt_updates\n",
      "92 p2/m2/m1_alt_elo\n",
      "93 p2/m2/m1_alt_updates\n",
      "94 p2/m2/m2_alt_elo\n",
      "95 p2/m2/m2_alt_updates\n",
      "96 p2/m1/m1_alt_ref_elo\n",
      "97 p2/m1/m1_alt_ref_updates\n",
      "98 p2/m2/m2_alt_ref_elo\n",
      "99 p2/m2/m2_alt_ref_updates\n",
      "100 p1/m1/m1_alt_ref_elo\n",
      "101 p1/m1/m1_alt_ref_updates\n",
      "102 p1/m2/m2_alt_ref_elo\n",
      "103 p1/m2/m2_alt_ref_updates\n",
      "104 p1/m1_elo\n",
      "105 p1/m1_rd\n",
      "106 p1/m1_updates\n",
      "107 p1/m2_elo\n",
      "108 p1/m2_rd\n",
      "109 p1/m2_updates\n",
      "110 p2/m1_elo\n",
      "111 p2/m1_rd\n",
      "112 p2/m1_updates\n",
      "113 p2/m2_elo\n",
      "114 p2/m2_rd\n",
      "115 p2/m2_updates\n",
      "116 winner\n",
      "117 regression_score\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(dataset_df.columns):\n",
    "    print(i, col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset_df.columns[36:46]\n",
    "general_elo = dataset_df.columns[46:50]\n",
    "all_elo = dataset_df.columns[46:-2]\n",
    "features = dataset_df.columns[36:-2]\n",
    "targets=['winner','regression_score']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model just on the general elo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only general ELO accuracy: 76.172%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly general ELO accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinner\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mpreds)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_elo\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwinner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(df_test[all_elo]) \n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll ELO accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinner\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mpreds)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "# Train-test splits\n",
    "df_train, df_test = train_test_split(df, stratify=df['winner'], train_size=.8, random_state=42)\n",
    "\n",
    "# model = XGBClassifier()\n",
    "# model.fit(df_train[general_elo], df_train['winner'])\n",
    "# preds = model.predict(df_test[general_elo]) \n",
    "# print(f\"Only general ELO accuracy: {accuracy_score(df_test['winner'], preds):.3%}\")\n",
    "\n",
    "# model = XGBClassifier()\n",
    "# model.fit(df_train[all_elo], df_train['winner'])\n",
    "# preds = model.predict(df_test[all_elo]) \n",
    "# print(f\"All ELO accuracy: {accuracy_score(df_test['winner'], preds):.3%}\")\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(df_train[features], df_train['winner'])\n",
    "preds = model.predict(df_test[features]) \n",
    "print(f\"All features accuracy: {accuracy_score(df_test['winner'], preds):.3%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on all data:\n",
      "All accuracy: 76.730%\n",
      "With results accuracy: 78.603%\n",
      "Without results accuracy: 75.787%\n",
      "\n",
      "Trained with results only:\n",
      "All accuracy: 72.621%\n",
      "With results accuracy: 78.388%\n",
      "Without results accuracy: 69.720%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "# Train-test splits\n",
    "df_train, df_test = train_test_split(df, stratify=df['winner'], train_size=.8, random_state=42)\n",
    "\n",
    "df_train_results = df_train[df_train['result_1']!=.5]\n",
    "df_test_results = df_test[df_test['result_1']!=.5]\n",
    "\n",
    "df_train_no_results = df_train[df_train['result_1']==.5]\n",
    "df_test_no_results = df_test[df_test['result_1']==.5]\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(df_train[features], df_train['winner'])\n",
    "preds_all = model.predict(df_test[features])\n",
    "preds_results = model.predict(df_test_results[features])\n",
    "preds_no_results = model.predict(df_test_no_results[features])\n",
    "\n",
    "print(\"Trained on all data:\")\n",
    "print(f\"All accuracy: {accuracy_score(df_test['winner'], preds_all):.3%}\")\n",
    "print(f\"With results accuracy: {accuracy_score(df_test_results['winner'], preds_results):.3%}\")\n",
    "print(f\"Without results accuracy: {accuracy_score(df_test_no_results['winner'], preds_no_results):.3%}\")\n",
    "print()\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(df_train_results[features], df_train_results['winner'])\n",
    "preds_all = model.predict(df_test[features])\n",
    "preds_results = model.predict(df_test_results[features])\n",
    "preds_no_results = model.predict(df_test_no_results[features])\n",
    "\n",
    "print(\"Trained with results only:\")\n",
    "print(f\"All accuracy: {accuracy_score(df_test['winner'], preds_all):.3%}\")\n",
    "print(f\"With results accuracy: {accuracy_score(df_test_results['winner'], preds_results):.3%}\")\n",
    "print(f\"Without results accuracy: {accuracy_score(df_test_no_results['winner'], preds_no_results):.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features accuracy: 76.403%\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(df_train[general_elo.append(results)], df_train['winner'])\n",
    "preds = model.predict(df_test[general_elo.append(results)]) \n",
    "print(f\"All features accuracy: {accuracy_score(df_test['winner'], preds):.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Results on Classification Test Set:\n",
      " - Accuracy: 76.73%\n",
      " - Log-Loss: 0.4833\n",
      "\n",
      "Regressor Results on Classification Test Set:\n",
      " - Accuracy: 76.72%\n",
      "\n",
      "Classifier Results on Regression Test Set:\n",
      " - Accuracy: 79.85%\n",
      " - Log-Loss: 0.4289\n",
      "\n",
      "Regressor Results on Regression Test Set:\n",
      " - Accuracy: 79.95%\n",
      " - MSE: 0.0952\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n",
    "from scipy.special import expit  # For sigmoid function\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "# Train-test splits\n",
    "df_train, df_test = train_test_split(df, stratify=df['winner'], train_size=.8, random_state=42)\n",
    "\n",
    "X_train_c = df_train[features]\n",
    "X_test_c = df_test[features]\n",
    "y_train_c = df_train['winner']\n",
    "y_test_c = df_test['winner']\n",
    "\n",
    "# Filter out the samples with valid scores\n",
    "df_train_r = df_train.dropna(subset=['regression_score'])\n",
    "df_test_r = df_test.dropna(subset=['regression_score'])\n",
    "\n",
    "X_train_r = df_train_r[features]\n",
    "X_test_r = df_test_r[features]\n",
    "y_train_r = df_train_r['regression_score']\n",
    "y_test_r = df_test_r['regression_score']\n",
    "\n",
    "# Train the Classifier\n",
    "model_classifier = XGBClassifier()\n",
    "model_classifier.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Train the Regressor\n",
    "model_regressor = XGBRegressor()\n",
    "model_regressor.fit(X_train_r, y_train_r)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "probs_c_test_c = model_classifier.predict_proba(X_test_c)[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds_c_test_c = model_classifier.predict(X_test_c)\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy_c_test_c = accuracy_score(y_test_c, preds_c_test_c)\n",
    "log_loss_c_test_c = log_loss(y_test_c, probs_c_test_c)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions for Classifier on regression test set\n",
    "probs_c_test_r = model_classifier.predict_proba(X_test_r)[:, 1]\n",
    "preds_c_test_r = model_classifier.predict(X_test_r)\n",
    "\n",
    "# Metrics for Classifier on regression test set\n",
    "accuracy_c_test_r = accuracy_score(df_test_r['winner'], preds_c_test_r)\n",
    "log_loss_c_test_r = log_loss(df_test_r['winner'], probs_c_test_r)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions for Regressor on classification test set\n",
    "preds_r_test_c = model_regressor.predict(X_test_c)  # Regressor predictions on classification test set\n",
    "\n",
    "# Metrics for Regressor on classification test set\n",
    "accuracy_r_test_c = accuracy_score(y_test_c, preds_r_test_c > 0.5)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions for Regressor on regression test set\n",
    "preds_r_test_r = model_regressor.predict(X_test_r)\n",
    "\n",
    "# Metrics for Regressor on regression test set\n",
    "accuracy_r_test_r = accuracy_score(df_test_r['winner'], preds_r_test_r > 0.5)\n",
    "mse_r_test_r = mean_squared_error(y_test_r, preds_r_test_r)\n",
    "\n",
    "\n",
    "\n",
    "# Display Results\n",
    "print(\"Classifier Results on Classification Test Set:\")\n",
    "print(f\" - Accuracy: {accuracy_c_test_c:.2%}\")\n",
    "print(f\" - Log-Loss: {log_loss_c_test_c:.4f}\")\n",
    "\n",
    "print(\"\\nRegressor Results on Classification Test Set:\")\n",
    "print(f\" - Accuracy: {accuracy_r_test_c:.2%}\")\n",
    "# print(f\" - Log-Loss (derived): {log_loss_r_c_test_c:.4f}\")\n",
    "\n",
    "print(\"\\nClassifier Results on Regression Test Set:\")\n",
    "print(f\" - Accuracy: {accuracy_c_test_r:.2%}\")\n",
    "print(f\" - Log-Loss: {log_loss_c_test_r:.4f}\")\n",
    "\n",
    "print(\"\\nRegressor Results on Regression Test Set:\")\n",
    "print(f\" - Accuracy: {accuracy_r_test_r:.2%}\")\n",
    "print(f\" - MSE: {mse_r_test_r:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Results on Top 8 Test Set:\n",
      " - Accuracy: 73.14%\n",
      " - Log-Loss: 0.5356\n",
      "\n",
      "Regressor Results on Top 8 Test Set:\n",
      " - Accuracy: 73.03%\n"
     ]
    }
   ],
   "source": [
    "testing_df = df_test[df_test['valid_top_8_bracket']==True]\n",
    "\n",
    "\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "probs_c_test_c = model_classifier.predict_proba(testing_df[features])[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds_c_test_c = model_classifier.predict(testing_df[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy_c_test_c = accuracy_score(testing_df['winner'], preds_c_test_c)\n",
    "log_loss_c_test_c = log_loss(testing_df['winner'], probs_c_test_c)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions for Regressor on classification test set\n",
    "preds_r_test_c = model_regressor.predict(testing_df[features])  # Regressor predictions on classification test set\n",
    "\n",
    "# Metrics for Regressor on classification test set\n",
    "accuracy_r_test_c = accuracy_score(testing_df['winner'], preds_r_test_c > 0.5)\n",
    "\n",
    "\n",
    "\n",
    "# # Predictions for Regressor on regression test set\n",
    "# preds_r_test_r = model_regressor.predict(X_test_r)\n",
    "\n",
    "# # Metrics for Regressor on regression test set\n",
    "# accuracy_r_test_r = accuracy_score(df_test_r['winner'], preds_r_test_r > 0.5)\n",
    "# mse_r_test_r = mean_squared_error(y_test_r, preds_r_test_r)\n",
    "\n",
    "\n",
    "\n",
    "# Display Results\n",
    "print(\"Classifier Results on Top 8 Test Set:\")\n",
    "print(f\" - Accuracy: {accuracy_c_test_c:.2%}\")\n",
    "print(f\" - Log-Loss: {log_loss_c_test_c:.4f}\")\n",
    "\n",
    "print(\"\\nRegressor Results on Top 8 Test Set:\")\n",
    "print(f\" - Accuracy: {accuracy_r_test_c:.2%}\")\n",
    "# print(f\" - Log-Loss (derived): {log_loss_r_c_test_c:.4f}\")\n",
    "\n",
    "# print(\"\\nClassifier Results on Regression Test Set:\")\n",
    "# print(f\" - Accuracy: {accuracy_c_test_r:.2%}\")\n",
    "# print(f\" - Log-Loss: {log_loss_c_test_r:.4f}\")\n",
    "\n",
    "# print(\"\\nRegressor Results on Regression Test Set:\")\n",
    "# print(f\" - Accuracy: {accuracy_r_test_r:.2%}\")\n",
    "# print(f\" - MSE: {mse_r_test_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Results on Classification Test Set:\n",
      " - Accuracy: 76.70%\n",
      " - Log-Loss: 0.4999\n",
      "\n",
      "Regressor Results on Classification Test Set:\n",
      " - Accuracy: 76.72%\n",
      "\n",
      "Classifier Results on Regression Test Set:\n",
      " - Accuracy: 79.93%\n",
      " - Log-Loss: 0.4181\n",
      "\n",
      "Regressor Results on Regression Test Set:\n",
      " - Accuracy: 79.95%\n",
      " - MSE: 0.0952\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n",
    "from scipy.special import expit  # For sigmoid function\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "# Train-test splits\n",
    "df_train, df_test = train_test_split(df, stratify=df['winner'], train_size=.8, random_state=42)\n",
    "\n",
    "X_train_c = df_train[features]\n",
    "X_test_c = df_test[features]\n",
    "y_train_c = df_train['winner']\n",
    "y_test_c = df_test['winner']\n",
    "\n",
    "# Filter out the samples with valid scores\n",
    "df_train_r = df_train.dropna(subset=['regression_score'])\n",
    "df_test_r = df_test.dropna(subset=['regression_score'])\n",
    "\n",
    "X_train_r = df_train_r[features]\n",
    "X_test_r = df_test_r[features]\n",
    "y_train_r = df_train_r['regression_score']\n",
    "y_test_r = df_test_r['regression_score']\n",
    "\n",
    "# Train the Classifier\n",
    "model_classifier = XGBClassifier()\n",
    "model_classifier.fit(X_train_r, df_train_r['winner'])\n",
    "\n",
    "# Train the Regressor\n",
    "model_regressor = XGBRegressor()\n",
    "model_regressor.fit(X_train_r, y_train_r)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "probs_c_test_c = model_classifier.predict_proba(X_test_c)[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds_c_test_c = model_classifier.predict(X_test_c)\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy_c_test_c = accuracy_score(y_test_c, preds_c_test_c)\n",
    "log_loss_c_test_c = log_loss(y_test_c, probs_c_test_c)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions for Classifier on regression test set\n",
    "probs_c_test_r = model_classifier.predict_proba(X_test_r)[:, 1]\n",
    "preds_c_test_r = model_classifier.predict(X_test_r)\n",
    "\n",
    "# Metrics for Classifier on regression test set\n",
    "accuracy_c_test_r = accuracy_score(df_test_r['winner'], preds_c_test_r)\n",
    "log_loss_c_test_r = log_loss(df_test_r['winner'], probs_c_test_r)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions for Regressor on classification test set\n",
    "preds_r_test_c = model_regressor.predict(X_test_c)  # Regressor predictions on classification test set\n",
    "\n",
    "# Metrics for Regressor on classification test set\n",
    "accuracy_r_test_c = accuracy_score(y_test_c, preds_r_test_c > 0.5)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions for Regressor on regression test set\n",
    "preds_r_test_r = model_regressor.predict(X_test_r)\n",
    "\n",
    "# Metrics for Regressor on regression test set\n",
    "accuracy_r_test_r = accuracy_score(df_test_r['winner'], preds_r_test_r > 0.5)\n",
    "mse_r_test_r = mean_squared_error(y_test_r, preds_r_test_r)\n",
    "\n",
    "\n",
    "\n",
    "# Display Results\n",
    "print(\"Classifier Results on Classification Test Set:\")\n",
    "print(f\" - Accuracy: {accuracy_c_test_c:.2%}\")\n",
    "print(f\" - Log-Loss: {log_loss_c_test_c:.4f}\")\n",
    "\n",
    "print(\"\\nRegressor Results on Classification Test Set:\")\n",
    "print(f\" - Accuracy: {accuracy_r_test_c:.2%}\")\n",
    "# print(f\" - Log-Loss (derived): {log_loss_r_c_test_c:.4f}\")\n",
    "\n",
    "print(\"\\nClassifier Results on Regression Test Set:\")\n",
    "print(f\" - Accuracy: {accuracy_c_test_r:.2%}\")\n",
    "print(f\" - Log-Loss: {log_loss_c_test_r:.4f}\")\n",
    "\n",
    "print(\"\\nRegressor Results on Regression Test Set:\")\n",
    "print(f\" - Accuracy: {accuracy_r_test_r:.2%}\")\n",
    "print(f\" - MSE: {mse_r_test_r:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.9072%\n",
      "Log-Loss: 0.5602\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n",
    "from scipy.special import expit  # For sigmoid function\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "valid_score_false_df = df[df['valid_score']==False]\n",
    "# Train-test splits\n",
    "df_train, df_test = train_test_split(valid_score_false_df, stratify=valid_score_false_df['winner'], train_size=.8, random_state=42)\n",
    "\n",
    "# Train the Classifier\n",
    "model_classifier = XGBClassifier()\n",
    "model_classifier.fit(df_train[features], df_train['winner'])\n",
    "\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "probs = model_classifier.predict_proba(df_test[features])[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds = model_classifier.predict(df_test[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy = accuracy_score(df_test['winner'], preds)\n",
    "log_loss_score = log_loss(df_test['winner'], probs)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4%}\")\n",
    "print(f\"Log-Loss: {log_loss_score:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Trained on valid and not valid score\n",
      "Accuracy when test set has no valid score\n",
      "Accuracy: 70.64%\n",
      "Log-Loss: 0.5911\n",
      "\n",
      "Accuracy when test set has a valid score\n",
      "Accuracy: 79.96%\n",
      "Log-Loss: 0.4269\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n",
    "from scipy.special import expit  # For sigmoid function\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "valid_score_false_df = df[df['valid_score']==False]\n",
    "valid_score_true_df = df[df['valid_score']==True]\n",
    "# Train-test splits\n",
    "df_false_train, df_false_test = train_test_split(valid_score_false_df, stratify=valid_score_false_df['winner'], train_size=.8, random_state=42)\n",
    "df_true_train, df_true_test = train_test_split(valid_score_true_df, stratify=valid_score_true_df['winner'], train_size=.8, random_state=42)\n",
    "\n",
    "df_train = pd.concat([df_false_train, df_true_train])\n",
    "# df_train = df_true_train\n",
    "# df_train = df_false_train\n",
    "\n",
    "# Train the Classifier\n",
    "model_classifier = XGBClassifier()\n",
    "model_classifier.fit(df_train[features], df_train['winner'])\n",
    "\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "probs = model_classifier.predict_proba(df_false_test[features])[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds = model_classifier.predict(df_false_test[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy = accuracy_score(df_false_test['winner'], preds)\n",
    "log_loss_score = log_loss(df_false_test['winner'], probs)\n",
    "\n",
    "print(\"Classifier Trained on valid and not valid score\")\n",
    "print(\"Accuracy when test set has no valid score\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Log-Loss: {log_loss_score:.4f}\")\n",
    "print()\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "probs = model_classifier.predict_proba(df_true_test[features])[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds = model_classifier.predict(df_true_test[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy = accuracy_score(df_true_test['winner'], preds)\n",
    "log_loss_score = log_loss(df_true_test['winner'], probs)\n",
    "\n",
    "print(\"Accuracy when test set has a valid score\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Log-Loss: {log_loss_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Trained on valid and not valid score\n",
      "Accuracy and Log-Loss for df_false_test:\n",
      " - Accuracy: 70.6377%\n",
      " - Log-Loss: 0.5911\n",
      "\n",
      "Accuracy and Log-Loss for df_true_test:\n",
      " - Accuracy: 79.9632%\n",
      " - Log-Loss: 0.4269\n",
      "\n",
      "Combined Accuracy and Log-Loss across both test sets:\n",
      " - Combined Accuracy: 76.8399%\n",
      " - Combined Log-Loss: 0.4819\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "# Split data into valid_score=True and valid_score=False subsets\n",
    "valid_score_false_df = df[df['valid_score'] == False]\n",
    "valid_score_true_df = df[df['valid_score'] == True]\n",
    "\n",
    "# Train-test splits for each subset\n",
    "df_false_train, df_false_test = train_test_split(\n",
    "    valid_score_false_df, stratify=valid_score_false_df['winner'], train_size=0.8, random_state=42\n",
    ")\n",
    "df_true_train, df_true_test = train_test_split(\n",
    "    valid_score_true_df, stratify=valid_score_true_df['winner'], train_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "# Combine training sets\n",
    "df_train = pd.concat([df_false_train, df_true_train])\n",
    "\n",
    "# Train the Classifier\n",
    "model_classifier = XGBClassifier()\n",
    "model_classifier.fit(df_train[features], df_train['winner'])\n",
    "\n",
    "# Predictions and metrics for df_false_test\n",
    "probs_false = model_classifier.predict_proba(df_false_test[features])[:, 1]\n",
    "preds_false = model_classifier.predict(df_false_test[features])\n",
    "accuracy_false = accuracy_score(df_false_test['winner'], preds_false)\n",
    "log_loss_false = log_loss(df_false_test['winner'], probs_false)\n",
    "\n",
    "# Predictions and metrics for df_true_test\n",
    "probs_true = model_classifier.predict_proba(df_true_test[features])[:, 1]\n",
    "preds_true = model_classifier.predict(df_true_test[features])\n",
    "accuracy_true = accuracy_score(df_true_test['winner'], preds_true)\n",
    "log_loss_true = log_loss(df_true_test['winner'], probs_true)\n",
    "\n",
    "# Combine predictions and probabilities across both sets\n",
    "y_test_combined = pd.concat([df_false_test['winner'], df_true_test['winner']])\n",
    "probs_combined = np.concatenate([probs_false, probs_true])\n",
    "preds_combined = np.concatenate([preds_false, preds_true])\n",
    "\n",
    "# Calculate combined metrics\n",
    "accuracy_combined = accuracy_score(y_test_combined, preds_combined)\n",
    "log_loss_combined = log_loss(y_test_combined, probs_combined)\n",
    "\n",
    "# Print results\n",
    "print(\"Classifier Trained on valid and not valid score\")\n",
    "print(\"Accuracy and Log-Loss for df_false_test:\")\n",
    "print(f\" - Accuracy: {accuracy_false:.4%}\")\n",
    "print(f\" - Log-Loss: {log_loss_false:.4f}\")\n",
    "print()\n",
    "print(\"Accuracy and Log-Loss for df_true_test:\")\n",
    "print(f\" - Accuracy: {accuracy_true:.4%}\")\n",
    "print(f\" - Log-Loss: {log_loss_true:.4f}\")\n",
    "print()\n",
    "print(\"Combined Accuracy and Log-Loss across both test sets:\")\n",
    "print(f\" - Combined Accuracy: {accuracy_combined:.4%}\")\n",
    "print(f\" - Combined Log-Loss: {log_loss_combined:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Trained on valid score\n",
      "Accuracy when test set has no valid score\n",
      "Accuracy: 70.2494%\n",
      "Log-Loss: 0.6606\n",
      "\n",
      "Accuracy when test set has a valid score\n",
      "Accuracy: 80.0931%\n",
      "Log-Loss: 0.4161\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n",
    "from scipy.special import expit  # For sigmoid function\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "valid_score_false_df = df[df['valid_score']==False]\n",
    "valid_score_true_df = df[df['valid_score']==True]\n",
    "# Train-test splits\n",
    "df_false_train, df_false_test = train_test_split(valid_score_false_df, stratify=valid_score_false_df['winner'], train_size=.8, random_state=42)\n",
    "df_true_train, df_true_test = train_test_split(valid_score_true_df, stratify=valid_score_true_df['winner'], train_size=.8, random_state=42)\n",
    "\n",
    "# df_train = pd.concat([df_false_train, df_true_train])\n",
    "df_train = df_true_train\n",
    "# df_train = df_false_train\n",
    "\n",
    "# Train the Classifier\n",
    "model_classifier = XGBClassifier()\n",
    "model_classifier.fit(df_train[features], df_train['winner'])\n",
    "\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "probs = model_classifier.predict_proba(df_false_test[features])[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds = model_classifier.predict(df_false_test[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy = accuracy_score(df_false_test['winner'], preds)\n",
    "log_loss_score = log_loss(df_false_test['winner'], probs)\n",
    "\n",
    "print(\"Classifier Trained on valid score\")\n",
    "print(\"Accuracy when test set has no valid score\")\n",
    "print(f\"Accuracy: {accuracy:.4%}\")\n",
    "print(f\"Log-Loss: {log_loss_score:.4f}\")\n",
    "print()\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "probs = model_classifier.predict_proba(df_true_test[features])[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds = model_classifier.predict(df_true_test[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy = accuracy_score(df_true_test['winner'], preds)\n",
    "log_loss_score = log_loss(df_true_test['winner'], probs)\n",
    "\n",
    "print(\"Accuracy when test set has a valid score\")\n",
    "print(f\"Accuracy: {accuracy:.4%}\")\n",
    "print(f\"Log-Loss: {log_loss_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Trained on no valid score\n",
      "Accuracy when test set has no valid score\n",
      "Accuracy: 70.91%\n",
      "Log-Loss: 0.5602\n",
      "\n",
      "Accuracy when test set has a valid score\n",
      "Accuracy: 77.18%\n",
      "Log-Loss: 0.5027\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n",
    "from scipy.special import expit  # For sigmoid function\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "valid_score_false_df = df[df['valid_score']==False]\n",
    "valid_score_true_df = df[df['valid_score']==True]\n",
    "# Train-test splits\n",
    "df_false_train, df_false_test = train_test_split(valid_score_false_df, stratify=valid_score_false_df['winner'], train_size=.8, random_state=42)\n",
    "df_true_train, df_true_test = train_test_split(valid_score_true_df, stratify=valid_score_true_df['winner'], train_size=.8, random_state=42)\n",
    "\n",
    "# df_train = pd.concat([df_false_train, df_true_train])\n",
    "# df_train = df_true_train\n",
    "df_train = df_false_train\n",
    "\n",
    "# Train the Classifier\n",
    "model_classifier = XGBClassifier()\n",
    "model_classifier.fit(df_train[features], df_train['winner'])\n",
    "\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "probs = model_classifier.predict_proba(df_false_test[features])[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds = model_classifier.predict(df_false_test[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy = accuracy_score(df_false_test['winner'], preds)\n",
    "log_loss_score = log_loss(df_false_test['winner'], probs)\n",
    "\n",
    "print(\"Classifier Trained on no valid score\")\n",
    "print(\"Accuracy when test set has no valid score\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Log-Loss: {log_loss_score:.4f}\")\n",
    "print()\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "probs = model_classifier.predict_proba(df_true_test[features])[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds = model_classifier.predict(df_true_test[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy = accuracy_score(df_true_test['winner'], preds)\n",
    "log_loss_score = log_loss(df_true_test['winner'], probs)\n",
    "\n",
    "print(\"Accuracy when test set has a valid score\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Log-Loss: {log_loss_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy a classifier predicts the corresponding valid_score that it was trained on\n",
      "Accuracy: 77.0165%\n",
      "Log-Loss: 0.4644\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import numpy as np\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "# Split based on 'valid_score'\n",
    "valid_score_false_df = df[df['valid_score'] == False]\n",
    "valid_score_true_df = df[df['valid_score'] == True]\n",
    "\n",
    "# Train-test splits\n",
    "df_false_train, df_false_test = train_test_split(\n",
    "    valid_score_false_df, stratify=valid_score_false_df['winner'], train_size=0.8, random_state=42\n",
    ")\n",
    "df_true_train, df_true_test = train_test_split(\n",
    "    valid_score_true_df, stratify=valid_score_true_df['winner'], train_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "# Train and predict on false valid_score data\n",
    "model_classifier = XGBClassifier()\n",
    "model_classifier.fit(df_false_train[features], df_false_train['winner'])\n",
    "\n",
    "probs_false = model_classifier.predict_proba(df_false_test[features])[:, 1]  # Probability for class 1\n",
    "preds_false = model_classifier.predict(df_false_test[features])\n",
    "\n",
    "# Train and predict on true valid_score data\n",
    "model_classifier = XGBClassifier()\n",
    "model_classifier.fit(df_true_train[features], df_true_train['winner'])\n",
    "\n",
    "probs_true = model_classifier.predict_proba(df_true_test[features])[:, 1]  # Probability for class 1\n",
    "preds_true = model_classifier.predict(df_true_test[features])\n",
    "\n",
    "# Combine predictions and probabilities\n",
    "y_test = pd.concat([df_false_test, df_true_test])['winner']\n",
    "probs = np.concatenate([probs_false, probs_true])  # Probabilities for log_loss\n",
    "preds = np.concatenate([preds_false, preds_true])  # Predictions for accuracy\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "log_loss_score = log_loss(y_test, probs)\n",
    "\n",
    "print(\"Accuracy a classifier predicts the corresponding valid_score that it was trained on\")\n",
    "print(f\"Accuracy: {accuracy:.4%}\")\n",
    "print(f\"Log-Loss: {log_loss_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor Trained on some valid score\n",
      "Accuracy when test set has no valid score:\n",
      "Accuracy: 70.2053%\n",
      "\n",
      "Accuracy when test set has a valid score:\n",
      "Accuracy: 79.9992%\n",
      "\n",
      "Combined Accuracy across both test sets:\n",
      "Accuracy: 76.7190%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "# Split data into subsets based on `valid_score`\n",
    "valid_score_false_df = df[df['valid_score'] == False]\n",
    "valid_score_true_df = df[df['valid_score'] == True]\n",
    "\n",
    "# Train-test splits for each subset\n",
    "df_false_train, df_false_test = train_test_split(\n",
    "    valid_score_false_df, stratify=valid_score_false_df['winner'], train_size=0.8, random_state=44\n",
    ")\n",
    "df_true_train, df_true_test = train_test_split(\n",
    "    valid_score_true_df, stratify=valid_score_true_df['winner'], train_size=0.8, random_state=44\n",
    ")\n",
    "\n",
    "# Use only `df_true_train` for training the regressor\n",
    "df_train = df_true_train\n",
    "\n",
    "# Train the Regressor\n",
    "model_regressor = XGBRegressor()\n",
    "model_regressor.fit(df_train[features], df_train['regression_score'])\n",
    "\n",
    "# Predictions and accuracy for df_false_test\n",
    "preds_false = model_regressor.predict(df_false_test[features])\n",
    "accuracy_false = accuracy_score(df_false_test['winner'], preds_false > 0.5)\n",
    "\n",
    "print(\"Regressor Trained on some valid score\")\n",
    "print(\"Accuracy when test set has no valid score:\")\n",
    "print(f\"Accuracy: {accuracy_false:.4%}\")\n",
    "print()\n",
    "\n",
    "# Predictions and accuracy for df_true_test\n",
    "preds_true = model_regressor.predict(df_true_test[features])\n",
    "accuracy_true = accuracy_score(df_true_test['winner'], preds_true > 0.5)\n",
    "\n",
    "print(\"Accuracy when test set has a valid score:\")\n",
    "print(f\"Accuracy: {accuracy_true:.4%}\")\n",
    "print()\n",
    "\n",
    "# Combined accuracy across both test sets\n",
    "y_test_combined = pd.concat([df_false_test['winner'], df_true_test['winner']])\n",
    "preds_combined = np.concatenate([preds_false, preds_true])\n",
    "accuracy_combined = accuracy_score(y_test_combined, preds_combined > 0.5)\n",
    "\n",
    "print(\"Combined Accuracy across both test sets:\")\n",
    "print(f\"Accuracy: {accuracy_combined:.4%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_false: 70.9072%\n",
      "accuracy_true: 80.0981%\n",
      "Accuracy: 77.0198%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n",
    "from scipy.special import expit  # For sigmoid function\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "valid_score_false_df = df[df['valid_score']==False]\n",
    "valid_score_true_df = df[df['valid_score']==True]\n",
    "# Train-test splits\n",
    "df_false_train, df_false_test = train_test_split(valid_score_false_df, stratify=valid_score_false_df['winner'], train_size=.8, random_state=42)\n",
    "df_true_train, df_true_test = train_test_split(valid_score_true_df, stratify=valid_score_true_df['winner'], train_size=.8, random_state=42)\n",
    "\n",
    "# df_train = pd.concat([df_false_train, df_true_train])\n",
    "# df_train = df_true_train\n",
    "df_train = df_false_train\n",
    "\n",
    "# Train the Classifier\n",
    "model_classifier = XGBClassifier()\n",
    "model_classifier.fit(df_train[features], df_train['winner'])\n",
    "\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "preds_false = model_classifier.predict(df_false_test[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy_false = accuracy_score(df_false_test['winner'], preds_false)\n",
    "df_train = df_true_train\n",
    "\n",
    "model_regressor = XGBRegressor()\n",
    "model_regressor.fit(df_train[features], df_train['regression_score'])\n",
    "\n",
    "preds_true = model_regressor.predict(df_true_test[features]) > .5\n",
    "accuracy_true = accuracy_score(df_true_test['winner'], preds_true)\n",
    "\n",
    "y_test = pd.concat([df_false_test, df_true_test])['winner']\n",
    "preds = np.concatenate([preds_false, preds_true])  # Probabilities for log_loss\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "print(f\"accuracy_false: {accuracy_false:.4%}\")\n",
    "print(f\"accuracy_true: {accuracy_true:.4%}\")\n",
    "print(f\"Accuracy: {accuracy:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor Trained on some valid score\n",
      "Accuracy when test set has no valid score\n",
      "Accuracy: 70.4132%\n",
      "\n",
      "Accuracy when test set has a valid score\n",
      "Accuracy: 80.0981%\n",
      "\n",
      "Accuracy across both:\n",
      "Accuracy: 76.8544%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n",
    "from scipy.special import expit  # For sigmoid function\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df = dataset_df.dropna(subset=dataset_df.columns[36:-1])\n",
    "\n",
    "valid_score_false_df = df[df['valid_score']==False]\n",
    "valid_score_true_df = df[df['valid_score']==True]\n",
    "# Train-test splits\n",
    "df_false_train, df_false_test = train_test_split(valid_score_false_df, stratify=valid_score_false_df['winner'], train_size=.8, random_state=42)\n",
    "df_true_train, df_true_test = train_test_split(valid_score_true_df, stratify=valid_score_true_df['winner'], train_size=.8, random_state=42)\n",
    "\n",
    "# df_train = pd.concat([df_false_train, df_true_train])\n",
    "df_train = df_true_train\n",
    "# df_train = df_false_train\n",
    "\n",
    "# Train the Classifier\n",
    "model_classifier = XGBRegressor()\n",
    "model_classifier.fit(df_train[features], df_train['regression_score'])\n",
    "\n",
    "preds_false = model_classifier.predict(df_false_test[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy = accuracy_score(df_false_test['winner'], preds_false>.5)\n",
    "# log_loss_score = log_loss(df_false_test['winner'], probs)\n",
    "\n",
    "print(\"Regressor Trained on some valid score\")\n",
    "print(\"Accuracy when test set has no valid score\")\n",
    "print(f\"Accuracy: {accuracy:.4%}\")\n",
    "# print(f\"Log-Loss: {log_loss_score:.4f}\")\n",
    "print()\n",
    "\n",
    "# Predictions for Classifier on classification test set\n",
    "# probs = model_classifier.predict_proba(df_true_test[features])[:, 1]  # Probability for class 1 (player 1 wins)\n",
    "preds_true = model_classifier.predict(df_true_test[features])\n",
    "\n",
    "# Metrics for Classifier on classification test set\n",
    "accuracy = accuracy_score(df_true_test['winner'], preds_true>.5)\n",
    "# log_loss_score = log_loss(df_true_test['winner'], probs)\n",
    "\n",
    "print(\"Accuracy when test set has a valid score\")\n",
    "print(f\"Accuracy: {accuracy:.4%}\")\n",
    "# print(f\"Log-Loss: {log_loss_score:.4f}\")\n",
    "\n",
    "y_test = pd.concat([df_false_test, df_true_test])['winner']\n",
    "preds = np.concatenate([preds_false, preds_true])  # Probabilities for log_loss\n",
    "accuracy = accuracy_score(y_test, preds >.5)\n",
    "print()\n",
    "print(\"Accuracy across both:\")\n",
    "print(f\"Accuracy: {accuracy:.4%}\")\n",
    "# print(f\"Log-Loss: {log_loss_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645.9201559999982"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-preds.shape[0]*(.768399-.770198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 19:23:27,024] A new study created in memory with name: no-name-4b161953-a507-4409-8f56-9fcbf0c2c5c9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b1712e82da46469bb7befd50653532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2024-11-23 19:23:34,354] Trial 0 failed with parameters: {'max_depth': 13, 'learning_rate': 0.24981048744100315, 'n_estimators': 700, 'subsample': 0.7901309435654792, 'colsample_bytree': 0.6560449755412733, 'gamma': 0.3102013628173872, 'min_child_weight': 6, 'lambda': 0.6632431546429975, 'alpha': 2.8932031398538576} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_18446/4098588530.py\", line 22, in objective\n",
      "    model.fit(X_train_r, y_train_r)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1108, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 2101, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2024-11-23 19:23:34,355] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Minimize MSE\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Run the optimization\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 50 trials or 1-hour time limit\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Display the best hyperparameters and the corresponding MSE\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model using the current set of hyperparameters\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_r\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Predictions for Regressor on regression test set\u001b[39;00m\n\u001b[1;32m     25\u001b[0m preds_r_test_c \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_c)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 13, step=2),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000, step=50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    # Train the model using the current set of hyperparameters\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train_r, y_train_r)\n",
    "    \n",
    "    # Predictions for Regressor on regression test set\n",
    "    preds_r_test_c = model.predict(X_test_c)\n",
    "    \n",
    "    # Metrics for Regressor on regression test set\n",
    "    accuracy_r_test_c = accuracy_score(y_test_c, preds_r_test_c > 0.5)\n",
    "    \n",
    "    return accuracy_r_test_c\n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study(direction=\"maximize\")  # Minimize MSE\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(objective, n_trials=50, timeout=600, show_progress_bar=True)  # 50 trials or 1-hour time limit\n",
    "\n",
    "# Display the best hyperparameters and the corresponding MSE\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "print(\"Best MSE:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with 3 subplots in a single row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 12), sharey=False)  # 1 row, 3 columns\n",
    "\n",
    "# Plot Weight\n",
    "importance_df.sort_values(by='Weight', ascending=False).set_index('Feature')['Weight'].plot(\n",
    "    kind='barh', ax=axes[0], title=\"Feature Importance - Weight (Descending)\"\n",
    ")\n",
    "axes[0].set_xlabel(\"Weight Score\")\n",
    "axes[0].set_ylabel(\"Features\")\n",
    "axes[0].invert_yaxis()  # Invert y-axis for descending order\n",
    "\n",
    "# Plot Gain\n",
    "importance_df.sort_values(by='Gain', ascending=False).set_index('Feature')['Gain'].plot(\n",
    "    kind='barh', ax=axes[1], title=\"Feature Importance - Gain (Descending)\"\n",
    ")\n",
    "axes[1].set_xlabel(\"Gain Score\")\n",
    "axes[1].set_ylabel(\"Features\")\n",
    "axes[1].invert_yaxis()  # Invert y-axis for descending order\n",
    "\n",
    "# Plot Cover\n",
    "importance_df.sort_values(by='Cover', ascending=False).set_index('Feature')['Cover'].plot(\n",
    "    kind='barh', ax=axes[2], title=\"Feature Importance - Cover (Descending)\"\n",
    ")\n",
    "axes[2].set_xlabel(\"Cover Score\")\n",
    "axes[2].set_ylabel(\"Features\")\n",
    "axes[2].invert_yaxis()  # Invert y-axis for descending order\n",
    "\n",
    "# Adjust layout and spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
