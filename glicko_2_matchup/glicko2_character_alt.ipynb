{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from glicko2 import Player\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SQLite Database into Pandas DataFrames\n",
    "\n",
    "The following code connects to an SQLite database (`melee_player_database.db`) and converts each table within the database into a pandas DataFrame. The DataFrames will be stored in a dictionary, where each key corresponds to the table name with `_df` appended, and the values are the respective DataFrames.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Database Connection**: We use the `sqlite3` library to connect to the SQLite database file.\n",
    "2. **Retrieve Table Names**: A query retrieves all the table names in the database.\n",
    "3. **Convert Tables to DataFrames**: For each table:\n",
    "   - The table is loaded into a pandas DataFrame using `pd.read_sql()`.\n",
    "   - We check each column to see if any data is JSON-formatted (lists or dictionaries). If so, we convert these columns from strings into their corresponding Python objects using `json.loads()`.\n",
    "4. **Store DataFrames**: The DataFrames are stored in a dictionary, where the key is the table name with a `_df` suffix, and the value is the DataFrame.\n",
    "5. **Database Connection Closed**: Once all tables are loaded into DataFrames, the database connection is closed.\n",
    "\n",
    "### Example:\n",
    "If the database contains a table named `players`, the corresponding DataFrame will be stored in the dictionary with the key `players_df`, and can be accessed as:\n",
    "\n",
    "```python\n",
    "players_df = dfs['players_df']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the table names\n",
    "def get_table_names(conn):\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    return pd.read_sql(query, conn)['name'].tolist()\n",
    "\n",
    "# Function to load tables into DataFrames\n",
    "def load_tables_to_dfs(conn):\n",
    "    table_names = get_table_names(conn)\n",
    "    dataframes = {}\n",
    "    \n",
    "    for table in table_names:\n",
    "        # Load table into a DataFrame\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "        \n",
    "        # Detect and convert JSON formatted columns (if any)\n",
    "        for col in df.columns:\n",
    "            # Check if any entry in the column is a valid JSON (list or dictionary)\n",
    "            if df[col].apply(lambda x: isinstance(x, str)).all():\n",
    "                try:\n",
    "                    # Try parsing the column as JSON\n",
    "                    df[col] = df[col].apply(lambda x: json.loads(x) if pd.notnull(x) else x)\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    # If it fails, skip the column\n",
    "                    pass\n",
    "        \n",
    "        # Store the DataFrame with table name + '_df'\n",
    "        dataframes[f\"{table}_df\"] = df\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "if os.path.exists(data_path + 'dfs_dict.pkl'):\n",
    "    cell_has_run = True\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    with open(data_path + 'dfs_dict.pkl', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n",
    "# Check if the flag variable exists in the global scope so that this code does not run twice\n",
    "if 'cell_has_run' not in globals():\n",
    "    path = data_path + \"melee_player_database.db\"\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(path)\n",
    "\n",
    "    # Convert each table into a DataFrame\n",
    "    dfs = load_tables_to_dfs(conn)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Now, you have a dictionary 'dfs' where each key is the table name with '_df' suffix and value is the corresponding DataFrame.\n",
    "    # For example, to access the DataFrame for a table called 'players':\n",
    "    # players_df = dfs['players_df']\n",
    "\n",
    "    dfs['tournament_info_df']['start'] = pd.to_datetime(dfs['tournament_info_df']['start'], unit='s')\n",
    "    dfs['tournament_info_df']['end'] = pd.to_datetime(dfs['tournament_info_df']['end'], unit='s')\n",
    "\n",
    "    \n",
    "    # Set the flag to indicate that the cell has been run\n",
    "    cell_has_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we adjust the data types of the dataframes so that they are the correct type. (This will be updated as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['sets_df']['best_of'] = dfs['sets_df']['best_of'].fillna(0).astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we make dataframes that we will use and print the head.\n",
    "\n",
    "The integers in 'characters' count the number of games the player has played that character. (We verify this for Zain below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = dfs['players_df']\n",
    "players_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df = dfs['ranking_df']\n",
    "ranking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_seasons_df = dfs['ranking_seasons_df']\n",
    "ranking_seasons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_df = dfs['sets_df']\n",
    "print(f\"{sets_df[sets_df['game_data'].apply(lambda x: len(x) > 0)].shape[0] / sets_df.shape[0]:0.01%} percent of sets have some game data)\")\n",
    "\n",
    "sets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_info_df = dfs['tournament_info_df']\n",
    "tournament_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code optimization by Dan\n",
    "# Basically we want to replace this line in process_tournament with something more efficient:\n",
    "#\n",
    "#      tournament_sets_df = sets_df[sets_df['tournament_key'] == tournament_key]\n",
    "#\n",
    "# Instead, we can\n",
    "# - Merge the tournament date info into ``sets_df``\n",
    "# - Sort by date\n",
    "# - Store the start/end positions of each tournament in a separate dictionary\n",
    "# - Use tournament_sets_df = sets_df.iloc[start:end+1] instead.\n",
    "\n",
    "sets_df = sets_df.merge(tournament_info_df[['key', 'start', 'end']], left_on='tournament_key', right_on='key', how='left')\n",
    "sets_df = sets_df.drop(labels=['key_y'], axis='columns')\n",
    "sets_df = sets_df.rename(columns={\"key_x\": \"key\"})\n",
    "sets_df = sets_df.sort_values(by=['end', 'tournament_key']) # Just in case there are tournaments with the exact same end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of data cleanup\n",
    "# TODO: Rerun!\n",
    "min_date = datetime.datetime(2015, 1, 1)\n",
    "max_date = datetime.datetime(2024, 12, 31)\n",
    "\n",
    "sets_df = sets_df[(sets_df['start'] >= min_date) & (sets_df['end'] >= min_date) & (sets_df['start'] <= max_date) & (sets_df['end'] <= max_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data_sets_df = sets_df[sets_df['game_data'].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, for each row in game_data_sets_df,\n",
    "# we will replace it with multiple rows,\n",
    "# one for each entry in 'game_data',\n",
    "# with other row entries updated accordingly\n",
    "\n",
    "# No point in keeping track of the game data after this\n",
    "new_columns = list(game_data_sets_df.columns)\n",
    "new_columns.remove('game_data')\n",
    "\n",
    "# Primitive progress bar so we don't get bored waiting\n",
    "counter = 0\n",
    "\n",
    "# One row -> multiple rows as above\n",
    "def duplicate_row(x):\n",
    "    global counter\n",
    "\n",
    "    # Create a new row for each entry in ['game_data']\n",
    "    # Store the result here\n",
    "    games = []\n",
    "\n",
    "    for game_info in x.iloc[0]['game_data']:\n",
    "        game = x.iloc[0][new_columns].copy()\n",
    "\n",
    "        # Invalid character data, just skip the row \n",
    "        if game_info['winner_char'] is None or game_info['loser_char'] is None:\n",
    "            continue\n",
    "\n",
    "        # This helps deal with invalid player data in ['game_data']\n",
    "        # on the off chance that it exists\n",
    "        winner_id = game['p1_id'] if game['winner_id'] == game['p1_id'] else game['p2_id']\n",
    "        loser_id  = game['p2_id'] if game['winner_id'] == game['p1_id'] else game['p1_id']\n",
    "\n",
    "        # Update the player ids to include the character that they played\n",
    "        winner_pc = winner_id + '/' + game_info['winner_char']\n",
    "        loser_pc  = loser_id  + '/' + game_info['loser_char']\n",
    "\n",
    "        # Change ids and scores in this row to reflect that specific game\n",
    "        if game['winner_id'] == game['p1_id']:\n",
    "            game['p1_id'] = winner_pc\n",
    "            game['p1_score'] = game_info['winner_score']\n",
    "            game['p2_id'] = loser_pc\n",
    "            game['p2_score'] = game_info['loser_score']\n",
    "        else:\n",
    "            game['p1_id'] = loser_pc\n",
    "            game['p1_score'] = game_info['loser_score']\n",
    "            game['p2_id'] = winner_pc\n",
    "            game['p2_score'] = game_info['winner_score']\n",
    "\n",
    "        game['winner_id'] = winner_pc\n",
    "\n",
    "        games.append(game)\n",
    "\n",
    "    # Progress bar\n",
    "    if counter % 10000 == 0:\n",
    "        print(counter)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    return pd.DataFrame(games, columns=x.columns)\n",
    "\n",
    "# More janky nonsense.\n",
    "# reset_index() to get an index to groupby().\n",
    "# Then groupby() on that index (yes, one row per group),\n",
    "#     BECAUSE .apply() on groupby can take a function that returns a dataframe with a different number of rows\n",
    "processed_game_data_sets = game_data_sets_df.reset_index().groupby('index').apply(duplicate_row, include_groups=False).reset_index(drop=True)\n",
    "processed_game_data_sets[(processed_game_data_sets['p1_id'] == '1021/melee/yoshi') | (processed_game_data_sets['p2_id'] == '1021/melee/yoshi')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_pickle(processed_game_data_sets, data_path + 'individual_game_data.pkl')\n",
    "individual_game_data_df = pd.read_pickle(data_path + 'individual_game_data.pkl')\n",
    "individual_game_data_df = individual_game_data_df.sort_values('end')\n",
    "individual_game_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because it would take an ABSURDLY long time to compute the ELOs of\n",
    "# (specific_player)/(specific_character) \\union (anyone)/(specific_opponent_character)\n",
    "# we will instead approximate as follows:\n",
    "# 1. compute the ELOs of (anyone)/(specific_opponent_character) (ex: fox vs fox, to guage vaguely how good each fox player is)\n",
    "# 2. Having those values fixed, compute the ELO of (specific_player)/(specific_character) (ex: aMSa/yoshi vs all fixed fox player ELOs)\n",
    "#    in their matches against any such player from before.\n",
    "\n",
    "individual_game_data_df['winner_char'] = individual_game_data_df['winner_id'].apply(lambda x: x.split('/')[-1])\n",
    "individual_game_data_df['p1_char'] = individual_game_data_df['p1_id'].apply(lambda x: x.split('/')[-1])\n",
    "individual_game_data_df['p2_char'] = individual_game_data_df['p2_id'].apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "all_characters = list(set(list(individual_game_data_df['p1_char'].unique()) + list(individual_game_data_df['p1_char'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes containing only character vs same character\n",
    "\n",
    "char_vs_char_dataframes = {}\n",
    "\n",
    "for character in all_characters:\n",
    "    char_vs_char_df = individual_game_data_df[(individual_game_data_df['p1_char'] == character) & (individual_game_data_df['p2_char'] == character)]\n",
    "    char_vs_char_df = char_vs_char_df.reset_index(drop=True) # Necessary for proper position calculations later\n",
    "\n",
    "    # Some more data cleanup, although this should have been done earlier\n",
    "    # TODO: Do this earlier.\n",
    "    min_date = datetime.datetime(2015, 1, 1)\n",
    "    max_date = datetime.datetime(2024, 12, 31)\n",
    "    char_vs_char_df = char_vs_char_df[(char_vs_char_df['start'] >= min_date) &\n",
    "                                      (char_vs_char_df['end'] >= min_date) &\n",
    "                                      (char_vs_char_df['start'] <= max_date) &\n",
    "                                      (char_vs_char_df['end'] <= max_date)]\n",
    "    \n",
    "    # To avoid absurd pollution of all the datasets,\n",
    "    # let's only consider players who have actually played more than a few games\n",
    "    # against another person with the same character\n",
    "    MIN_GAMES = 3\n",
    "\n",
    "    game_players_series = pd.concat([char_vs_char_df['p1_id'], char_vs_char_df['p2_id']])\n",
    "    total_games = game_players_series.value_counts()\n",
    "    regular_players = total_games[total_games >= MIN_GAMES]\n",
    "\n",
    "    char_vs_char_dataframes[character] = char_vs_char_df[(char_vs_char_df['p1_id'].apply(lambda x: x in regular_players.index)) &\n",
    "                                                         (char_vs_char_df['p2_id'].apply(lambda x: x in regular_players.index))]\n",
    "\n",
    "char_vs_char_dataframes['mrgameandwatch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of dictionaries. First key is the specific character used for the matchup.\n",
    "# For each one, we will store the start/end indexes of each tournament in each dataframe.\n",
    "# This will be used for far more efficient processing later\n",
    "date_positions_by_char = {}\n",
    "\n",
    "for character in all_characters:\n",
    "    print(character)\n",
    "\n",
    "    char_games_df = char_vs_char_dataframes[character]\n",
    "    positions = {}\n",
    "\n",
    "    initial_date = datetime.datetime(2015, 1, 1)\n",
    "    interval = datetime.timedelta(weeks=1)\n",
    "\n",
    "    end_date = char_games_df['end'].max()\n",
    "\n",
    "    date = initial_date\n",
    "\n",
    "    # Loop through and test for containment in the interval [date, date+interval)\n",
    "    while date + interval <= end_date:\n",
    "        is_in_interval = (sets_df['end'] >= date) & (sets_df['end'] < date + interval)\n",
    "        is_in_interval = is_in_interval.reset_index(drop=True) # Make absolutely sure it can be used for iloc\n",
    "\n",
    "        if (~is_in_interval).all(): # all False\n",
    "            positions[date + interval] = (0,-1)\n",
    "        else: # At least one True\n",
    "            positions[date + interval] = (is_in_interval.idxmax(), is_in_interval[::-1].idxmax())\n",
    "\n",
    "        date += interval\n",
    "\n",
    "    date_positions_by_char[character] = positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLDR the ELO computation occasionally seems to go negative, and I think this can cause a lot of problems.\n",
    "# Likewise, the RD value seems to get unusually large occasionally\n",
    "MIN_ELO = 500.0\n",
    "MAX_RD = 350.0\n",
    "\n",
    "def process_tournament(player_ratings_df, player_rds_df, games_df, positions, specific_date, key_is_date, glicko_objects):\n",
    "    # Get the games for this specific period, using precomputed indices for speed.\n",
    "    period_df = games_df.iloc[positions[specific_date][0]:positions[specific_date][1]+1]\n",
    "    unique_players = list(player_ratings_df.columns)\n",
    "\n",
    "    # Prepare player matches DataFrame.\n",
    "    # Essentially, one copy where p1 is the player of consideration,\n",
    "    # and then another copy where p2 is under consideration.\n",
    "    df_p1 = period_df[['p1_id', 'p2_id', 'winner_id']].copy()\n",
    "    df_p1.rename(columns={'p1_id': 'player_id', 'p2_id': 'opponent_id'}, inplace=True)\n",
    "    df_p1['outcome'] = (df_p1['winner_id'] == df_p1['player_id']).astype(int)\n",
    "    df_p1 = df_p1[['player_id', 'opponent_id', 'outcome']]\n",
    "    \n",
    "    df_p2 = period_df[['p2_id', 'p1_id', 'winner_id']].copy()\n",
    "    df_p2.rename(columns={'p2_id': 'player_id', 'p1_id': 'opponent_id'}, inplace=True)\n",
    "    df_p2['outcome'] = (df_p2['winner_id'] == df_p2['player_id']).astype(int)\n",
    "    df_p2 = df_p2[['player_id', 'opponent_id', 'outcome']]\n",
    "\n",
    "    outcomes_df = pd.concat([df_p1, df_p2], ignore_index=True)\n",
    "\n",
    "    # Add opponent ELO and RD values\n",
    "    outcomes_df['opponent_rating'] = outcomes_df['opponent_id'].apply(lambda x: glicko_objects[x].getRating())\n",
    "    outcomes_df['opponent_rd'] = outcomes_df['opponent_id'].apply(lambda x: glicko_objects[x].getRd())\n",
    "\n",
    "    # Compile the opponent data into lists (one for ratings, one for rds, one for outcomes) for every player\n",
    "    grouped = outcomes_df.groupby('player_id').agg({\n",
    "        'opponent_rating': list,\n",
    "        'opponent_rd': list,\n",
    "        'outcome': list\n",
    "    }).reset_index()\n",
    "\n",
    "    # We will need to update the glicko objects separately for players that did or didn't play in this rating period\n",
    "    players_with_games = list(grouped['player_id'])\n",
    "    players_without_games = [x for x in unique_players if x not in players_with_games]\n",
    "\n",
    "    #First, let's handle the players that did play games in this period\n",
    "    def update_glicko(row):\n",
    "        glicko_objects[row['player_id']].update_player(row['opponent_rating'], row['opponent_rd'], row['outcome'])\n",
    "\n",
    "    grouped.apply(update_glicko, axis=1)\n",
    "\n",
    "    # Now let's handle players that did NOT play games in this period\n",
    "    for player in players_without_games:\n",
    "        glicko_objects[player].did_not_compete()\n",
    "\n",
    "    # This might be a bug in the glicko2 library that we are using,\n",
    "    # but occasionally we get weird values for the rating and/or RD\n",
    "    # and this DOES very occasionally seem to cause the whole thing to crash\n",
    "    # (division by zero somewhere)\n",
    "    # and so we use professional-grade duct tape here\n",
    "    for player in unique_players:\n",
    "        if glicko_objects[player].getRating() < MIN_ELO:\n",
    "            glicko_objects[player].setRating(MIN_ELO)\n",
    "\n",
    "        if glicko_objects[player].getRd() > MAX_RD:\n",
    "            glicko_objects[player].setRd(MAX_RD)\n",
    "\n",
    "    # Now update that specific row of the player ratings dataframe with the new glicko data\n",
    "    date = specific_date if key_is_date else games_df.iloc[positions[specific_date][0]]['end']\n",
    "\n",
    "    #new_player_data = {}\n",
    "    #for player in unique_players:\n",
    "    #    new_player_data[player] = glicko_objects[player].getRating()\n",
    "\n",
    "    # These should be in the same order as the columns, as unique_players was used to create the columns as well.\n",
    "    # This is slightly more efficient than using a dictionary.\n",
    "    new_player_ratings = [glicko_objects[player].getRating() for player in unique_players]\n",
    "    new_player_rds     = [glicko_objects[player].getRd()     for player in unique_players]\n",
    "\n",
    "    player_ratings_df.loc[date] = new_player_ratings\n",
    "    player_rds_df.loc[date]     = new_player_rds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically, we will instead create a dataframe with index of dates, columns of players\n",
    "player_ratings_char_vs_char = {}\n",
    "player_rds_char_vs_char = {}\n",
    "\n",
    "for character in all_characters:\n",
    "    print(character)\n",
    "\n",
    "    games_df = char_vs_char_dataframes[character]\n",
    "    positions = date_positions_by_char[character]\n",
    "\n",
    "    unique_players = list(set(list(games_df['p1_id'].unique()) + list(games_df['p2_id'].unique())))\n",
    "\n",
    "    # Note that this technically starts at initial_date + interval, not initial_date\n",
    "    dates = [x for x in positions]\n",
    "\n",
    "    # Convenient store of glicko objects\n",
    "    glicko_objects = {}\n",
    "    for player in unique_players:\n",
    "        glicko_objects[player] = Player()\n",
    "\n",
    "    # Initial row of default ELO values, at the earliest possible date.\n",
    "    # Note that we are also pre-allocating the dataframe with the rest of the possible dates.\n",
    "    player_ratings_df = pd.DataFrame([[1500.0] * len(unique_players)], columns=unique_players, index=[initial_date] + dates)\n",
    "    player_rds_df = pd.DataFrame([[350.0] * len(unique_players)], columns=unique_players, index=[initial_date] + dates)\n",
    "\n",
    "    # Loop over the individual date intervals above.\n",
    "    for specific_date in tqdm.tqdm(dates, total=len(dates)):\n",
    "        process_tournament(player_ratings_df, player_rds_df, games_df, positions, specific_date, True, glicko_objects)\n",
    "\n",
    "    Path(data_path + 'char_vs_char/').mkdir(parents=True, exist_ok=True)\n",
    "    player_ratings_df.to_pickle(data_path + 'char_vs_char/' + character + '.pkl')\n",
    "    #print('Size in memory: {0:.2f}'.format(sys.getsizeof(player_ratings_df) / (1024.0*1024.0)))\n",
    "    \n",
    "    player_ratings_char_vs_char[character] = player_ratings_df\n",
    "    player_rds_char_vs_char[character] = player_rds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1021/melee/yoshi - aMSa\n",
    "# 19554/melee/fox - Cody\n",
    "\n",
    "# For convenience, let us restrict our attention to players who actually have a reasonable amount of data with a certain character\n",
    "MIN_GAMES = 10\n",
    "\n",
    "game_players_series = pd.concat([individual_game_data_df['p1_id'], individual_game_data_df['p2_id']])\n",
    "total_games = game_players_series.value_counts()\n",
    "regular_players = total_games[total_games >= MIN_GAMES]\n",
    "\n",
    "print(\"{0} regular player/char combos\".format(len(regular_players.index)))\n",
    "\n",
    "# Lots of memory usage. Let's just reduce down to what we need.\n",
    "reduced_game_data_df = individual_game_data_df[['p1_id', 'p2_id', 'winner_id', 'p1_char', 'p2_char', 'winner_char', 'start', 'end']]\n",
    "\n",
    "# Each row should contribute twice, swapping 'player' and 'opponent'\n",
    "df_p1 = reduced_game_data_df[reduced_game_data_df['p1_id'].apply(lambda x: x in regular_players.index)].copy()\n",
    "df_p1.rename(columns={'p1_id': 'player_id', 'p2_id': 'opponent_id',\n",
    "                      'p1_char': 'player_char', 'p2_char': 'opponent_char'}, inplace=True)\n",
    "df_p1['outcome'] = (df_p1['winner_id'] == df_p1['player_id']).astype(int)\n",
    "df_p1 = df_p1[['player_id', 'opponent_id', 'player_char', 'opponent_char', 'outcome', 'start', 'end']]\n",
    "    \n",
    "df_p2 = reduced_game_data_df[reduced_game_data_df['p2_id'].apply(lambda x: x in regular_players.index)].copy()\n",
    "df_p2.rename(columns={'p2_id': 'player_id', 'p1_id': 'opponent_id',\n",
    "                      'p2_char': 'player_char', 'p1_char': 'opponent_char'}, inplace=True)\n",
    "df_p2['outcome'] = (df_p2['winner_id'] == df_p2['player_id']).astype(int)\n",
    "df_p2 = df_p2[['player_id', 'opponent_id', 'player_char', 'opponent_char', 'outcome', 'start', 'end']]\n",
    "\n",
    "reduced_game_data_df = pd.concat([df_p1, df_p2], ignore_index=True)\n",
    "\n",
    "# Save some memory - these are probably huge.\n",
    "del df_p1\n",
    "del df_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_game_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Rating, RD, and a bool for (actually found = True, default values = False)\n",
    "def get_opponent_elo_rd(row):\n",
    "    # See how opponent/char compares against (others)/char for the same char\n",
    "    player_ratings = player_ratings_char_vs_char[row['opponent_char']]\n",
    "    player_rds = player_rds_char_vs_char[row['opponent_char']]\n",
    "\n",
    "    # Not in our main list of players\n",
    "    if row['opponent_id'] not in player_ratings.columns:\n",
    "        return (1500.0, 350.0, False)\n",
    "\n",
    "    # No old enough data\n",
    "    if player_ratings.index[0] > row['start']:\n",
    "        return (1500.0, 350.0, False)\n",
    "\n",
    "    # We can take advantage of the fact that the index of player_ratings is always in regular intervals.\n",
    "    start_date = player_ratings.index[0]\n",
    "    interval = player_ratings.index[1] - player_ratings.index[0] # I guess we're assuming at least two entries?\n",
    "\n",
    "    newest_index = int((row['start'] - start_date) / interval)\n",
    "\n",
    "    # Might actually be out of bounds on the data we have,\n",
    "    # i.e. 'start' might be well beyond the dates we have data on.\n",
    "    # In this case, just use the newest piece of data.\n",
    "    if newest_index >= len(player_ratings.index):\n",
    "        newest_index = len(player_ratings.index) - 1\n",
    "\n",
    "    return (player_ratings.iloc[newest_index][row['opponent_id']], player_rds.iloc[newest_index][row['opponent_id']], True)\n",
    "\n",
    "reduced_game_data_df['result'] = reduced_game_data_df.apply(get_opponent_elo_rd, axis=1)\n",
    "\n",
    "reduced_game_data_df['opponent_rating'] = reduced_game_data_df['result'].apply(lambda x: x[0])\n",
    "reduced_game_data_df['opponent_rd']     = reduced_game_data_df['result'].apply(lambda x: x[1])\n",
    "reduced_game_data_df['opponent_found']  = reduced_game_data_df['result'].apply(lambda x: x[2])\n",
    "\n",
    "reduced_game_data_df.drop(columns=['result'], inplace=True)\n",
    "\n",
    "reduced_game_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again more data cleanup. Should probably be somewhere more convenient.\n",
    "min_date = datetime.datetime(2015,1,1)\n",
    "max_date = datetime.datetime(2024,12,31)\n",
    "\n",
    "reduced_game_data_df = reduced_game_data_df[(reduced_game_data_df['start'] >= min_date) &\n",
    "                                            (reduced_game_data_df['end'] >= min_date) &\n",
    "                                            (reduced_game_data_df['start'] <= max_date) &\n",
    "                                            (reduced_game_data_df['end'] <= max_date)]\n",
    "\n",
    "reduced_game_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weekly intervals to group by, quite easily.\n",
    "start_date = player_ratings_char_vs_char['fox'].index[0]\n",
    "interval = player_ratings_char_vs_char['fox'].index[1] - player_ratings_char_vs_char['fox'].index[0]\n",
    "\n",
    "reduced_game_data_df = reduced_game_data_df.copy() # \"Copy of a slice\" nonsense, this should fix it.\n",
    "reduced_game_data_df['end_index'] = reduced_game_data_df['end'].apply(lambda x: math.ceil((x - start_date) / interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_game_data_df.to_pickle(data_path + 'reduced_game_data.pkl')\n",
    "reduced_game_data_df = pd.read_pickle(data_path + 'reduced_game_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by player (includes their character!), opponent character, week index\n",
    "grouped_df = reduced_game_data_df[['player_id', 'opponent_char', 'end_index',\n",
    "                                   'opponent_rating', 'opponent_rd', 'outcome']].groupby(['player_id', 'opponent_char', 'end_index']).agg({\n",
    "        'opponent_rating': list,\n",
    "        'opponent_rd': list,\n",
    "        'outcome': list\n",
    "    }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['player_char_char'] = grouped_df.apply(lambda row: row['player_id'] + '/' + row['opponent_char'], axis=1)\n",
    "grouped_df.drop(columns=['player_id', 'opponent_char'], inplace=True)\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_df.to_pickle(data_path + 'temp_file_until_better_name.pkl')\n",
    "grouped_df = pd.read_pickle(data_path + 'temp_file_until_better_name.pkl')\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To deal with inlcude_groups=True being deprecated and disallowed soon,\n",
    "# let's just create a copy of this column\n",
    "grouped_df['pcc_duplicate'] = grouped_df['player_char_char']\n",
    "\n",
    "player_char_char_elos = {}\n",
    "\n",
    "# We will create a single table. Index is dates, columns is player/melee/pchar/ochar.\n",
    "unique_players = list(grouped_df['player_char_char'].unique())\n",
    "\n",
    "initial_date = datetime.datetime(2015, 1, 1)\n",
    "end_date = datetime.datetime(2024, 12, 31) # TODO: Properly compute this instead of just guessing\n",
    "interval = datetime.timedelta(weeks=1)\n",
    "\n",
    "# Bugfix stuff\n",
    "MIN_ELO = 500.0\n",
    "MAX_RD = 350.0\n",
    "\n",
    "# TODO: Surely there's a more professional way to do this bit.\n",
    "dates = {0: initial_date}\n",
    "\n",
    "date = initial_date + interval\n",
    "i = 1\n",
    "\n",
    "while date <= end_date:\n",
    "    dates[i] = date\n",
    "\n",
    "    date += interval\n",
    "    i += 1\n",
    "\n",
    "# Convenient store of glicko objects\n",
    "glicko_objects = {}\n",
    "for player in unique_players:\n",
    "    glicko_objects[player] = Player()\n",
    "\n",
    "# Pre-allocating the dataframe for maximum efficiency.\n",
    "player_ratings_df = pd.DataFrame([[1500.0] * len(unique_players)], columns=unique_players, index=list(dates.values())) \n",
    "\n",
    "def compute_pcc_elo(x):\n",
    "    # player/char/char\n",
    "    pcc = x.iloc[0]['pcc_duplicate']\n",
    "\n",
    "    # More easily allow for getting the week number\n",
    "    x = x.set_index('end_index')\n",
    "\n",
    "    glicko_object = glicko_objects[pcc]\n",
    "\n",
    "    # More efficient to keep track of where every occuring week number is (as an iloc).\n",
    "    weeknum_to_iloc = [-1]*len(dates)\n",
    "    for i in range(0, len(x.index)):\n",
    "        weeknum_to_iloc[x.index[i]] = i\n",
    "\n",
    "    for index in dates:\n",
    "        if weeknum_to_iloc[index] == -1:\n",
    "            glicko_object.did_not_compete()\n",
    "        else:\n",
    "            glicko_object.update_player(x.iloc[weeknum_to_iloc[index]]['opponent_rating'],\n",
    "                                        x.iloc[weeknum_to_iloc[index]]['opponent_rd'],\n",
    "                                        x.iloc[weeknum_to_iloc[index]]['outcome'])\n",
    "\n",
    "        # Bugfix stuff\n",
    "        if glicko_object.getRating() < MIN_ELO:\n",
    "            glicko_object.setRating(MIN_ELO)\n",
    "\n",
    "        if glicko_object.getRd() > MAX_RD:\n",
    "            glicko_object.setRd(MAX_RD)\n",
    "\n",
    "        player_ratings_df.loc[initial_date + index*interval, pcc] = glicko_object.getRating()\n",
    "\n",
    "# TODO: Figure out how to include the group \n",
    "grouped_df.groupby('player_char_char').apply(compute_pcc_elo, include_groups=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_ratings_df['1021/melee/yoshi/fox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_ratings_df.to_pickle(data_path + 'char_vs_char_player_rankings_weekly.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
