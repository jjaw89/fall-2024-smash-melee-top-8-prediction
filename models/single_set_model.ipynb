{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "#import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from glicko2 import Player\n",
    "import multiprocessing\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "if os.path.exists('/workspace/data_2'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data_2/'\n",
    "else:\n",
    "    data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Here, we assume that the ``sets_df`` file, potentially with player swapping, was saved as a separate pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['key_x', 'game', 'tournament_key', 'winner_id', 'loser_id', 'p1_id',\n",
       "       'p2_id', 'p1_score', 'p2_score', 'valid_score',\n",
       "       ...\n",
       "       'p2/m1/m1_alt2_rd', 'p2/m1/m1_alt2_updates', 'p1/m1_alt3_elo',\n",
       "       'p1/m1_alt3_rd', 'p1/m1_alt3_updates', 'p2/m1_alt3_elo',\n",
       "       'p2/m1_alt3_rd', 'p2/m1_alt3_updates', 'matchup_total', 'score_ratio'],\n",
       "      dtype='object', length=125)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_pickle(data_path + 'dataset_full.pkl')\n",
    "dataset_df.sort_values(by=['end', 'start']) # Just to be sure\n",
    "\n",
    "# If you want to use just the total instead.\n",
    "dataset_df['matchup_total'] = dataset_df[['matchup_1', 'matchup_2',\n",
    "       'matchup_3', 'matchup_4', 'matchup_5', 'matchup_6', 'matchup_7',\n",
    "       'matchup_8', 'matchup_9', 'matchup_10']].sum(axis=1)\n",
    "\n",
    "# If you wanna regress on the score ratio, rather than just the binary outcome\n",
    "dataset_df['score_ratio'] = dataset_df[['p1_score', 'p2_score', 'valid_score']].apply(lambda row: row['p1_score'] / (row['p1_score'] + row['p2_score']) if row['valid_score'] else 0.5, axis=1)\n",
    "\n",
    "dataset_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_x</th>\n",
       "      <th>game</th>\n",
       "      <th>tournament_key</th>\n",
       "      <th>winner_id</th>\n",
       "      <th>loser_id</th>\n",
       "      <th>p1_id</th>\n",
       "      <th>p2_id</th>\n",
       "      <th>p1_score</th>\n",
       "      <th>p2_score</th>\n",
       "      <th>valid_score</th>\n",
       "      <th>...</th>\n",
       "      <th>p2/m1/m1_alt2_rd</th>\n",
       "      <th>p2/m1/m1_alt2_updates</th>\n",
       "      <th>p1/m1_alt3_elo</th>\n",
       "      <th>p1/m1_alt3_rd</th>\n",
       "      <th>p1/m1_alt3_updates</th>\n",
       "      <th>p2/m1_alt3_elo</th>\n",
       "      <th>p2/m1_alt3_rd</th>\n",
       "      <th>p2/m1_alt3_updates</th>\n",
       "      <th>matchup_total</th>\n",
       "      <th>score_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td></td>\n",
       "      <td>melee</td>\n",
       "      <td>httpsbeastsmashchallongecomb5msb</td>\n",
       "      <td>Däumling</td>\n",
       "      <td>Schlimmshady</td>\n",
       "      <td>Schlimmshady</td>\n",
       "      <td>Däumling</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td></td>\n",
       "      <td>melee</td>\n",
       "      <td>httpsbeastsmashchallongecomb5msb</td>\n",
       "      <td>NamiNami</td>\n",
       "      <td>ProSmasherTim</td>\n",
       "      <td>ProSmasherTim</td>\n",
       "      <td>NamiNami</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td></td>\n",
       "      <td>melee</td>\n",
       "      <td>httpsbeastsmashchallongecomb5msb</td>\n",
       "      <td>Gusti</td>\n",
       "      <td>Kuja</td>\n",
       "      <td>Gusti</td>\n",
       "      <td>Kuja</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td></td>\n",
       "      <td>melee</td>\n",
       "      <td>httpsbeastsmashchallongecomb5msb</td>\n",
       "      <td>Mikehaggar</td>\n",
       "      <td>Fauster</td>\n",
       "      <td>Fauster</td>\n",
       "      <td>Mikehaggar</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td></td>\n",
       "      <td>melee</td>\n",
       "      <td>httpsbeastsmashchallongecomb5msb</td>\n",
       "      <td>V-Dogg</td>\n",
       "      <td>Dev</td>\n",
       "      <td>V-Dogg</td>\n",
       "      <td>Dev</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792527</th>\n",
       "      <td>gg__76189357</td>\n",
       "      <td>melee</td>\n",
       "      <td>newsflash-59__super-smash-bros-melee-singles</td>\n",
       "      <td>148391</td>\n",
       "      <td>3822249</td>\n",
       "      <td>3822249</td>\n",
       "      <td>148391</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>341.183956</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1572.660481</td>\n",
       "      <td>48.218999</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1602.563043</td>\n",
       "      <td>67.308275</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771307</th>\n",
       "      <td>gg__75580131</td>\n",
       "      <td>melee</td>\n",
       "      <td>triple-threat-tuesdays-30-the-peak__melee-singles</td>\n",
       "      <td>342875</td>\n",
       "      <td>1377337</td>\n",
       "      <td>1377337</td>\n",
       "      <td>342875</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>91.588819</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1370.998161</td>\n",
       "      <td>51.384483</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1338.705624</td>\n",
       "      <td>67.010325</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489897</th>\n",
       "      <td>gg__67589317</td>\n",
       "      <td>melee</td>\n",
       "      <td>shuuten-tokyo-1__shuuten-1-melee-singles-shinjuku</td>\n",
       "      <td>34804</td>\n",
       "      <td>1158912</td>\n",
       "      <td>1158912</td>\n",
       "      <td>34804</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>113.408033</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1285.261707</td>\n",
       "      <td>131.246926</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1366.212582</td>\n",
       "      <td>99.257620</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489903</th>\n",
       "      <td>gg__67589323</td>\n",
       "      <td>melee</td>\n",
       "      <td>shuuten-tokyo-1__shuuten-1-melee-singles-shinjuku</td>\n",
       "      <td>1602510</td>\n",
       "      <td>10565</td>\n",
       "      <td>1602510</td>\n",
       "      <td>10565</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>278.200801</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1589.041503</td>\n",
       "      <td>106.374142</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1579.035666</td>\n",
       "      <td>93.860494</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489887</th>\n",
       "      <td>gg__67589286</td>\n",
       "      <td>melee</td>\n",
       "      <td>shuuten-tokyo-1__shuuten-1-melee-singles-shinjuku</td>\n",
       "      <td>25647</td>\n",
       "      <td>3726512</td>\n",
       "      <td>25647</td>\n",
       "      <td>3726512</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1417.296324</td>\n",
       "      <td>105.687238</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162814 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                key_x   game  \\\n",
       "719                    melee   \n",
       "773                    melee   \n",
       "772                    melee   \n",
       "771                    melee   \n",
       "770                    melee   \n",
       "...               ...    ...   \n",
       "1792527  gg__76189357  melee   \n",
       "1771307  gg__75580131  melee   \n",
       "1489897  gg__67589317  melee   \n",
       "1489903  gg__67589323  melee   \n",
       "1489887  gg__67589286  melee   \n",
       "\n",
       "                                            tournament_key   winner_id  \\\n",
       "719                       httpsbeastsmashchallongecomb5msb    Däumling   \n",
       "773                       httpsbeastsmashchallongecomb5msb    NamiNami   \n",
       "772                       httpsbeastsmashchallongecomb5msb       Gusti   \n",
       "771                       httpsbeastsmashchallongecomb5msb  Mikehaggar   \n",
       "770                       httpsbeastsmashchallongecomb5msb      V-Dogg   \n",
       "...                                                    ...         ...   \n",
       "1792527       newsflash-59__super-smash-bros-melee-singles      148391   \n",
       "1771307  triple-threat-tuesdays-30-the-peak__melee-singles      342875   \n",
       "1489897  shuuten-tokyo-1__shuuten-1-melee-singles-shinjuku       34804   \n",
       "1489903  shuuten-tokyo-1__shuuten-1-melee-singles-shinjuku     1602510   \n",
       "1489887  shuuten-tokyo-1__shuuten-1-melee-singles-shinjuku       25647   \n",
       "\n",
       "              loser_id          p1_id       p2_id  p1_score  p2_score  \\\n",
       "719       Schlimmshady   Schlimmshady    Däumling         0         2   \n",
       "773      ProSmasherTim  ProSmasherTim    NamiNami         1         2   \n",
       "772               Kuja          Gusti        Kuja         2         1   \n",
       "771            Fauster        Fauster  Mikehaggar         1         2   \n",
       "770                Dev         V-Dogg         Dev         2         1   \n",
       "...                ...            ...         ...       ...       ...   \n",
       "1792527        3822249        3822249      148391         0         3   \n",
       "1771307        1377337        1377337      342875         0         2   \n",
       "1489897        1158912        1158912       34804         1         2   \n",
       "1489903          10565        1602510       10565         3         1   \n",
       "1489887        3726512          25647     3726512         2         0   \n",
       "\n",
       "         valid_score  ...  p2/m1/m1_alt2_rd p2/m1/m1_alt2_updates  \\\n",
       "719             True  ...        350.000000                   0.0   \n",
       "773             True  ...        350.000000                   0.0   \n",
       "772             True  ...        350.000000                   0.0   \n",
       "771             True  ...        350.000000                   0.0   \n",
       "770             True  ...        350.000000                   0.0   \n",
       "...              ...  ...               ...                   ...   \n",
       "1792527        False  ...        341.183956                   2.0   \n",
       "1771307         True  ...         91.588819                  13.0   \n",
       "1489897         True  ...        113.408033                   5.0   \n",
       "1489903        False  ...        278.200801                   2.0   \n",
       "1489887         True  ...        350.000000                   0.0   \n",
       "\n",
       "        p1/m1_alt3_elo p1/m1_alt3_rd p1/m1_alt3_updates p2/m1_alt3_elo  \\\n",
       "719        1500.000000    350.000000                0.0    1500.000000   \n",
       "773        1500.000000    350.000000                0.0    1500.000000   \n",
       "772        1500.000000    350.000000                0.0    1500.000000   \n",
       "771        1500.000000    350.000000                0.0    1500.000000   \n",
       "770        1500.000000    350.000000                0.0    1500.000000   \n",
       "...                ...           ...                ...            ...   \n",
       "1792527    1572.660481     48.218999                5.0    1602.563043   \n",
       "1771307    1370.998161     51.384483               58.0    1338.705624   \n",
       "1489897    1285.261707    131.246926                2.0    1366.212582   \n",
       "1489903    1589.041503    106.374142                4.0    1579.035666   \n",
       "1489887    1417.296324    105.687238                3.0    1500.000000   \n",
       "\n",
       "         p2/m1_alt3_rd p2/m1_alt3_updates  matchup_total score_ratio  \n",
       "719         350.000000                0.0            5.0    0.000000  \n",
       "773         350.000000                0.0            5.0    0.333333  \n",
       "772         350.000000                0.0            5.0    0.666667  \n",
       "771         350.000000                0.0            5.0    0.333333  \n",
       "770         350.000000                0.0            5.0    0.666667  \n",
       "...                ...                ...            ...         ...  \n",
       "1792527      67.308275                6.0            4.5    0.500000  \n",
       "1771307      67.010325               20.0            3.0    0.000000  \n",
       "1489897      99.257620                4.0            5.0    0.333333  \n",
       "1489903      93.860494                6.0            5.0    0.500000  \n",
       "1489887     350.000000                0.0            5.0    1.000000  \n",
       "\n",
       "[162814 rows x 125 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_default_elo = ['p1_default_elo', 'p2_default_elo']\n",
    "features_all_elo = ['p1_default_elo', 'p2_default_elo', 'p1/m1/m1_alt2_elo', 'p2/m1/m1_alt2_elo', 'p1/m1_alt3_elo', 'p2/m1_alt3_elo']\n",
    "features_all_rd = [x.replace('elo', 'rd') for x in features_all_elo]\n",
    "features_all_updates = [x.replace('elo', 'updates') for x in features_all_elo]\n",
    "features_all_eru = features_all_elo + features_all_rd + features_all_updates\n",
    "\n",
    "# I'm lazy\n",
    "features_all_everything = ['p1_default_elo', 'p2_default_elo', 'p1_default_rd', 'p2_default_rd',\n",
    "       'p1_default_updates', 'p2_default_updates', 'matchup_1', 'matchup_2',\n",
    "       'matchup_3', 'matchup_4', 'matchup_5', 'matchup_6', 'matchup_7',\n",
    "       'matchup_8', 'matchup_9', 'matchup_10', 'p1_m1_usage', 'p2_m1_usage',\n",
    "       'p1/m1/m1_alt2_elo', 'p1/m1/m1_alt2_rd', 'p1/m1/m1_alt2_updates',\n",
    "       'p2/m1/m1_alt2_elo', 'p2/m1/m1_alt2_rd', 'p2/m1/m1_alt2_updates',\n",
    "       'p1/m1_alt3_elo', 'p1/m1_alt3_rd', 'p1/m1_alt3_updates',\n",
    "       'p2/m1_alt3_elo', 'p2/m1_alt3_rd', 'p2/m1_alt3_updates']\n",
    "\n",
    "# Filter by elos that actually have very nontrivial data\n",
    "quality_filter = pd.Series(True, index=dataset_df.index)\n",
    "for update_col in features_all_updates:\n",
    "    quality_filter = quality_filter & (dataset_df[update_col] >= 10.0)\n",
    "\n",
    "low_quality_filter = pd.Series(True, index=dataset_df.index)\n",
    "for update_col in features_all_updates:\n",
    "    low_quality_filter = low_quality_filter & ((dataset_df[update_col] >= 2.0) & (dataset_df[update_col] <= 10.0))\n",
    "\n",
    "similar_default_elo_filter = (dataset_df['p1_default_elo'] - dataset_df['p2_default_elo']).abs() <= 20.0\n",
    "\n",
    "dataset_df[similar_default_elo_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Skippable code below\n",
    "\n",
    "Most of the code below involves forming various observations about the data and testing various models to see if they would improve prediction accuracy. Ultimately, most ideas fail, but they might still be interesting and are kept nonetheless, especially to see how well they perform relative to our best model.\n",
    "\n",
    "Training and testing of our final model will resume near the end of this notebook, and a note is placed to show where this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An observation about the data\n",
    "\n",
    "First, we note the motivation for including ``quality_filter`` as an option for the data. With it, ELO scores appear to (mostly) follow a multivariate normal distribution, while lower-quality data tends to cluster far more around the default elo values of 1500. This suggests, at least in the case of high-quality data, that very simplistic linear models might actually yield the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('default, alt2, alt3 ELOs for quality data')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.subplot(3,2,2*i+1)\n",
    "    plt.scatter(dataset_df[quality_filter & (dataset_df['winner'] == 1.0)][features_all_elo[2*i]],\n",
    "                dataset_df[quality_filter & (dataset_df['winner'] == 1.0)][features_all_elo[2*i+1]],\n",
    "                s=0.3, alpha=0.2, label='p1 wins')\n",
    "    plt.xlim(0, 3000)\n",
    "    plt.ylim(0, 3000)\n",
    "    \n",
    "    if i != 2:\n",
    "        plt.xticks([])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3,2,2*i+2)\n",
    "    plt.scatter(dataset_df[quality_filter & (dataset_df['winner'] == 0.0)][features_all_elo[2*i]],\n",
    "                dataset_df[quality_filter & (dataset_df['winner'] == 0.0)][features_all_elo[2*i+1]],\n",
    "                s=0.3, alpha=0.2, label='p2 wins')\n",
    "    plt.xlim(0, 3000)\n",
    "    plt.ylim(0, 3000)\n",
    "    plt.legend()\n",
    "\n",
    "    if i != 2:\n",
    "        plt.xticks([])\n",
    "\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing some basic models\n",
    "\n",
    "Before we begin, some important remarks are in order. In some sense, this is time series data, and in another sense, it is not. It somewhat showcases the evolution of player ELO ratings (and related features) over time, albeit without directly linking them to certain players. However, these ELO scores, in some sense, already take into account all of the player's past performance up to a certain point. Likewise, RD values are meant to indicate the \"uncertainty\" in any player's current ELO score, and at any one point in time it takes into account all of their previous games, and even how long it has been since they've last played.\n",
    "\n",
    "That being said, it is somewhat ill-advised to shuffle the data when training. Especially for more advanced models, it might potentially be able to recognize that a very precise ELO score has shown up in some future match, and conclude that it must have won some previous matches (note that we have updated ELO scores only once a week).\n",
    "\n",
    "Here, we begin by training some basic models that have the goal of predicting the outcome of individual sets, with no ability to look back on any past performance (single-set models). We first want to observe the impact of the following factors, and are not yet interested in serious hyperparameter tuning:\n",
    "\n",
    "* Is it better to only train on more recent data rather than all data up to a certain point (perhaps game meta, average ELO, etc... shifts over time)\n",
    "* Are we actually gaining anything by including all of our engineered ELO scores, rather than just the default ones?\n",
    "* What is the impact of considering only on \"high quality\" data which has received many updates to all ELO scores?\n",
    "\n",
    "We also note that for this single-set predictor, we train on data up to 2022 (and test on 2023) for cross-validation and hyperparameter tuning. The secondary model which takes tournament performance into account (which will use this single-set model), tuned on 2023 data and have its final performance tested on 2024 data. However, for the interest of obtaining a final performance score for the single-set predictor, it should be safely testable on 2024 data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models that only use the ELO scores\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2017, 2022+1)\n",
    "\n",
    "# These models work best with normally distributed data, which appears to be the case for the various ELOs\n",
    "models = {'lr': LogisticRegression(penalty=None, max_iter=10000),\n",
    "          'lda': LinearDiscriminantAnalysis(),\n",
    "          'qda': QuadraticDiscriminantAnalysis(),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "training_modes = ['past_year', 'all_years'] # Train on just the past year, or all years (starting from 2016).\n",
    "elo_modes = ['default_elo', 'all_elo'] # Only the default glicko2 elo scores, or all engineered ones\n",
    "data_modes = ['quality_data', 'all_data'] # Whether or not each elo has had at least 10 updates\n",
    "\n",
    "for training_mode in training_modes:\n",
    "    for elo_mode in elo_modes:\n",
    "        for data_mode in data_modes:\n",
    "            # Just using default ELOs\n",
    "            for i, y in enumerate(tqdm(years)):\n",
    "                # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "                # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "                dataset_train_df = dataset_df[(dataset_df['start'] >= datetime.datetime(2016 if training_mode == 'all_years' else y, 1, 1)) &\n",
    "                                            (dataset_df['end'] <= datetime.datetime(y,12,31)) &\n",
    "                                            (quality_filter if data_mode == 'quality_data' else True)]\n",
    "                dataset_test_df = dataset_df[(dataset_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                            (dataset_df['end'] <= datetime.datetime(y+1,12,31)) &\n",
    "                                            (quality_filter if data_mode == 'quality_data' else True)]\n",
    "                \n",
    "                for j, name in enumerate(models):\n",
    "                    models[name].fit(dataset_train_df[features_default_elo if elo_mode == 'default_elo' else features_all_elo], dataset_train_df['winner'])\n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_default_elo if elo_mode == 'default_elo' else features_all_elo])\n",
    "                    y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "                    ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "                    acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "            print(\"Using \" + training_mode + \" and \" + elo_mode + \" and \" + data_mode)\n",
    "            print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "                        index=years,\n",
    "                        columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of particular interest are the latter few years, because the data is of substantially higher quality, and it will also more closely follow years 2023 and 2024.\n",
    "\n",
    "Without any need to run any statistical tests, a simple side-by-side comparison reveals that\n",
    "* In all instances, training on all data seems to either do comparably, or substantially better (possibly due to poor quality data for that specific training year), than training on all data.\n",
    "* LDA appears favourable over QDA.\n",
    "* When dealing with higher-quality data, basic linear models substantially outperform XGBoost, at least without any hyperparameter tuning and training only on a very specific subset of the data. This could, however, be due to the substantially smaller dataset size in those instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A closer look at linear models\n",
    "\n",
    "Here, we test the following linear models in the case of high-quality data, and explicitly low-quality data. The models we will be testing are LogisticRegression (classification), LDA, and a custom ErrorLDA model which should take into account the various RD values (roughly interpreted as a measurement error on the player ELOs, with the \"true\" ELOs being unknown values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced models\n",
    "import xgboost as xgb\n",
    "import errorlda\n",
    "import importlib\n",
    "\n",
    "# Just in case we make changes to this model.\n",
    "importlib.reload(errorlda)\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2020, 2022+1)\n",
    "\n",
    "models = {'lr': LogisticRegression(penalty=None, max_iter=10000),\n",
    "          'lda': LinearDiscriminantAnalysis(),\n",
    "          'errorlda': errorlda.ErrorLDA(),\n",
    "          #'errorlda_scaling': errorlda.ErrorLDA(),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "data_modes = ['low_quality', 'high_quality']\n",
    "\n",
    "for data_mode in data_modes:\n",
    "    for i, y in enumerate(tqdm(years)):\n",
    "        # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "        # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "        dataset_train_df = dataset_df[(dataset_df['start'] >= datetime.datetime(2016,1,1)) &\n",
    "                                    (dataset_df['end'] <= datetime.datetime(y,12,31)) &\n",
    "                                    (low_quality_filter if data_mode == 'low_quality' else quality_filter)]\n",
    "        dataset_test_df = dataset_df[(dataset_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                    (dataset_df['end'] <= datetime.datetime(y+1,12,31)) &\n",
    "                                    (low_quality_filter if data_mode == 'low_quality' else quality_filter)]\n",
    "        \n",
    "        for j, name in enumerate(models):\n",
    "            y_prob = None\n",
    "            y_pred = None\n",
    "\n",
    "            # Basically, all of these require slightly different syntax and restriction of features\n",
    "            # * ErrorLDA to include the variances (RD values)\n",
    "            # * LDA to just use the the ELO values without RD values (mainly to compare to ErrorLDA)\n",
    "            match name:\n",
    "                case 'lr' | 'lda':\n",
    "                    models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'])\n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_all_elo])\n",
    "\n",
    "                case 'errorlda' | 'errorlda_scaling':\n",
    "                    models[name] = errorlda.ErrorLDA() # Not sure if I've implemented .fit() to reset everything upon every new fit.\n",
    "\n",
    "                    # Experimental pre-scaling of the RD values\n",
    "                    pre_scaler = np.diag([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "                    models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'],\n",
    "                                    X_train_errors=dataset_train_df[features_all_rd].apply(lambda row: np.diag(row.to_numpy() ** 2) @ pre_scaler, axis=1),\n",
    "                                    error_scaling=(True if name == 'errorlda_scaling' else False)) # Note that error-scaling is slower by like a factor of 8, unfortunately\n",
    "                    \n",
    "                    # Literally just for numerical stability, in case some eigenvalues are near zero.\n",
    "                    # This will add 1 to each eigenvalue.\n",
    "                    models[name].variance += np.identity(len(features_all_rd))\n",
    "                    \n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_all_elo],\n",
    "                                                        X_error=dataset_test_df[features_all_rd].apply(lambda row: np.diag(row.to_numpy() ** 2) @ pre_scaler, axis=1))\n",
    "\n",
    "                case 'xgb': # Special syntax required here.\n",
    "                    models[name].fit(dataset_train_df[features_all_elo + features_all_rd], dataset_train_df['winner'])\n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_all_elo + features_all_rd])\n",
    "\n",
    "            # Rest of the prediction code is the same among all models    \n",
    "            y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "            ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "            acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "    print(\"Scores involving all ELOs and RDs in \" + data_mode + \" mode\")\n",
    "    print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "                index=years,\n",
    "                columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best linear model\n",
    "\n",
    "At least from the looks of things, it does not appear that there is any noticeable difference in accuracy and log loss with the ErrorLDA model, compared to the other two linear models. As such, considering the enormous lack of speed in training these models, we will just stick with LogisticRegression() in such special cases for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using separate depending on the quality of the data\n",
    "\n",
    "Here, we try another experimental approach, where we split the data into different \"quality classes\" (depending on the RD values of each elo), and train linear models on each class (and test if it is better than XGBoost in each case). We then combine all of the better models into one ensemble, and see if the quality of the predictions improve at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data according to the various amount of updates (\"no data/low quality data/high quality data\")\n",
    "\n",
    "def get_quality_encoding(row):\n",
    "    result = ''\n",
    "\n",
    "    for update_col in features_all_updates:\n",
    "        #if row[update_col] >= 20.0:  # High quality\n",
    "        #    result += '3'\n",
    "        if row[update_col] >= 10.0:  # High quality\n",
    "            result += '2'\n",
    "        elif row[update_col] >= 3.0: # Low quality\n",
    "            result += '1'\n",
    "        else:                        # None (or basically none)\n",
    "            result += '0'\n",
    "    \n",
    "    return result\n",
    "\n",
    "dataset_df = dataset_df.copy() # Fixes \"copy of a slice\" nonsense\n",
    "dataset_df['quality_class'] = dataset_df.apply(get_quality_encoding, axis=1)\n",
    "\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splits off the data into different classes and applies distinct models to each\n",
    "class SplitRegression:\n",
    "    # min_data is the minimum number of occurences of a class to split it off\n",
    "    def __init__(self, min_data=1000,\n",
    "                 default_model=xgb.XGBClassifier()):\n",
    "        self.features = None # Basically the list of columns of the training data\n",
    "        self.min_data = min_data\n",
    "\n",
    "        self.large_classes = None\n",
    "        self.small_classes = None\n",
    "        self.zero_classes = None\n",
    "\n",
    "        self.large_models = {} # A dictionary of models, one for each class with lots of data\n",
    "        self.large_models_reduced_features = {} # Keep track of whether to only pass reduced features to each large model\n",
    "\n",
    "        # TODO: Possibly get rid of small/zero separation        \n",
    "        self.small_model = default_model # Just bundle together all small classes (except zero) into one group and apply a more complex model to them\n",
    "        self.zero_model = default_model # Apply another (possibly) separate model to the \"basically no data\" group\n",
    "\n",
    "        self.default_model = default_model\n",
    "\n",
    "    # Figure out what features we actually need to pull from (and ignore the ones that have basically zero data)\n",
    "    def class_to_features(self, c):\n",
    "        return [self.features[i] for i,x in enumerate(c) if x != '0']\n",
    "    \n",
    "    # Jankiness, but it works. If there is *technically* only one class in whatever we've split off here,\n",
    "    # then we should make sure to add the other class. Ideally with fake data.\n",
    "    def patch_missing_outcome(self, X_train, y_train):\n",
    "        old_len = len(X_train.index)\n",
    "\n",
    "        y_unique = list(y_train.unique()) # Could very well be empty, or just one value. This handles both cases.\n",
    "        for y in [y for y in [0.0, 1.0] if y not in y_unique]:\n",
    "            X_train = pd.concat([X_train, pd.DataFrame([1500.0] * len(X_train.columns), index=X_train.columns).T], axis=0) # Most canonical fake data\n",
    "            y_train = pd.concat([y_train, pd.Series([y])])\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    # Kind of assumes these are all dataframes and series-es\n",
    "    # X_class corresponds to a unique id of the form n_1 ... n_k for the class\n",
    "    # A value of n_i = 0 means we will not actually use data from that class\n",
    "    def fit(self, X, y, X_class):\n",
    "        # First and foremost, don't forget to fit the default model to all the data\n",
    "        # (Subsequent models will be compared to it, and it will be used when they don't have better performance)\n",
    "        self.default_model.fit(X,y)\n",
    "\n",
    "        self.features = list(X.columns)\n",
    "\n",
    "        class_counts = X_class.value_counts()\n",
    "\n",
    "        # The zero classes, treated as separate classes. Realistically, there should only be at most one.\n",
    "        self.zero_classes = [c for c in class_counts.index if self.class_to_features(c) == []]\n",
    "        \n",
    "        self.large_classes = [x for x in class_counts[class_counts >= self.min_data].index if x not in self.zero_classes]\n",
    "        self.small_classes = [x for x in class_counts[class_counts < self.min_data].index if x not in self.zero_classes]\n",
    "\n",
    "        # As we've seen, LogisticRegression does quite well on classes with lots of data.\n",
    "        # We kind of assume each class has slightly different means, hence the need to train separate models,\n",
    "        # and also just skip out entirely on passing it info from features with basically no info.\n",
    "        #\n",
    "        # Train and test such a model, and very importantly, actually see if it does better than the default!\n",
    "        for c in self.large_classes:\n",
    "            class_features = self.class_to_features(c)\n",
    "\n",
    "            # Just in case the outcome is constant on this training subset.\n",
    "            X_train, y_train = self.patch_missing_outcome(X[X_class == c], y[X_class == c])\n",
    "\n",
    "            # Gonna assume some (possibly very slight) correlation between past and future entries, and so we avoid shuffling\n",
    "            X_train_tt, X_train_ho, y_train_tt, y_train_ho = train_test_split(X_train, y_train, shuffle=False)\n",
    "\n",
    "            default_ll = log_loss(y_train_ho, self.default_model.predict_proba(X_train_ho))\n",
    "\n",
    "            test_model = LogisticRegression(penalty=None, max_iter=10000)\n",
    "\n",
    "            test_model.fit(X_train_tt[class_features], y_train_tt)\n",
    "            test_ll = log_loss(y_train_ho, test_model.predict_proba(X_train_ho[class_features]))\n",
    "\n",
    "            # Depending on if we stick with the default or the new model,\n",
    "            # we will also need to keep track of whether or not the features need to be reduced or not.\n",
    "            if test_ll <= 0.9 * default_ll: # Just to account for RNG in some way\n",
    "                test_model.fit(X_train[class_features], y_train) # Extra training data - let's not throw out the holdout set!\n",
    "                self.large_models[c] = test_model\n",
    "                self.large_models_reduced_features[c] = True\n",
    "            else:\n",
    "                self.large_models[c] = self.default_model\n",
    "                self.large_models_reduced_features[c] = False\n",
    "\n",
    "        # Now the zero classes (realistically at most one of them) get lumped together and have a single model used.\n",
    "        # Same for the small classes.\n",
    "        # Might as well toss all features in there, just in case it feels like extracting *some* kind of info, somehow.\n",
    "        small_class_filter = X_class.apply(lambda c: c in self.small_classes)\n",
    "        zero_class_filter = X_class.apply(lambda c: c in self.zero_classes)\n",
    "        \n",
    "        # TODO: Tuning of hyperparameters\n",
    "        self.small_model = xgb.XGBClassifier()\n",
    "        self.zero_model = xgb.XGBClassifier()        \n",
    "\n",
    "        # These could technically be empty, or have just one outcome\n",
    "        # Small class\n",
    "        X_train, y_train = self.patch_missing_outcome(X[small_class_filter], y[small_class_filter])\n",
    "        self.small_model.fit(X_train, y_train)\n",
    "        # Zero class\n",
    "        X_train, y_train = self.patch_missing_outcome(X[zero_class_filter], y[zero_class_filter])\n",
    "        self.zero_model.fit(X_train, y_train)\n",
    "\n",
    "    def predict_proba(self, X, X_class):\n",
    "\n",
    "        merged_df = pd.concat([X, X_class], axis=1)\n",
    "        merged_df.columns = list(X.columns) + ['quality_class']\n",
    "\n",
    "        # It is substantially more efficient to figure out which model applies to which row,\n",
    "        # use a groupby(), and feed the entire block of data into the model,\n",
    "        # rather than doing this entire operation row by row.\n",
    "\n",
    "        # Also, this breaks if we try to put the models directly in the dataframe.\n",
    "        # Let's instead just assign them numeric values.\n",
    "        model_list = [self.large_models[c] for c in self.large_models] + [self.zero_model, self.small_model]\n",
    "        model_to_num_dict = {}\n",
    "\n",
    "        for i, model in enumerate(model_list):\n",
    "            model_to_num_dict[model] = i\n",
    "\n",
    "        def assign_model(row):\n",
    "            c = row['quality_class']\n",
    "\n",
    "            if c in self.large_classes:\n",
    "                return model_to_num_dict[self.large_models[c]]\n",
    "            \n",
    "            # Not a large model. Perhaps zero?\n",
    "            if self.class_to_features(c) == []:\n",
    "                return model_to_num_dict[self.zero_model]\n",
    "            \n",
    "            # Only possibility is the \"small\" model. Just lump it in with the rest of the data.\n",
    "            return model_to_num_dict[self.small_model]\n",
    "        \n",
    "        merged_df['model'] = merged_df.apply(assign_model, axis=1)\n",
    "        merged_df['model_copy'] = merged_df['model'] # include_groups=True deprecation nonsense\n",
    "\n",
    "        # Run predict_proba on entire blocks of data that use the same model,\n",
    "        # rather than running it row by row (slow)!\n",
    "        def block_proba(df):\n",
    "            model_num = df.iloc[0]['model_copy']\n",
    "            c = df.iloc[0]['quality_class']\n",
    "            c_features = df.columns[:-2] # Ignore 'quality_class' and 'model_copy' columns\n",
    "\n",
    "            # Large model, might need to restrict features\n",
    "            # (usually this is the case if it's not pointing to the default model)\n",
    "            if model_num < len(self.large_models) and self.large_models_reduced_features[c]:\n",
    "                c_features = self.class_to_features(c)\n",
    "            \n",
    "            model = model_list[model_num]\n",
    "            \n",
    "            return pd.DataFrame(model.predict_proba(df[c_features]), index=df.index)\n",
    "        \n",
    "        \n",
    "        result = merged_df.groupby('model').apply(block_proba, include_groups=False)\n",
    "\n",
    "        # Note that this will have a two-layered index now.\n",
    "        # One for the model number, and one for the original index.\n",
    "        # Let's remove it.\n",
    "        result = result.droplevel(0)\n",
    "\n",
    "        # NOTE: The index is NOT the original order anymore, because of the above! Let's fix that\n",
    "        result = result.loc[X.index]\n",
    "\n",
    "        return result.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced models\n",
    "import xgboost as xgb\n",
    "import errorlda\n",
    "import importlib\n",
    "\n",
    "# Just in case we make changes to this model.\n",
    "importlib.reload(errorlda)\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2018, 2022+1)\n",
    "\n",
    "models = {'split': SplitRegression(min_data=5000),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "\n",
    "\n",
    "for i, y in enumerate(tqdm(years)):\n",
    "    # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "    # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "    dataset_train_df = dataset_df[(dataset_df['start'] >= datetime.datetime(2016,1,1)) &\n",
    "                                (dataset_df['end'] <= datetime.datetime(y,12,31))]\n",
    "    dataset_test_df = dataset_df[(dataset_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                (dataset_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "    \n",
    "    for j, name in enumerate(models):\n",
    "        y_prob = None\n",
    "        y_pred = None\n",
    "\n",
    "        # Basically, all of these require slightly different syntax and restriction of features\n",
    "        # * ErrorLDA to include the variances (RD values)\n",
    "        # * LDA to just use the the ELO values without RD values (mainly to compare to ErrorLDA)\n",
    "        match name:\n",
    "            case 'split': # Special syntax required here.\n",
    "                models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'], dataset_train_df['quality_class'])\n",
    "                y_prob = models[name].predict_proba(dataset_test_df[features_all_elo], dataset_test_df['quality_class'])\n",
    "            case 'xgb':\n",
    "                models[name].fit(dataset_train_df[features_all_elo + features_all_rd], dataset_train_df['winner'])\n",
    "                y_prob = models[name].predict_proba(dataset_test_df[features_all_elo + features_all_rd])\n",
    "\n",
    "        # Rest of the prediction code is the same among all models    \n",
    "        y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "        ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "        acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "print(\"Scores involving all ELOs and RDs in \" + data_mode + \" mode\")\n",
    "print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "            index=years,\n",
    "            columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the above linear/XGBoost ensemble\n",
    "\n",
    "Unfortunately, it again appears as if the performance of this ensemble is comparable (possibly ever so slightly worse) than untuned XGBoost. You'll have to take my word for it that this was tested on many different combinations of parameters and data test sets, and the results are remarkably consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "years = range(2021, 2022+1)\n",
    "restrict_to_top_8 = False\n",
    "\n",
    "# Actually partition the data ahead of time, to avoid doing it every time.\n",
    "# It will be the same, every single time.\n",
    "data_partitioned = {}\n",
    "\n",
    "for y in years: \n",
    "    dataset_train_df = dataset_df[(dataset_df['start'] >= datetime.datetime(2016,1,1)) &\n",
    "                                  (dataset_df['end'] <= datetime.datetime(y,12,31)) &\n",
    "                                  (dataset_df['top_8'] if restrict_to_top_8 else True)]\n",
    "    dataset_test_df = dataset_df[(dataset_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                 (dataset_df['end'] <= datetime.datetime(y+1,12,31)) &\n",
    "                                 (dataset_df['top_8'] if restrict_to_top_8 else True)]\n",
    "    \n",
    "    data_partitioned[y] = (dataset_train_df, dataset_test_df)\n",
    "\n",
    "# Baseline results to compare to.\n",
    "xgb_baseline = xgb.XGBClassifier()\n",
    "results_baseline = np.zeros(len(years))\n",
    "\n",
    "for i,y in enumerate(years):\n",
    "    dataset_train_df = data_partitioned[y][0]\n",
    "    dataset_test_df = data_partitioned[y][1]\n",
    "\n",
    "    xgb_baseline.fit(dataset_train_df[features_all_everything], dataset_train_df['winner'])    \n",
    "    y_prob = xgb_baseline.predict_proba(dataset_test_df[features_all_everything])\n",
    "\n",
    "    results_baseline[i] = log_loss(dataset_test_df['winner'], y_prob)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Years to train on. We will train on one and test on the following.\n",
    "    # Three years for vaguely decent cross-validation.\n",
    "    results = np.zeros(len(years))\n",
    "\n",
    "    for i,y in enumerate(years):\n",
    "\n",
    "        # Far more efficient to compute these only once, as they don't actually change.\n",
    "        dataset_train_df = data_partitioned[y][0]\n",
    "        dataset_test_df = data_partitioned[y][1]\n",
    "\n",
    "        max_depth        = trial.suggest_int(\"max_depth\", 2, 10, step=1)\n",
    "        learning_rate    = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "        n_estimators     = trial.suggest_int(\"n_estimators\", 50, 1000, step=50)\n",
    "        subsample        = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
    "        gamma            = trial.suggest_float(\"gamma\", 0.0, 5.0)\n",
    "        min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "\n",
    "        # Let's not regularize, due to the large ELO scores and small other values\n",
    "        #reg_lambda       = trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True)\n",
    "        #reg_alpha        = trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n",
    "\n",
    "        model = xgb.XGBClassifier(max_depth=max_depth,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  subsample=subsample,\n",
    "                                  colsample_bytree=colsample_bytree,\n",
    "                                  gamma=gamma,\n",
    "                                  min_child_weight=min_child_weight)\n",
    "        \n",
    "        model.fit(dataset_train_df[features_all_everything], dataset_train_df['winner'])\n",
    "\n",
    "        y_prob = model.predict_proba(dataset_test_df[features_all_everything])\n",
    "        results[i] = log_loss(dataset_test_df['winner'], y_prob)\n",
    "\n",
    "    # See how much we've improved over the baseline.\n",
    "    # Measured as the average fractional decrease in log loss\n",
    "\n",
    "    return (results_baseline / results).mean()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100, timeout=1800)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Skippable code ends\n",
    "\n",
    "Resume running from here to train and save the final model. A small note that the previous cell is for running hyperparameter tuning on the final model, but that optimal values are already given in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size (all data):  247608\n",
      "Test set size (top 8):  58243\n",
      "\n",
      "'Who has the higher ELO' all-data test\n",
      "Accuracy:  77.54\n",
      "'Who has the higher ELO' top8 test\n",
      "Accuracy:  73.88\n",
      "\n",
      "XGBoost (default elo only) all-data test\n",
      "Log loss:  0.456\n",
      "Accuracy:  79.04\n",
      "XGBoost (default elo only) top8 test\n",
      "Log loss:  0.527\n",
      "Accuracy:  74.11\n",
      "\n",
      "XGBoost (all features) all-data test\n",
      "Log loss:  0.439\n",
      "Accuracy:  79.86\n",
      "XGBoost (all features) top8 test\n",
      "Log loss:  0.512\n",
      "Accuracy:  74.97\n",
      "                  Feature   Weight        Gain         Cover\n",
      "0          p1_default_elo  12875.0  383.010071   9716.694336\n",
      "1          p2_default_elo  12405.0  285.458893   9982.966797\n",
      "27         p2/m1_alt3_elo   8370.0   42.037930   4743.486816\n",
      "24         p1/m1_alt3_elo   8307.0   30.942427   4671.899414\n",
      "2           p1_default_rd   7640.0   29.098961   4247.143555\n",
      "3           p2_default_rd   6827.0   32.539692   3837.238281\n",
      "4      p1_default_updates   5326.0   44.226654   7225.262695\n",
      "5      p2_default_updates   5292.0   46.274437   7735.509277\n",
      "18      p1/m1/m1_alt2_elo   4903.0   21.786356   4345.327148\n",
      "21      p2/m1/m1_alt2_elo   4851.0   28.986639   5293.540039\n",
      "28          p2/m1_alt3_rd   4498.0   12.588698   3440.138672\n",
      "25          p1/m1_alt3_rd   4474.0   12.636799   2752.572510\n",
      "6               matchup_1   3468.0   74.417542  11458.651367\n",
      "22       p2/m1/m1_alt2_rd   3237.0    9.648394   3119.121338\n",
      "19       p1/m1/m1_alt2_rd   2934.0    9.397090   2881.300049\n",
      "17            p2_m1_usage   2824.0   10.916095   2938.651611\n",
      "16            p1_m1_usage   2648.0   10.181857   2877.623291\n",
      "23  p2/m1/m1_alt2_updates   2502.0   22.301147   8018.361328\n",
      "20  p1/m1/m1_alt2_updates   2355.0   23.045538   7709.516602\n",
      "29     p2/m1_alt3_updates   1889.0   10.636058   3542.263672\n",
      "26     p1/m1_alt3_updates   1878.0   10.591161   3047.087646\n",
      "7               matchup_2    944.0   38.115749   9168.270508\n",
      "8               matchup_3    792.0   18.364569   8954.365234\n",
      "9               matchup_4    723.0   13.789273  10101.967773\n",
      "10              matchup_5    549.0   12.200754   7729.642090\n",
      "11              matchup_6    368.0    9.838923  10971.309570\n",
      "12              matchup_7    245.0    9.236521   5434.021973\n",
      "14              matchup_9    180.0    9.507809   5600.261230\n",
      "13              matchup_8    178.0    8.287845   3509.191406\n",
      "15             matchup_10    138.0    7.822376   2684.300781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Train the final single-set model to use for top 8 prediction\n",
    "# Actually, we can train two. One for generic sets (to see how well someone has performed up to that point)\n",
    "# and one more optimized for top 8 sets only.\n",
    "dataset_train_df = dataset_df[(dataset_df['start'] >= datetime.datetime(2016,1,1)) &\n",
    "                              (dataset_df['end'] <= datetime.datetime(2022,12,31))]\n",
    "\n",
    "dataset_test_df = dataset_df[(dataset_df['start'] >= datetime.datetime(2024,1,1)) &\n",
    "                             (dataset_df['end'] <= datetime.datetime(2024,12,31))]\n",
    "\n",
    "dataset_test_top8_df = dataset_df[(dataset_df['start'] >= datetime.datetime(2024,1,1)) &\n",
    "                                  (dataset_df['end'] <= datetime.datetime(2024,12,31)) &\n",
    "                                  (dataset_df['top_8'])]\n",
    "\n",
    "\n",
    "print(\"Test set size (all data): \", dataset_test_df.shape[0])\n",
    "print(\"Test set size (top 8): \",    dataset_test_top8_df.shape[0])\n",
    "\n",
    "print()\n",
    "\n",
    "# Purely looking at who has the higher ELO\n",
    "print(\"'Who has the higher ELO' all-data test\")\n",
    "print(\"Accuracy: \", round(100.0 * accuracy_score(dataset_test_df['winner'],\n",
    "                                                 (dataset_test_df['p1_default_elo'] >= dataset_test_df['p2_default_elo']).astype(float)), 2))\n",
    "\n",
    "print(\"'Who has the higher ELO' top8 test\")\n",
    "print(\"Accuracy: \", round(100.0 * accuracy_score(dataset_test_top8_df['winner'],\n",
    "                                                 (dataset_test_top8_df['p1_default_elo'] >= dataset_test_top8_df['p2_default_elo']).astype(float)), 2))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "baseline_model = xgb.XGBClassifier()\n",
    "baseline_model.fit(dataset_train_df[features_default_elo], dataset_train_df['winner'])\n",
    "\n",
    "y_prob = baseline_model.predict_proba(dataset_test_df[features_default_elo])\n",
    "print(\"XGBoost (default elo only) all-data test\")\n",
    "print(\"Log loss: \", round(log_loss(dataset_test_df['winner'], y_prob), 3))\n",
    "print(\"Accuracy: \", round(100.0 * accuracy_score(dataset_test_df['winner'], y_prob[:,1] >= 0.5), 2))\n",
    "\n",
    "y_prob = baseline_model.predict_proba(dataset_test_top8_df[features_default_elo])\n",
    "print(\"XGBoost (default elo only) top8 test\")\n",
    "print(\"Log loss: \", round(log_loss(dataset_test_top8_df['winner'], y_prob), 3))\n",
    "print(\"Accuracy: \", round(100.0 * accuracy_score(dataset_test_top8_df['winner'], y_prob[:,1] >= 0.5), 2))\n",
    "\n",
    "print()\n",
    "\n",
    "single_set_model = xgb.XGBClassifier(max_depth=8,\n",
    "                                     learning_rate=0.011288,\n",
    "                                     n_estimators=850,\n",
    "                                     subsample=0.60077,\n",
    "                                     colsample_bytree=0.98525,\n",
    "                                     gamma=5,\n",
    "                                     min_child_weight=10)\n",
    "\n",
    "single_set_model.fit(dataset_train_df[features_all_everything], dataset_train_df['winner'])\n",
    "\n",
    "# Test on 2024 data to see what the final performance roughly looks like\n",
    "y_prob = single_set_model.predict_proba(dataset_test_df[features_all_everything])\n",
    "print(\"XGBoost (all features) all-data test\")\n",
    "print(\"Log loss: \", round(log_loss(dataset_test_df['winner'], y_prob), 3))\n",
    "print(\"Accuracy: \", round(100.0 * accuracy_score(dataset_test_df['winner'], y_prob[:,1] >= 0.5), 2))\n",
    "\n",
    "y_prob = single_set_model.predict_proba(dataset_test_top8_df[features_all_everything])\n",
    "print(\"XGBoost (all features) top8 test\")\n",
    "print(\"Log loss: \", round(log_loss(dataset_test_top8_df['winner'], y_prob), 3))\n",
    "print(\"Accuracy: \", round(100.0 * accuracy_score(dataset_test_top8_df['winner'], y_prob[:,1] >= 0.5), 2))\n",
    "\n",
    "# See feature importance\n",
    "importance_weight = single_set_model.get_booster().get_score(importance_type='weight')\n",
    "importance_gain = single_set_model.get_booster().get_score(importance_type='gain')\n",
    "importance_cover = single_set_model.get_booster().get_score(importance_type='cover')\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': importance_weight.keys(),  # Map f0, f1, ... to feature names\n",
    "    'Weight': [importance_weight.get(f, 0) for f in importance_weight.keys()],\n",
    "    'Gain': [importance_gain.get(f, 0) for f in importance_weight.keys()],\n",
    "    'Cover': [importance_cover.get(f, 0) for f in importance_weight.keys()]\n",
    "}).sort_values(by='Weight', ascending=False)\n",
    "\n",
    "print(importance_df)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Who has the higher ELO' model accuracies\n",
      "\n",
      "Baseline XGBoost model (default elo only) accuracies\n",
      "\n",
      "Single Set XGBoost model (all features) accuracies\n",
      "\n",
      "Accuracies for Higher ELO:\n",
      "Consistent Accuracy: 79.70%\n",
      "Inconsistent Accuracy: 79.76%\n",
      "None Accuracy: 75.43%\n",
      "Overall Accuracy: 77.54%\n",
      "\n",
      "Accuracies for Baseline XGBoost:\n",
      "Consistent Accuracy: 79.62%\n",
      "Inconsistent Accuracy: 79.72%\n",
      "None Accuracy: 78.43%\n",
      "Overall Accuracy: 79.04%\n",
      "\n",
      "Accuracies for Single Set XGBoost:\n",
      "Consistent Accuracy: 80.40%\n",
      "Inconsistent Accuracy: 80.57%\n",
      "None Accuracy: 79.26%\n",
      "Overall Accuracy: 79.86%\n"
     ]
    }
   ],
   "source": [
    "# Define subsets based on 'matchup_1' in the test data\n",
    "test_consistent_df = dataset_test_df[dataset_test_df['matchup_1'] == 1]\n",
    "test_inconsistent_df = dataset_test_df[dataset_test_df['matchup_1'] == 0]\n",
    "test_none_df = dataset_test_df[dataset_test_df['matchup_1'] == 0.5]\n",
    "\n",
    "# Initialize dictionaries to store accuracies\n",
    "model_names = ['Higher ELO', 'Baseline XGBoost', 'Single Set XGBoost']\n",
    "accuracies = {model: {} for model in model_names}\n",
    "\n",
    "# Colors matching the pie chart\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# -----------------------------\n",
    "# 'Who has the higher ELO' model accuracies\n",
    "# -----------------------------\n",
    "print(\"'Who has the higher ELO' model accuracies\")\n",
    "\n",
    "# Overall accuracy\n",
    "who_higher_overall_accuracy = accuracy_score(\n",
    "    dataset_test_df['winner'],\n",
    "    (dataset_test_df['p1_default_elo'] >= dataset_test_df['p2_default_elo']).astype(float)\n",
    ")\n",
    "\n",
    "# Consistent subset\n",
    "who_higher_consistent_predictions = (test_consistent_df['p1_default_elo'] >= test_consistent_df['p2_default_elo']).astype(float)\n",
    "who_higher_consistent_accuracy = accuracy_score(test_consistent_df['winner'], who_higher_consistent_predictions)\n",
    "\n",
    "# Inconsistent subset\n",
    "who_higher_inconsistent_predictions = (test_inconsistent_df['p1_default_elo'] >= test_inconsistent_df['p2_default_elo']).astype(float)\n",
    "who_higher_inconsistent_accuracy = accuracy_score(test_inconsistent_df['winner'], who_higher_inconsistent_predictions)\n",
    "\n",
    "# None subset\n",
    "who_higher_none_predictions = (test_none_df['p1_default_elo'] >= test_none_df['p2_default_elo']).astype(float)\n",
    "who_higher_none_accuracy = accuracy_score(test_none_df['winner'], who_higher_none_predictions)\n",
    "\n",
    "# Store accuracies\n",
    "accuracies['Higher ELO']['Consistent'] = who_higher_consistent_accuracy * 100\n",
    "accuracies['Higher ELO']['Inconsistent'] = who_higher_inconsistent_accuracy * 100\n",
    "accuracies['Higher ELO']['None'] = who_higher_none_accuracy * 100\n",
    "accuracies['Higher ELO']['Overall'] = who_higher_overall_accuracy * 100\n",
    "\n",
    "# -----------------------------\n",
    "# Baseline XGBoost model (default elo only)\n",
    "# -----------------------------\n",
    "print(\"\\nBaseline XGBoost model (default elo only) accuracies\")\n",
    "\n",
    "# Overall predictions\n",
    "y_prob_overall = baseline_model.predict_proba(dataset_test_df[features_default_elo])\n",
    "baseline_overall_accuracy = accuracy_score(dataset_test_df['winner'], y_prob_overall[:,1] >= 0.5)\n",
    "\n",
    "# Consistent subset\n",
    "y_prob_consistent = baseline_model.predict_proba(test_consistent_df[features_default_elo])\n",
    "baseline_consistent_accuracy = accuracy_score(test_consistent_df['winner'], y_prob_consistent[:,1] >= 0.5)\n",
    "\n",
    "# Inconsistent subset\n",
    "y_prob_inconsistent = baseline_model.predict_proba(test_inconsistent_df[features_default_elo])\n",
    "baseline_inconsistent_accuracy = accuracy_score(test_inconsistent_df['winner'], y_prob_inconsistent[:,1] >= 0.5)\n",
    "\n",
    "# None subset\n",
    "y_prob_none = baseline_model.predict_proba(test_none_df[features_default_elo])\n",
    "baseline_none_accuracy = accuracy_score(test_none_df['winner'], y_prob_none[:,1] >= 0.5)\n",
    "\n",
    "# Store accuracies\n",
    "accuracies['Baseline XGBoost']['Consistent'] = baseline_consistent_accuracy * 100\n",
    "accuracies['Baseline XGBoost']['Inconsistent'] = baseline_inconsistent_accuracy * 100\n",
    "accuracies['Baseline XGBoost']['None'] = baseline_none_accuracy * 100\n",
    "accuracies['Baseline XGBoost']['Overall'] = baseline_overall_accuracy * 100\n",
    "\n",
    "# -----------------------------\n",
    "# Single Set XGBoost model (all features)\n",
    "# -----------------------------\n",
    "print(\"\\nSingle Set XGBoost model (all features) accuracies\")\n",
    "\n",
    "# Overall predictions\n",
    "y_prob_overall = single_set_model.predict_proba(dataset_test_df[features_all_everything])\n",
    "single_set_overall_accuracy = accuracy_score(dataset_test_df['winner'], y_prob_overall[:,1] >= 0.5)\n",
    "\n",
    "# Consistent subset\n",
    "y_prob_consistent = single_set_model.predict_proba(test_consistent_df[features_all_everything])\n",
    "single_set_consistent_accuracy = accuracy_score(test_consistent_df['winner'], y_prob_consistent[:,1] >= 0.5)\n",
    "\n",
    "# Inconsistent subset\n",
    "y_prob_inconsistent = single_set_model.predict_proba(test_inconsistent_df[features_all_everything])\n",
    "single_set_inconsistent_accuracy = accuracy_score(test_inconsistent_df['winner'], y_prob_inconsistent[:,1] >= 0.5)\n",
    "\n",
    "# None subset\n",
    "y_prob_none = single_set_model.predict_proba(test_none_df[features_all_everything])\n",
    "single_set_none_accuracy = accuracy_score(test_none_df['winner'], y_prob_none[:,1] >= 0.5)\n",
    "\n",
    "# Store accuracies\n",
    "accuracies['Single Set XGBoost']['Consistent'] = single_set_consistent_accuracy * 100\n",
    "accuracies['Single Set XGBoost']['Inconsistent'] = single_set_inconsistent_accuracy * 100\n",
    "accuracies['Single Set XGBoost']['None'] = single_set_none_accuracy * 100\n",
    "accuracies['Single Set XGBoost']['Overall'] = single_set_overall_accuracy * 100\n",
    "\n",
    "# Print the accuracies for each model\n",
    "for model in model_names:\n",
    "    print(f\"\\nAccuracies for {model}:\")\n",
    "    for category in ['Consistent', 'Inconsistent', 'None', 'Overall']:\n",
    "        print(f\"{category} Accuracy: {accuracies[model][category]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Categories and colors\n",
    "categories = ['Consistent', 'Inconsistent', 'None', 'Overall']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#9467bd']  # Added a color for 'Overall'\n",
    "\n",
    "# Prepare data for plotting\n",
    "x = np.arange(len(categories))  # Label locations\n",
    "width = 0.25  # Width of the bars\n",
    "\n",
    "# Collect accuracies for each model\n",
    "higher_elo_accuracies = [accuracies['Higher ELO'][cat] for cat in categories]\n",
    "baseline_accuracies = [accuracies['Baseline XGBoost'][cat] for cat in categories]\n",
    "single_set_accuracies = [accuracies['Single Set XGBoost'][cat] for cat in categories]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "rects1 = ax.bar(x - width, higher_elo_accuracies, width, label='Higher ELO', alpha=0.7)\n",
    "rects2 = ax.bar(x, baseline_accuracies, width, label='Baseline XGBoost', alpha=0.7)\n",
    "rects3 = ax.bar(x + width, single_set_accuracies, width, label='Single Set XGBoost', alpha=0.7)\n",
    "\n",
    "# Set colors for categories\n",
    "for i, rect in enumerate(rects1):\n",
    "    rect.set_color(colors[i])\n",
    "for i, rect in enumerate(rects2):\n",
    "    rect.set_color(colors[i])\n",
    "for i, rect in enumerate(rects3):\n",
    "    rect.set_color(colors[i])\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Model Accuracies by Category')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "# Function to add labels above bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.1f}%',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # Offset label above bar\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the final model\n",
    "# with open(data_path + 'single_set_model.pkl','wb') as f:\n",
    "#     pickle.dump(single_set_model, f)\n",
    "\n",
    "with open(data_path + 'single_set_model.pkl', 'rb') as f:\n",
    "   single_set_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
