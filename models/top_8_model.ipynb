{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "#import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from glicko2 import Player\n",
    "import multiprocessing\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "if os.path.exists('/workspace/data_2'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data_2/'\n",
    "else:\n",
    "    data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the single-set model saved from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024 single-set performance metrics\n",
      "\n",
      "On all sets:\n",
      "Log loss:  0.444\n",
      "Accuracy:  79.5\n",
      "\n",
      "Restricting to top 8 sets only:\n",
      "Log loss:  0.514\n",
      "Accuracy:  75.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "dataset_df = pd.read_pickle(data_path + 'dataset_full.pkl')\n",
    "dataset_df.sort_index(inplace=True) # For convenience, mostly. Not really necessary, we use .loc[] anyways\n",
    "\n",
    "# Note that there is no real reason to keep separately computing the individual probabilities of winning each individual set.\n",
    "# Let's just compute them all at once here.\n",
    "\n",
    "single_set_model = None\n",
    "with open(data_path + 'single_set_model.pkl', 'rb') as f:\n",
    "    single_set_model = pickle.load(f)\n",
    "\n",
    "# Make sure these match up with what features the model was trained on.\n",
    "features_all_everything = ['p1_default_elo', 'p2_default_elo', 'p1_default_rd', 'p2_default_rd',\n",
    "       'p1_default_updates', 'p2_default_updates', 'matchup_1', 'matchup_2',\n",
    "       'matchup_3', 'matchup_4', 'matchup_5', 'matchup_6', 'matchup_7',\n",
    "       'matchup_8', 'matchup_9', 'matchup_10', 'p1_m1_usage', 'p2_m1_usage',\n",
    "       'p1/m1/m1_alt2_elo', 'p1/m1/m1_alt2_rd', 'p1/m1/m1_alt2_updates',\n",
    "       'p2/m1/m1_alt2_elo', 'p2/m1/m1_alt2_rd', 'p2/m1/m1_alt2_updates',\n",
    "       'p1/m1_alt3_elo', 'p1/m1_alt3_rd', 'p1/m1_alt3_updates',\n",
    "       'p2/m1_alt3_elo', 'p2/m1_alt3_rd', 'p2/m1_alt3_updates']\n",
    "\n",
    "dataset_df['p1_win_prob'] = single_set_model.predict_proba(dataset_df[features_all_everything])[:,1]\n",
    "\n",
    "# As a sanity check, let's verify the accuracy and log loss on 2024 data\n",
    "# Total accuracy and log loss (including lots of data that the model was trained on)\n",
    "date_filter = (dataset_df['start'] >= datetime.datetime(2024,1,1)) & (dataset_df['end'] <= datetime.datetime(2024,12,31))\n",
    "print(\"2024 single-set performance metrics\")\n",
    "print()\n",
    "print(\"On all sets:\")\n",
    "print(\"Log loss: \", round(log_loss(dataset_df[date_filter]['winner'], dataset_df[date_filter]['p1_win_prob']), 3))\n",
    "print(\"Accuracy: \", round(100.0 * accuracy_score(dataset_df[date_filter]['winner'], dataset_df[date_filter]['p1_win_prob'] >= 0.5), 1))\n",
    "print()\n",
    "print(\"Restricting to top 8 sets only:\")\n",
    "print(\"Log loss: \", round(log_loss(dataset_df[date_filter & dataset_df['top_8']]['winner'], dataset_df[date_filter & dataset_df['top_8']]['p1_win_prob']), 3))\n",
    "print(\"Accuracy: \", round(100.0 * accuracy_score(dataset_df[date_filter & dataset_df['top_8']]['winner'], dataset_df[date_filter & dataset_df['top_8']]['p1_win_prob'] >= 0.5), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean up the tournament dataframe\n",
    "\n",
    "In particular, this dataframe contains info on all top 8 players, along with the paths that they took to get to where they are in the tournament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner_id</th>\n",
       "      <th>LN_A_p1</th>\n",
       "      <th>LN_A_p2</th>\n",
       "      <th>LN_B_p1</th>\n",
       "      <th>LN_B_p2</th>\n",
       "      <th>WSF_A_p1</th>\n",
       "      <th>WSF_A_p2</th>\n",
       "      <th>WSF_B_p1</th>\n",
       "      <th>WSF_B_p2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18988</th>\n",
       "      <td>374889</td>\n",
       "      <td>212438</td>\n",
       "      <td>262548</td>\n",
       "      <td>378028</td>\n",
       "      <td>1727716</td>\n",
       "      <td>1971084</td>\n",
       "      <td>374889</td>\n",
       "      <td>29873</td>\n",
       "      <td>21515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18995</th>\n",
       "      <td>2931588</td>\n",
       "      <td>1652036</td>\n",
       "      <td>2720652</td>\n",
       "      <td>2512785</td>\n",
       "      <td>3188220</td>\n",
       "      <td>2931588</td>\n",
       "      <td>1470721</td>\n",
       "      <td>3188222</td>\n",
       "      <td>2551763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18992</th>\n",
       "      <td>701767</td>\n",
       "      <td>2670568</td>\n",
       "      <td>1693244</td>\n",
       "      <td>2159124</td>\n",
       "      <td>2382715</td>\n",
       "      <td>2998349</td>\n",
       "      <td>579172</td>\n",
       "      <td>2207927</td>\n",
       "      <td>701767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18999</th>\n",
       "      <td>26574</td>\n",
       "      <td>1482652</td>\n",
       "      <td>264041</td>\n",
       "      <td>2326935</td>\n",
       "      <td>650242</td>\n",
       "      <td>3024047</td>\n",
       "      <td>26574</td>\n",
       "      <td>674084</td>\n",
       "      <td>557126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18994</th>\n",
       "      <td>631158</td>\n",
       "      <td>2559523</td>\n",
       "      <td>1240442</td>\n",
       "      <td>1077370</td>\n",
       "      <td>3013285</td>\n",
       "      <td>631158</td>\n",
       "      <td>1007980</td>\n",
       "      <td>3188449</td>\n",
       "      <td>2557927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39703</th>\n",
       "      <td>2788878</td>\n",
       "      <td>3596098</td>\n",
       "      <td>3540721</td>\n",
       "      <td>4110509</td>\n",
       "      <td>287537</td>\n",
       "      <td>2410418</td>\n",
       "      <td>2788878</td>\n",
       "      <td>2956817</td>\n",
       "      <td>3815971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39718</th>\n",
       "      <td>180292</td>\n",
       "      <td>2298994</td>\n",
       "      <td>519020</td>\n",
       "      <td>36152</td>\n",
       "      <td>4916</td>\n",
       "      <td>36285</td>\n",
       "      <td>180292</td>\n",
       "      <td>37001</td>\n",
       "      <td>25701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39644</th>\n",
       "      <td>19641</td>\n",
       "      <td>405463</td>\n",
       "      <td>1445708</td>\n",
       "      <td>246491</td>\n",
       "      <td>769516</td>\n",
       "      <td>19641</td>\n",
       "      <td>2249893</td>\n",
       "      <td>3822249</td>\n",
       "      <td>148391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39077</th>\n",
       "      <td>267849</td>\n",
       "      <td>533918</td>\n",
       "      <td>56918</td>\n",
       "      <td>55591</td>\n",
       "      <td>342875</td>\n",
       "      <td>893866</td>\n",
       "      <td>267849</td>\n",
       "      <td>216979</td>\n",
       "      <td>512704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31274</th>\n",
       "      <td>609081</td>\n",
       "      <td>34804</td>\n",
       "      <td>25647</td>\n",
       "      <td>7564</td>\n",
       "      <td>1602510</td>\n",
       "      <td>698885</td>\n",
       "      <td>136697</td>\n",
       "      <td>10565</td>\n",
       "      <td>609081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15295 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      winner_id  LN_A_p1  LN_A_p2  LN_B_p1  LN_B_p2 WSF_A_p1 WSF_A_p2  \\\n",
       "18988    374889   212438   262548   378028  1727716  1971084   374889   \n",
       "18995   2931588  1652036  2720652  2512785  3188220  2931588  1470721   \n",
       "18992    701767  2670568  1693244  2159124  2382715  2998349   579172   \n",
       "18999     26574  1482652   264041  2326935   650242  3024047    26574   \n",
       "18994    631158  2559523  1240442  1077370  3013285   631158  1007980   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "39703   2788878  3596098  3540721  4110509   287537  2410418  2788878   \n",
       "39718    180292  2298994   519020    36152     4916    36285   180292   \n",
       "39644     19641   405463  1445708   246491   769516    19641  2249893   \n",
       "39077    267849   533918    56918    55591   342875   893866   267849   \n",
       "31274    609081    34804    25647     7564  1602510   698885   136697   \n",
       "\n",
       "      WSF_B_p1 WSF_B_p2  \n",
       "18988    29873    21515  \n",
       "18995  3188222  2551763  \n",
       "18992  2207927   701767  \n",
       "18999   674084   557126  \n",
       "18994  3188449  2557927  \n",
       "...        ...      ...  \n",
       "39703  2956817  3815971  \n",
       "39718    37001    25701  \n",
       "39644  3822249   148391  \n",
       "39077   216979   512704  \n",
       "31274    10565   609081  \n",
       "\n",
       "[15295 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tournament_df = pd.read_pickle(data_path + 'top_8_tournament_previous_sets_and_results_with_winners_df')\n",
    "\n",
    "# Filter down to tournaments which actually have valid top 8 data, and previous data on getting there.\n",
    "tournament_df = tournament_df.loc[tournament_df[['LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2',\n",
    "                                                 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2',\n",
    "                                                 'LN_A_p1_non_top_8_sets', 'LN_A_p2_non_top_8_sets',\n",
    "                                                 'LN_B_p1_non_top_8_sets', 'LN_B_p2_non_top_8_sets',\n",
    "                                                 'WSF_A_p1_non_top_8_sets', 'WSF_A_p2_non_top_8_sets',\n",
    "                                                 'WSF_B_p1_non_top_8_sets', 'WSF_B_p2_non_top_8_sets']].dropna().index]\n",
    "\n",
    "\n",
    "# Very rarely (not sure where the problem is) you get something not actually in the single-set dataframe\n",
    "# It is not actually that common though, so let's just delete those instances.\n",
    "def references_valid_sets(prev_sets):\n",
    "    for x in prev_sets:\n",
    "        if x[0] not in dataset_df.index:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "filter = tournament_df[['LN_A_p1_non_top_8_sets', 'LN_A_p2_non_top_8_sets',\n",
    "                        'LN_B_p1_non_top_8_sets', 'LN_B_p2_non_top_8_sets',\n",
    "                        'WSF_A_p1_non_top_8_sets', 'WSF_A_p2_non_top_8_sets',\n",
    "                        'WSF_B_p1_non_top_8_sets', 'WSF_B_p2_non_top_8_sets']].map(references_valid_sets).all(axis=1)\n",
    "\n",
    "tournament_df = tournament_df[filter]\n",
    "\n",
    "# Likewise, some of these sets don't seem to have a valid winner\n",
    "tournament_df = tournament_df[~tournament_df['winner_id'].isna()]\n",
    "\n",
    "# A bit more cleanup, for sanity\n",
    "min_date = datetime.datetime(2015,1,1)\n",
    "max_date = datetime.datetime(2024,12,31)\n",
    "\n",
    "tournament_df = tournament_df[(tournament_df['start'] >= min_date) &\n",
    "                              (tournament_df['end'] >= min_date) &\n",
    "                              (tournament_df['start'] <= max_date) &\n",
    "                              (tournament_df['end'] <= max_date)]\n",
    "\n",
    "# We will only be dealing with data from 2023 onwards, because the single-set predictor that we will be using\n",
    "# was trained on data up to the end of 2022, and we don't want it leaking data.\n",
    "# This will also speed up computations by not performing them on data we don't care about.\n",
    "tournament_df = tournament_df[tournament_df['start'] >= datetime.datetime(2023,1,1)]\n",
    "\n",
    "tournament_df.sort_values(by=['end', 'start'], inplace=True)\n",
    "\n",
    "tournament_df[['winner_id', 'LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2', 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit more feature engineering\n",
    "\n",
    "Here, we compute a \"score\" that shows how well a player is doing relative to their skill level and the skill level of their opponents in this tournament. If they are beating players well beyond their normal skill level, their score goes up. If they lose, it goes down.\n",
    "\n",
    "More specifically, we compute the probability that they win, and add the negative log of that probability if they indeed win. Losses are similar, but based on the probability that they lose, and subtracted instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1531865, True), (1531870, False), (1531891, True)]\n",
      "\n",
      "         p1_default_elo  p2_default_elo  winner  p1_win_prob\n",
      "1531865          1500.0          1500.0     1.0     0.503824\n",
      "1531870          1500.0          1500.0     1.0     0.503824\n",
      "1531891          1500.0          1500.0     1.0     0.503824\n",
      "\n",
      "Previous set score:  0.6855285167694092\n"
     ]
    }
   ],
   "source": [
    "# Compute a score based on how likely it was that they actually made it to the top.\n",
    "# A sort of sum of negative log probabilities, where wins are positive and losses are negative\n",
    "def prev_set_score(prev_sets):\n",
    "    result = 0\n",
    "\n",
    "    for x in prev_sets:\n",
    "        data = dataset_df.loc[x[0], ['winner', 'p1_win_prob']]\n",
    "        outcome = x[1]\n",
    "\n",
    "        # We don't know if this player is p1 or p2 in this list, but this can determine it without looking at player id\n",
    "        # Compare if (player we are interested in wins) vs (did p1 win)\n",
    "        if outcome == (data['winner'] == 1.0): # The player is p1\n",
    "            if outcome: # player wins, as p1\n",
    "                result += (-np.log(data['p1_win_prob']))\n",
    "            else:       # player loses, as p1\n",
    "                result -= (-np.log(1-data['p1_win_prob']))\n",
    "        else:                                  # The player is p2\n",
    "            if outcome: # player wins, as p2\n",
    "                result += (-np.log(1-data['p1_win_prob']))\n",
    "            else:       # player loses, as p2\n",
    "                result -= (-np.log(data['p1_win_prob']))\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example data (note that there is a consistent ELO throughout this entire dataset)\n",
    "prev_sets = tournament_df.iloc[10000]['LN_A_p1_non_top_8_sets']\n",
    "print(prev_sets)\n",
    "print()\n",
    "print(dataset_df.loc[[x[0] for x in prev_sets], ['p1_default_elo', 'p2_default_elo', 'winner', 'p1_win_prob']])\n",
    "print()\n",
    "print(\"Previous set score: \", prev_set_score(tournament_df.iloc[10000]['LN_A_p1_non_top_8_sets']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we will keep track of all of the necessary data for each player in the top 8 by keeping track of what their starting position was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_A_p1_non_top_8_sets</th>\n",
       "      <th>LN_A_p2_non_top_8_sets</th>\n",
       "      <th>LN_B_p1_non_top_8_sets</th>\n",
       "      <th>LN_B_p2_non_top_8_sets</th>\n",
       "      <th>WSF_A_p1_non_top_8_sets</th>\n",
       "      <th>WSF_A_p2_non_top_8_sets</th>\n",
       "      <th>WSF_B_p1_non_top_8_sets</th>\n",
       "      <th>WSF_B_p2_non_top_8_sets</th>\n",
       "      <th>LN_A_p1_non_top_8_sets_len</th>\n",
       "      <th>LN_A_p2_non_top_8_sets_len</th>\n",
       "      <th>...</th>\n",
       "      <th>WSF_B_p1_non_top_8_sets_len</th>\n",
       "      <th>WSF_B_p2_non_top_8_sets_len</th>\n",
       "      <th>LN_A_p1_non_top_8_sets_score</th>\n",
       "      <th>LN_A_p2_non_top_8_sets_score</th>\n",
       "      <th>LN_B_p1_non_top_8_sets_score</th>\n",
       "      <th>LN_B_p2_non_top_8_sets_score</th>\n",
       "      <th>WSF_A_p1_non_top_8_sets_score</th>\n",
       "      <th>WSF_A_p2_non_top_8_sets_score</th>\n",
       "      <th>WSF_B_p1_non_top_8_sets_score</th>\n",
       "      <th>WSF_B_p2_non_top_8_sets_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18988</th>\n",
       "      <td>[(1028338, True), (1028347, True), (1028351, F...</td>\n",
       "      <td>[(1028335, True), (1028345, True), (1028350, F...</td>\n",
       "      <td>[(1028337, True), (1028346, False), (1028361, ...</td>\n",
       "      <td>[(1028330, True), (1028341, True), (1028348, F...</td>\n",
       "      <td>[(1028342, True), (1028349, True)]</td>\n",
       "      <td>[(1028340, True), (1028348, True)]</td>\n",
       "      <td>[(1028344, True), (1028350, True)]</td>\n",
       "      <td>[(1028346, True), (1028351, True)]</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.371057</td>\n",
       "      <td>1.371057</td>\n",
       "      <td>2.056586</td>\n",
       "      <td>1.371057</td>\n",
       "      <td>1.371057</td>\n",
       "      <td>1.371057</td>\n",
       "      <td>1.371057</td>\n",
       "      <td>1.371057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18995</th>\n",
       "      <td>[(1028523, False), (1028528, True)]</td>\n",
       "      <td>[(1028522, False)]</td>\n",
       "      <td>[(1028521, False)]</td>\n",
       "      <td>[(1028519, True), (1028520, False)]</td>\n",
       "      <td>[(1028520, True)]</td>\n",
       "      <td>[(1028521, True)]</td>\n",
       "      <td>[(1028522, True)]</td>\n",
       "      <td>[(1028523, True)]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.685529</td>\n",
       "      <td>-0.685529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.685529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18992</th>\n",
       "      <td>[(1028465, True), (1028469, False), (1028478, ...</td>\n",
       "      <td>[(1028463, True), (1028468, False), (1028479, ...</td>\n",
       "      <td>[(1028459, True), (1028466, False), (1028477, ...</td>\n",
       "      <td>[(1028461, True), (1028467, False), (1028476, ...</td>\n",
       "      <td>[(1028460, True), (1028467, True)]</td>\n",
       "      <td>[(1028458, True), (1028466, True)]</td>\n",
       "      <td>[(1028464, True), (1028469, True)]</td>\n",
       "      <td>[(1028462, True), (1028468, True)]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.421395</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.949662</td>\n",
       "      <td>1.106924</td>\n",
       "      <td>1.371057</td>\n",
       "      <td>1.371057</td>\n",
       "      <td>1.371057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18999</th>\n",
       "      <td>[(1028610, True), (1028614, False), (1028622, ...</td>\n",
       "      <td>[(1028608, True), (1028613, False), (1028623, ...</td>\n",
       "      <td>[(1028606, True), (1028612, False), (1028624, ...</td>\n",
       "      <td>[(1028605, True), (1028611, False), (1028625, ...</td>\n",
       "      <td>[(1028607, True), (1028612, True)]</td>\n",
       "      <td>[(1028611, True)]</td>\n",
       "      <td>[(1028609, True), (1028614, True)]</td>\n",
       "      <td>[(1028613, True)]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>1.371057</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>1.371057</td>\n",
       "      <td>0.685529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18994</th>\n",
       "      <td>[(1028500, True), (1028504, False)]</td>\n",
       "      <td>[(1028501, True), (1028505, False), (1028510, ...</td>\n",
       "      <td>[(1028500, False), (1028511, True)]</td>\n",
       "      <td>[(1028501, False), (1028512, True)]</td>\n",
       "      <td>[(1028502, True)]</td>\n",
       "      <td>[(1028503, True)]</td>\n",
       "      <td>[(1028504, True)]</td>\n",
       "      <td>[(1028505, True)]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.685529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  LN_A_p1_non_top_8_sets  \\\n",
       "18988  [(1028338, True), (1028347, True), (1028351, F...   \n",
       "18995                [(1028523, False), (1028528, True)]   \n",
       "18992  [(1028465, True), (1028469, False), (1028478, ...   \n",
       "18999  [(1028610, True), (1028614, False), (1028622, ...   \n",
       "18994                [(1028500, True), (1028504, False)]   \n",
       "\n",
       "                                  LN_A_p2_non_top_8_sets  \\\n",
       "18988  [(1028335, True), (1028345, True), (1028350, F...   \n",
       "18995                                 [(1028522, False)]   \n",
       "18992  [(1028463, True), (1028468, False), (1028479, ...   \n",
       "18999  [(1028608, True), (1028613, False), (1028623, ...   \n",
       "18994  [(1028501, True), (1028505, False), (1028510, ...   \n",
       "\n",
       "                                  LN_B_p1_non_top_8_sets  \\\n",
       "18988  [(1028337, True), (1028346, False), (1028361, ...   \n",
       "18995                                 [(1028521, False)]   \n",
       "18992  [(1028459, True), (1028466, False), (1028477, ...   \n",
       "18999  [(1028606, True), (1028612, False), (1028624, ...   \n",
       "18994                [(1028500, False), (1028511, True)]   \n",
       "\n",
       "                                  LN_B_p2_non_top_8_sets  \\\n",
       "18988  [(1028330, True), (1028341, True), (1028348, F...   \n",
       "18995                [(1028519, True), (1028520, False)]   \n",
       "18992  [(1028461, True), (1028467, False), (1028476, ...   \n",
       "18999  [(1028605, True), (1028611, False), (1028625, ...   \n",
       "18994                [(1028501, False), (1028512, True)]   \n",
       "\n",
       "                  WSF_A_p1_non_top_8_sets             WSF_A_p2_non_top_8_sets  \\\n",
       "18988  [(1028342, True), (1028349, True)]  [(1028340, True), (1028348, True)]   \n",
       "18995                   [(1028520, True)]                   [(1028521, True)]   \n",
       "18992  [(1028460, True), (1028467, True)]  [(1028458, True), (1028466, True)]   \n",
       "18999  [(1028607, True), (1028612, True)]                   [(1028611, True)]   \n",
       "18994                   [(1028502, True)]                   [(1028503, True)]   \n",
       "\n",
       "                  WSF_B_p1_non_top_8_sets             WSF_B_p2_non_top_8_sets  \\\n",
       "18988  [(1028344, True), (1028350, True)]  [(1028346, True), (1028351, True)]   \n",
       "18995                   [(1028522, True)]                   [(1028523, True)]   \n",
       "18992  [(1028464, True), (1028469, True)]  [(1028462, True), (1028468, True)]   \n",
       "18999  [(1028609, True), (1028614, True)]                   [(1028613, True)]   \n",
       "18994                   [(1028504, True)]                   [(1028505, True)]   \n",
       "\n",
       "       LN_A_p1_non_top_8_sets_len  LN_A_p2_non_top_8_sets_len  ...  \\\n",
       "18988                           4                           4  ...   \n",
       "18995                           2                           1  ...   \n",
       "18992                           3                           3  ...   \n",
       "18999                           3                           3  ...   \n",
       "18994                           2                           3  ...   \n",
       "\n",
       "       WSF_B_p1_non_top_8_sets_len  WSF_B_p2_non_top_8_sets_len  \\\n",
       "18988                            2                            2   \n",
       "18995                            1                            1   \n",
       "18992                            2                            2   \n",
       "18999                            2                            1   \n",
       "18994                            1                            1   \n",
       "\n",
       "       LN_A_p1_non_top_8_sets_score  LN_A_p2_non_top_8_sets_score  \\\n",
       "18988                      1.371057                      1.371057   \n",
       "18995                      0.000000                     -0.685529   \n",
       "18992                      0.685529                      0.421395   \n",
       "18999                      0.685529                      0.685529   \n",
       "18994                      0.000000                      0.685529   \n",
       "\n",
       "       LN_B_p1_non_top_8_sets_score  LN_B_p2_non_top_8_sets_score  \\\n",
       "18988                      2.056586                      1.371057   \n",
       "18995                     -0.685529                      0.000000   \n",
       "18992                      0.685529                      0.949662   \n",
       "18999                      0.685529                      0.685529   \n",
       "18994                      0.000000                      0.000000   \n",
       "\n",
       "       WSF_A_p1_non_top_8_sets_score  WSF_A_p2_non_top_8_sets_score  \\\n",
       "18988                       1.371057                       1.371057   \n",
       "18995                       0.685529                       0.685529   \n",
       "18992                       1.106924                       1.371057   \n",
       "18999                       1.371057                       0.685529   \n",
       "18994                       0.685529                       0.685529   \n",
       "\n",
       "       WSF_B_p1_non_top_8_sets_score  WSF_B_p2_non_top_8_sets_score  \n",
       "18988                       1.371057                       1.371057  \n",
       "18995                       0.685529                       0.685529  \n",
       "18992                       1.371057                       1.371057  \n",
       "18999                       1.371057                       0.685529  \n",
       "18994                       0.685529                       0.685529  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_8_pos = ['LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2', 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2']\n",
    "# top_8_players = [x + '_non_top_8_sets' for x in top_8_pos]\n",
    "top_8_prevs = [x + '_non_top_8_sets' for x in top_8_pos]\n",
    "top_8_prevs_lengths = [x + \"_len\" for x in top_8_prevs] # These columns will just keep track of how many sets the player went through to get to the top 8\n",
    "top_8_prevs_scores = [x + \"_score\" for x in top_8_prevs] # These will keep track of their \"score\" that shows how \"well\" they are performing relative to their predicted odds.\n",
    "\n",
    "tournament_df[top_8_prevs_lengths] = tournament_df[top_8_prevs].map(lambda x: len(x)).to_numpy()\n",
    "tournament_df[top_8_prevs_scores] = tournament_df[top_8_prevs].map(prev_set_score).to_numpy()\n",
    "tournament_df[top_8_prevs + top_8_prevs_lengths + top_8_prevs_scores].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a column that contains the top 8 starting position of the winner\n",
    "# (or more specifically, the numeric index in top_8_pos)\n",
    "\n",
    "tournament_df['winner_index'] = 8 # Dummy value, has to be 0-7\n",
    "\n",
    "for i,position in enumerate(top_8_pos):\n",
    "    found_filter = (tournament_df['winner_id'] == tournament_df[top_8_pos[i]])\n",
    "    tournament_df.loc[found_filter, 'winner_index'] = i\n",
    "\n",
    "(tournament_df['winner_index'] == 8).sum() # Should be zero (everything found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start pulling in data (like ELO) for each of the top 8 players in the tournaments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to have all of the features we've engineered for each of the players that made it to the top 8\n",
    "# We can be clever and pull most of them (elo-based features) from the previous sets in the dataframe\n",
    "# The rest (player vs player stats, also called \"matchup\", but might be renamed) have to be pulled in manually\n",
    "\n",
    "features_elo = ['p1_default_elo', 'p2_default_elo', 'p1_default_rd', 'p2_default_rd',\n",
    "       'p1_default_updates', 'p2_default_updates', 'p1_m1_usage', 'p2_m1_usage',\n",
    "       'p1/m1/m1_alt2_elo', 'p1/m1/m1_alt2_rd', 'p1/m1/m1_alt2_updates',\n",
    "       'p2/m1/m1_alt2_elo', 'p2/m1/m1_alt2_rd', 'p2/m1/m1_alt2_updates',\n",
    "       'p1/m1_alt3_elo', 'p1/m1_alt3_rd', 'p1/m1_alt3_updates',\n",
    "       'p2/m1_alt3_elo', 'p2/m1_alt3_rd', 'p2/m1_alt3_updates']\n",
    "\n",
    "features_matchup = ['matchup_1', 'matchup_2', 'matchup_3', 'matchup_4', 'matchup_5',\n",
    "                    'matchup_6', 'matchup_7', 'matchup_8', 'matchup_9', 'matchup_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c9cb350b524e768b1a008823dc35a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In order to avoid an organizational nightmare, each of the following:\n",
    "#     'LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2', 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2'\n",
    "# will have their data stored in separate dataframes, each held in the following dictionary\n",
    "\n",
    "top_8_stats = {}\n",
    "\n",
    "def pull_data_from_set(loc, outcome):\n",
    "    set_data = dataset_df.loc[loc]\n",
    "    player_num = 'p1' if outcome == (set_data['winner'] == 1.0) else 'p2' # Sneaky way of getting the player number\n",
    "\n",
    "    features_to_pull = [x for x in features_elo if player_num in x]\n",
    "    pulled_data = set_data[features_to_pull].copy()\n",
    "    pulled_data.index = [x.replace(player_num, '') for x in pulled_data.index] # We will add player numbers on an as-needed basis later\n",
    "\n",
    "    return pulled_data\n",
    "\n",
    "for top_8_position in tqdm(top_8_pos):\n",
    "    # First, pull in player 1 data from a previous match.\n",
    "    # Note that the player might NOT be player 1 in the match that we are pulling from\n",
    "    top_8_stats[top_8_position] = tournament_df[top_8_position + '_non_top_8_sets'].apply(lambda x: pull_data_from_set(x[0][0], x[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important information about the players in the top 8 is essentially how likely each player is to win against the other player. We can use our single-set predictor to estimate this. We can also make use of the information on how well each player has been doing in the tournament relative to their skill level/opponent skill levels, possibly in order to \"adjust\" the aforementioned probabilities. Or just to toss it as extra information in whatever ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing more than half the data when we look up the matchup data makes it faster by roughly 10x\n",
    "smaller_df = dataset_df[dataset_df['matchup_1']!=.5].sort_values('end').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m     pairwise_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (pairwise_probs \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m pairwise_probs\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pairwise_probs\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m---> 49\u001b[0m pairwise_prob \u001b[38;5;241m=\u001b[39m \u001b[43mtournament_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_pairwise_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m pairwise_prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(pairwise_prob\u001b[38;5;241m.\u001b[39mto_numpy()) \u001b[38;5;66;03m# Fixes \"single column of np arrays\" nonsense\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Add that data as columns in the original dataframe\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m, in \u001b[0;36mcompute_pairwise_prob\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     23\u001b[0m         matchup_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;241m0.5\u001b[39m, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatchup_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m     25\u001b[0m         total_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([p1_data, p2_data, matchup_data])\n\u001b[0;32m---> 26\u001b[0m         total_data \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures_all_everything\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# Entries need to be in the correct order\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         combination_stats\u001b[38;5;241m.\u001b[39mappend(total_data)\n\u001b[1;32m     30\u001b[0m combination_stats \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(combination_stats)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py:1153\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py:1194\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[0;32m-> 1194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:6195\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6192\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(keyarr)\n\u001b[1;32m   6194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6195\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6196\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex(keyarr)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:6182\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6164\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6165\u001b[0m \u001b[38;5;124;03mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[1;32m   6166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6179\u001b[0m \u001b[38;5;124;03marray([0, 2])\u001b[39;00m\n\u001b[1;32m   6180\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6183\u001b[0m indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m   6184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:3938\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pself \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ptarget \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target:\n\u001b[1;32m   3934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pself\u001b[38;5;241m.\u001b[39mget_indexer(\n\u001b[1;32m   3935\u001b[0m         ptarget, method\u001b[38;5;241m=\u001b[39mmethod, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance\n\u001b[1;32m   3936\u001b[0m     )\n\u001b[0;32m-> 3938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m target\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3939\u001b[0m     \u001b[38;5;66;03m# Only call equals if we have same dtype to avoid inference/casting\u001b[39;00m\n\u001b[1;32m   3940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(target), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_partial_index(target):\n\u001b[1;32m   3943\u001b[0m     \u001b[38;5;66;03m# _should_partial_index e.g. IntervalIndex with numeric scalars\u001b[39;00m\n\u001b[1;32m   3944\u001b[0m     \u001b[38;5;66;03m#  that can be matched to Interval scalars.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:5649\u001b[0m, in \u001b[0;36mIndex.equals\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   5645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype):\n\u001b[1;32m   5646\u001b[0m     \u001b[38;5;66;03m# All EA-backed Index subclasses override equals\u001b[39;00m\n\u001b[1;32m   5647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m other\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 5649\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_equivalent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/missing.py:531\u001b[0m, in \u001b[0;36marray_equivalent\u001b[0;34m(left, right, strict_nan, dtype_equal)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# Slow path when we allow comparing different dtypes.\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# Object arrays can contain None, NaN and NaT.\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# string dtypes must be come to this path for NumPy 1.7.1 compat\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOSU\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m right\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOSU\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;66;03m# Note: `in \"OSU\"` is non-trivially faster than `in [\"O\", \"S\", \"U\"]`\u001b[39;00m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;66;03m#  or `in (\"O\", \"S\", \"U\")`\u001b[39;00m\n\u001b[0;32m--> 531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_equivalent_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# NaNs can occur in float and complex arrays.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/missing.py:576\u001b[0m, in \u001b[0;36m_array_equivalent_object\u001b[0;34m(left, right, strict_nan)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 576\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_equivalent_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39marray_equivalent_object(left[\u001b[38;5;241m~\u001b[39mmask], right[\u001b[38;5;241m~\u001b[39mmask]):\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_matchup(p1_id, p2_id, start, dataset_df=smaller_df):\n",
    "    \"\"\"Gets the most recent matchup data from before the start of the tournament.\n",
    "        This is pretty slow and will likely slow down this cell a lot.\n",
    "    Args:\n",
    "        p1_id: The ID of player 1.\n",
    "        p2_id: The ID of player 2.\n",
    "        start: The start of the tournament.\n",
    "        dataset_df: Use only the data that have values other than .5 in matchup_1\n",
    "\n",
    "    Returns:\n",
    "        A pd.Series containing the matchup data or default values if no prior sets exist.\n",
    "    \"\"\"\n",
    "    # Ensure the dataset is sorted by 'end'\n",
    "    assert dataset_df['end'].is_monotonic_increasing, \"Dataset must be sorted by 'end'.\"\n",
    "\n",
    "    # Use NumPy for faster filtering\n",
    "    p1_mask = (dataset_df['p1_id'] == p1_id) & (dataset_df['p2_id'] == p2_id)\n",
    "    p2_mask = (dataset_df['p1_id'] == p2_id) & (dataset_df['p2_id'] == p1_id)\n",
    "    mask = (p1_mask | p2_mask).to_numpy()\n",
    "\n",
    "    # Find indices where mask is True\n",
    "    valid_indices = np.where(mask & (dataset_df['end'].to_numpy() < start))[0]\n",
    "\n",
    "    if len(valid_indices) == 0:\n",
    "        # No prior matches, return default values\n",
    "        return pd.Series(0.5, index=[f'matchup_{n}' for n in range(1, 11)])\n",
    "\n",
    "    # Get the last valid index\n",
    "    last_index = valid_indices[-1]\n",
    "    last_row = dataset_df.iloc[last_index]\n",
    "\n",
    "    # Define matchup columns\n",
    "    matchup_cols = [f'matchup_{n}' for n in range(1, 11)]\n",
    "\n",
    "    # Determine if we need to swap values\n",
    "    if p1_id == last_row['p1_id'] and p2_id == last_row['p2_id']:\n",
    "        return last_row[matchup_cols]\n",
    "    elif p1_id == last_row['p2_id'] and p2_id == last_row['p1_id']:\n",
    "        return 1 - last_row[matchup_cols]\n",
    "    else:\n",
    "        # Should not happen but acts as a fallback\n",
    "        print(\"Matchup Data Failed\")\n",
    "        return pd.Series(0.5, index=[f'matchup_{n}' for n in range(1, 11)])\n",
    "    \n",
    "\n",
    "\n",
    "# First, let's compute pairwise probabilities of one player in the top 8 winning against another player\n",
    "def compute_pairwise_prob(row):\n",
    "    # Row represents p1, column represents p2 (or specifically, the index in top_8_pos). Always follows this order:\n",
    "    # 'LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2', 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2'\n",
    "    pairwise_probs = np.zeros(shape=(8,8))\n",
    "    \n",
    "    # For convenience, put everything into one dataframe and then run the single set model\n",
    "    # This is probably more efficient than doing things line by line\n",
    "    combination_stats = []\n",
    "\n",
    "    for r in range(0,8):\n",
    "        for c in range(0,8):\n",
    "            p1_data = top_8_stats[top_8_pos[r]].loc[row.name]\n",
    "            p1_data.index = ['p1' + x for x in p1_data.index]\n",
    "\n",
    "            p2_data = top_8_stats[top_8_pos[c]].loc[row.name]\n",
    "            p2_data.index = ['p2' + x for x in p2_data.index]\n",
    "\n",
    "            #TODO: Actually populate this with proper data!\n",
    "            #      This is currently only just placeholder data,\n",
    "            #      indicating that the players have never played together before (0.5).\n",
    "            matchup_data = get_matchup(p1_id, p2_id, row['start'], dataset_df=dataset_df)\n",
    "\n",
    "            total_data = pd.concat([p1_data, p2_data, matchup_data])\n",
    "            total_data = total_data[features_all_everything] # Entries need to be in the correct order\n",
    "\n",
    "            combination_stats.append(total_data)\n",
    "\n",
    "    combination_stats = pd.DataFrame(combination_stats)\n",
    "\n",
    "    y_prob = single_set_model.predict_proba(combination_stats)\n",
    "\n",
    "    # Now actually populate this probability matrix with data.\n",
    "    # Note that we can just use the same nested loop and read off the entries of the 1D probability array one at a time.\n",
    "    # This will put things in the correct order.\n",
    "    i = 0\n",
    "    for r in range(0,8):\n",
    "        for c in range(0,8):\n",
    "            pairwise_probs[r,c] = y_prob[i,1]\n",
    "            i += 1\n",
    "\n",
    "    # NOTE: Some models that we train are highly non-symmetric, even though they very much should be,\n",
    "    #       given that we have randomized the players. We can fix that issue here.\n",
    "    pairwise_probs = 0.5 * (pairwise_probs + (1 - pairwise_probs.T))\n",
    "\n",
    "    return pairwise_probs.flatten()\n",
    "\n",
    "pairwise_prob = tournament_df.apply(compute_pairwise_prob, axis=1)\n",
    "pairwise_prob = np.stack(pairwise_prob.to_numpy()) # Fixes \"single column of np arrays\" nonsense\n",
    "\n",
    "# Add that data as columns in the original dataframe\n",
    "pairwise_prob_cols = []\n",
    "for r in range(0,8):\n",
    "    for c in range(0,8):\n",
    "        pairwise_prob_cols.append(\"pairprob/\" + top_8_pos[r] + \"/\" + top_8_pos[c])\n",
    "\n",
    "tournament_df[pairwise_prob_cols] = pairwise_prob\n",
    "\n",
    "tournament_df[pairwise_prob_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add that data as columns in the original dataframe\n",
    "\n",
    "# Note that we only really need the strictly lower triangular part of the probability matrix\n",
    "# Everything else is redundant (the diagonal is 0.5, upper triangular part is 1 - lower triangular)\n",
    "# The entire matrix is kept in there just for easy reading.\n",
    "\n",
    "pairwise_prob_cols = []\n",
    "pairwise_prob_cols_reduced = []\n",
    "\n",
    "for r in range(0,8):\n",
    "    for c in range(0,8):\n",
    "        col_name = \"pairprob/\" + top_8_pos[r] + \"/\" + top_8_pos[c]\n",
    "\n",
    "        pairwise_prob_cols.append(col_name)\n",
    "\n",
    "        if r > c:\n",
    "            pairwise_prob_cols_reduced.append(col_name)\n",
    "\n",
    "tournament_df[pairwise_prob_cols] = pairwise_prob\n",
    "tournament_df[pairwise_prob_cols_reduced]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can (disclaimer: slightly inaccurately) compute all possible paths throughout the top 8, and the probability of winning along each path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slighltly incorrectly implemented algorithm that goes through all possible paths throughout the top 8,\n",
    "# and computes the corresponding probabilities of each player making it through.\n",
    "def compute_path_prob(row):\n",
    "    pairwise_probs = row[pairwise_prob_cols].to_numpy().astype(float).reshape((8,8))\n",
    "    pairwise_probs_zero_diagonal = pairwise_probs - 0.5 * np.identity(8) # Used for janky computations\n",
    "\n",
    "    # Now start building the tree structure of how the tournament can play out.\n",
    "    # Each \"cell\" will represent some set in the tournament played by some p1 and p2.\n",
    "    # The cell will have to keep track of all of the probabilities of each player making it to that point.\n",
    "    #\n",
    "    # Links should have the form (cell, 'winner') or (cell, 'loser'),\n",
    "    # describing if it is the winner or the loser of the previous that gets to this one\n",
    "    class cell:\n",
    "        def __init__(self, p1=None, p2=None, p1_link=None, p2_link=None):\n",
    "            if p1==None:\n",
    "                self.p1_probs = None\n",
    "            else:\n",
    "                self.p1_probs = np.zeros(8)\n",
    "                self.p1_probs[p1] = 1.0\n",
    "\n",
    "            if p2==None:\n",
    "                self.p2_probs = None\n",
    "            else:\n",
    "                self.p2_probs = np.zeros(8)\n",
    "                self.p2_probs[p2] = 1.0\n",
    "\n",
    "            # Links to previous cells\n",
    "            self.p1_link = p1_link\n",
    "            self.p2_link = p2_link\n",
    "\n",
    "            # Used for a (hopefully) temporary patch on the fact that these computations are not entirely accurate\n",
    "            self.pairwise_probs_zero_diagonal = pairwise_probs - 0.5 * np.identity(8)\n",
    "\n",
    "        # Get the probabilities from the previous cell.\n",
    "        # Should not be called if there are no links to previous cells.\n",
    "        def fetch_probs(self):\n",
    "            self.p1_probs = self.p1_link[0].compute_winner_probs() if self.p1_link[1] == 'winner' else self.p1_link[0].compute_loser_probs()\n",
    "            self.p2_probs = self.p2_link[0].compute_winner_probs() if self.p2_link[1] == 'winner' else self.p2_link[0].compute_loser_probs()\n",
    "        \n",
    "        # Probability of making it to this cell, and then proceeding to win\n",
    "        def compute_winner_probs(self):\n",
    "            if self.p1_probs is None or self.p2_probs is None:\n",
    "                self.fetch_probs()\n",
    "\n",
    "            probs = np.zeros(8)\n",
    "\n",
    "            # Old code, far less efficient. Might make the numpy operations make sense though.\n",
    "            '''\n",
    "            for p1 in range(0,8):\n",
    "                # Save a result for p1.\n",
    "                # It will be the sum over all p2 of\n",
    "                # (probability that p1 got there) * (probability that p2 got there) * (probability p1 beats p2)\n",
    "                for p2 in range(0,8):\n",
    "                    probs[p1] += self.p1_probs[p1] * self.p2_probs[p2] * pairwise_probs[p1, p2]\n",
    "                    probs[p2] += self.p1_probs[p1] * self.p2_probs[p2] * (1.0 - pairwise_probs[p1, p2])\n",
    "            '''\n",
    "            # Just remember that 1-pairwise_probs is the transpose of pairwise_probs, by symmetry\n",
    "            #\n",
    "            # TODO: I just realized that the probability of a certain player becoming p1 and another becoming p2 are NOT independent.\n",
    "            #       In particular, these probabilities become correlated when you could potentially have the same player as p1 or p2.\n",
    "            #       This is a bit of a janky patch that hopefully gives accurate enough probabilities, but we should come up with a proper fix.       \n",
    "            probs += self.p1_probs * (pairwise_probs_zero_diagonal @ self.p2_probs) # Probability that (specific p1) wins\n",
    "            probs += self.p2_probs * (pairwise_probs_zero_diagonal @ self.p1_probs) # Same but p2\n",
    "\n",
    "            probs /= probs.sum() # Purely due to zeroing out the diagonal of pairwise_probs\n",
    "\n",
    "            return probs\n",
    "\n",
    "        # Probability of making it to this cell, and then proceeding to lose\n",
    "        def compute_loser_probs(self):\n",
    "            if self.p1_probs is None or self.p2_probs is None:\n",
    "                self.fetch_probs()\n",
    "\n",
    "            probs = np.zeros(8)\n",
    "\n",
    "            '''\n",
    "            for p1 in range(0,8):\n",
    "                # Same, except use probability of p1 losing\n",
    "                for p2 in range(0,8):\n",
    "                    probs[p1] += self.p1_probs[p1] * self.p2_probs[p2] * (1.0 - pairwise_probs[p1, p2])\n",
    "                    probs[p2] += self.p1_probs[p1] * self.p2_probs[p2] * pairwise_probs[p1, p2]\n",
    "            '''\n",
    "            # TODO: Same janky patch as in winners case here.\n",
    "            probs += self.p1_probs * (pairwise_probs_zero_diagonal.T @ self.p2_probs)\n",
    "            probs += self.p2_probs * (pairwise_probs_zero_diagonal.T @ self.p1_probs)\n",
    "\n",
    "            probs /= probs.sum()\n",
    "             \n",
    "            return probs\n",
    "        \n",
    "    # 'LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2', 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2'\n",
    "    WSFA = cell(p1=4, p2=5)\n",
    "    WSFB = cell(p1=6, p2=7)\n",
    "    LNA  = cell(p1=0, p2=1)\n",
    "    LNB  = cell(p1=2, p2=3)\n",
    "\n",
    "    WF = cell(p1_link=(WSFA, 'winner'), p2_link=(WSFB, 'winner'))\n",
    "\n",
    "    LQFA = cell(p1_link=(WSFA, 'loser'), p2_link=(LNA, 'winner'))\n",
    "    LQFB = cell(p1_link=(WSFB, 'loser'), p2_link=(LNB, 'winner'))\n",
    "\n",
    "    LSF = cell(p1_link=(LQFA, 'winner'), p2_link=(LQFB, 'winner'))\n",
    "\n",
    "    LF = cell(p1_link=(WF, 'loser'), p2_link=(LSF, 'winner'))\n",
    "\n",
    "    GF = cell(p1_link=(WF, 'winner'), p2_link=(LF, 'winner'))\n",
    "\n",
    "    # From the Grand Final onwards, some special cases are required, due to how the Grand Final Reset works\n",
    "    GF.fetch_probs()\n",
    "\n",
    "    # TODO: Again, same janky fix as before, \"removing\" correlation between p1 and p2\n",
    "    win_as_p1_probs = GF.p1_probs * (pairwise_probs_zero_diagonal @ GF.p2_probs) # direct win as p1 (WF winner)\n",
    "    win_as_p1_probs += GF.p1_probs * ((pairwise_probs_zero_diagonal.T * pairwise_probs_zero_diagonal) @ GF.p2_probs) # p2 win, then p1 win in GFR\n",
    "\n",
    "    win_as_p2_probs = GF.p2_probs * ((pairwise_probs_zero_diagonal ** 2) @ GF.p1_probs) # win by 2 required for LF winner\n",
    "\n",
    "    probs = win_as_p1_probs + win_as_p2_probs\n",
    "    probs /= probs.sum() # Again due to that janky fix\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and baselines\n",
    "\n",
    "Here, we start comparing the performance of a few \"obvious\" models, such as just choosing the person with the highest ELO out of the top 8 (or winners' side of the top 8), or simulating all possible paths throughout the top 8 and computing the probability of winning as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features += pairwise_prob_cols_reduced\n",
    "features += top_8_prevs_lengths + top_8_prevs_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline of \"who has the higher elo\"\n",
    "\n",
    "def pull_elo_from_set(loc, outcome):\n",
    "    set_data = dataset_df.loc[loc]\n",
    "    player_num = 'p1' if outcome == (set_data['winner'] == 1.0) else 'p2' # Sneaky way of getting the player number\n",
    "\n",
    "    feature_to_pull = player_num + '_default_elo'\n",
    "    pulled_data = set_data[feature_to_pull]\n",
    "\n",
    "    return pulled_data\n",
    "\n",
    "# First, pull in player 1 data from a previous match.\n",
    "# Note that the player might NOT be player 1 in the match that we are pulling from\n",
    "tournament_df[[x + '_elo' for x in top_8_pos]] = tournament_df[[x + '_non_top_8_sets' for x in top_8_pos]].map(lambda x: pull_elo_from_set(x[0][0], x[0][1])).to_numpy()\n",
    "\n",
    "tournament_df['elo_prediction'] = tournament_df[[x + '_elo' for x in top_8_pos]].idxmax(axis=1).apply(lambda x: x.replace('_elo', ''))\n",
    "tournament_df['elo_prediction'] = tournament_df['elo_prediction'].apply(lambda x: top_8_pos.index(x))\n",
    "\n",
    "tournament_df['elo_WSF_prediction'] = tournament_df[[x + '_elo' for x in top_8_pos if \"WSF\" in x]].idxmax(axis=1).apply(lambda x: x.replace('_elo', ''))\n",
    "tournament_df['elo_WSF_prediction'] = tournament_df['elo_WSF_prediction'].apply(lambda x: top_8_pos.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor of \"who has the highest path probability, taking into account all possible paths on how the top 8 will play out\"\n",
    "\n",
    "result = tournament_df.apply(compute_path_prob, axis=1)\n",
    "tournament_df = pd.concat([tournament_df, pd.DataFrame(np.stack(result.to_numpy()), index=result.index, columns=[x + '_winprob' for x in top_8_pos])], axis=1)\n",
    "\n",
    "tournament_df['path_prediction'] = tournament_df[[x + '_winprob' for x in top_8_pos]].idxmax(axis=1).apply(lambda x: x.replace('_winprob', ''))\n",
    "tournament_df['path_prediction'] = tournament_df['path_prediction'].apply(lambda x: top_8_pos.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tournament_df[(tournament_df['start'] >= datetime.datetime(2023,1,1)) & (tournament_df['end'] <= datetime.datetime(2023,12,31))].copy()\n",
    "test_df  = tournament_df[(tournament_df['start'] >= datetime.datetime(2024,1,1)) & (tournament_df['end'] <= datetime.datetime(2024,12,31))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set baselines:\")\n",
    "print(\"Highest ELO out of top 8, accuracy:               \", round(100.0 * (train_df['winner_index'] == train_df['elo_prediction']).astype(float).mean(), 1))\n",
    "print(\"Highest ELO out of WSF, accuracy:                 \", round(100.0 * (train_df['winner_index'] == train_df['elo_WSF_prediction']).astype(float).mean(), 1))\n",
    "print(\"Computing all ways top 8 can play out, accuracy:  \", round(100.0 * (train_df['winner_index'] == train_df['path_prediction']).astype(float).mean(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more advanced model and hyperparameter tuning\n",
    "\n",
    "Just because choosing the highest ELO seems to be such a powerful predictor to begin with, we will make sure to add those to the list of features that we will use. We won't add any of the other old engineered features, because the model already has a bunch of new \"fancier\" engineered features, and too many might make it perform worse.\n",
    "\n",
    "**NOTE:** Hyperparameter tuning trials was reduced down to a very low number, so that this entire notebook could be run in a reasonable amount of time. Optimal parameters from a proper number of trials have already been found and are already provided after the hyperparameter tuning block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features += [x + '_elo' for x in top_8_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some features should NOT be scaled, like probabilities.\n",
    "# Let's just manually scale ELO (note that we will be using regularization)\n",
    "if tournament_df[[x + '_elo' for x in top_8_pos][0]].mean() >= 400.0: # Prevents this from accidentally being run twice.\n",
    "    print(\"Scaling ELO!\")\n",
    "    tournament_df[[x + '_elo' for x in top_8_pos]] /= 1500.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning on XGBoost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    max_depth        = trial.suggest_int(\"max_depth\", 2, 10, step=1)\n",
    "    learning_rate    = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "    n_estimators     = trial.suggest_int(\"n_estimators\", 50, 1000, step=50)\n",
    "    subsample        = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
    "    gamma            = trial.suggest_float(\"gamma\", 0.0, 5.0)\n",
    "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "    reg_lambda       = trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True)\n",
    "    reg_alpha        = trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n",
    "\n",
    "    model = xgb.XGBClassifier(max_depth=max_depth,\n",
    "                              learning_rate=learning_rate,\n",
    "                              n_estimators=n_estimators,\n",
    "                              subsample=subsample,\n",
    "                              colsample_bytree=colsample_bytree,\n",
    "                              gamma=gamma,\n",
    "                              min_child_weight=min_child_weight,\n",
    "                              reg_lambda=reg_lambda,\n",
    "                              reg_alpha=reg_alpha)\n",
    "    \n",
    "    # The percentage of people that win the tournament when starting from losers side is very small, but not zero\n",
    "    # Hence, it is probably good to use a stratified k-fold split for cross-validation\n",
    "\n",
    "    n_splits = 3\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "    results = np.zeros(n_splits)\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(skf.split(train_df[features], train_df['winner_index'])):\n",
    "        model.fit(train_df.iloc[train_index][features], train_df.iloc[train_index]['winner_index'])\n",
    "\n",
    "        y_pred = model.predict(train_df.iloc[val_index][features])\n",
    "        results[i] = accuracy_score(train_df.iloc[val_index]['winner_index'], y_pred)\n",
    "    \n",
    "    return results.mean()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10, timeout=1800)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plucked from the above hyperparameter tuning session.\n",
    "# Doesn't really seem to be getting much better.\n",
    "xgb_tuned = xgb.XGBClassifier(max_depth=5,\n",
    "                              learning_rate=0.013278022811244645,\n",
    "                              n_estimators=300,\n",
    "                              subsample=0.7451791331241274,\n",
    "                              colsample_bytree=0.7785615347277948,\n",
    "                              gamma=2.819394518381603,\n",
    "                              min_child_weight=6,\n",
    "                              reg_lambda=0.41270859938111326,\n",
    "                              reg_alpha=1.9859484482942584)\n",
    "\n",
    "xgb_tuned.fit(train_df[features], train_df['winner_index'])\n",
    "print(\"Accuracy of XGBoost on test set: \", round(100.0 * accuracy_score(test_df['winner_index'], xgb_tuned.predict(test_df[features])), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set baselines:\")\n",
    "print()\n",
    "print(\"Highest ELO out of top 8, accuracy:               \", round(100.0 * (test_df['winner_index'] == test_df['elo_prediction']).astype(float).mean(), 1))\n",
    "print(\"Highest ELO out of WSF, accuracy:                 \", round(100.0 * (test_df['winner_index'] == test_df['elo_WSF_prediction']).astype(float).mean(), 1))\n",
    "print(\"Computing all ways top 8 can play out, accuracy:  \", round(100.0 * (test_df['winner_index'] == test_df['path_prediction']).astype(float).mean(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
