{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Match Winners with XGBoost Classifier\n",
    "\n",
    "In this notebook, we aim to predict the winner of a match using an XGBoost classifier. We want to give higher importance to correctly predicting upsets, defined as instances where a player with a lower default ELO wins the match.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Preparation](#data-preparation)\n",
    "2. [Baseline Model](#baseline-model)\n",
    "3. [Weighted Model](#weighted-model)\n",
    "4. [Hyperparameter Optimization with Optuna](#hyperparameter-optimization)\n",
    "5. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import datetime\n",
    "import os\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import optuna\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if os.path.exists('/workspace/data_2'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data_2/'\n",
    "else:\n",
    "    data_path = '../data/'\n",
    "    \n",
    "# if torch.cuda.is_available() == False:\n",
    "#     RuntimeError(\"GPU detected: False\")\n",
    "#     print(\"GPU detected: False\")\n",
    "# else:\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"The GPU is detected.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_pickle(data_path + 'full_dataset_df.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify columns for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 key_x\n",
      "1 game\n",
      "2 tournament_key\n",
      "3 winner_id\n",
      "4 loser_id\n",
      "5 p1_id\n",
      "6 p2_id\n",
      "7 p1_score\n",
      "8 p2_score\n",
      "9 valid_score\n",
      "10 best_of\n",
      "11 location_names\n",
      "12 bracket_name\n",
      "13 bracket_order\n",
      "14 set_order\n",
      "15 game_data\n",
      "16 top_8\n",
      "17 top_8_location_names\n",
      "18 valid_top_8_bracket\n",
      "19 top_8_bracket_location_names\n",
      "20 major\n",
      "21 key_y\n",
      "22 start\n",
      "23 end\n",
      "24 start_week\n",
      "25 p1_characters\n",
      "26 p2_characters\n",
      "27 p1_consistent\n",
      "28 p2_consistent\n",
      "29 matchup_strings\n",
      "30 end_week\n",
      "31 players_have_history\n",
      "32 (p1/p2)_sorted\n",
      "33 (p1/p2)_was_sorted\n",
      "34 results_sorted\n",
      "35 results\n",
      "36 matchup_1\n",
      "37 matchup_2\n",
      "38 matchup_3\n",
      "39 matchup_4\n",
      "40 matchup_5\n",
      "41 matchup_6\n",
      "42 matchup_7\n",
      "43 matchup_8\n",
      "44 matchup_9\n",
      "45 matchup_10\n",
      "46 winner\n",
      "47 p1_default_elo\n",
      "48 p2_default_elo\n",
      "49 p1_default_rd\n",
      "50 p2_default_rd\n",
      "51 p1_default_updates\n",
      "52 p2_default_updates\n",
      "53 start_index\n",
      "54 start_date\n",
      "55 p1_fox_count\n",
      "56 p1_falco_count\n",
      "57 p1_marth_count\n",
      "58 p1_sheik_count\n",
      "59 p1_captainfalcon_count\n",
      "60 p1_jigglypuff_count\n",
      "61 p1_peach_count\n",
      "62 p1_luigi_count\n",
      "63 p1_samus_count\n",
      "64 p1_ganondorf_count\n",
      "65 p1_iceclimbers_count\n",
      "66 p1_drmario_count\n",
      "67 p1_yoshi_count\n",
      "68 p1_pikachu_count\n",
      "69 p1_link_count\n",
      "70 p1_mrgameandwatch_count\n",
      "71 p1_donkeykong_count\n",
      "72 p1_mario_count\n",
      "73 p1_zelda_count\n",
      "74 p1_roy_count\n",
      "75 p1_younglink_count\n",
      "76 p1_kirby_count\n",
      "77 p1_ness_count\n",
      "78 p1_bowser_count\n",
      "79 p1_pichu_count\n",
      "80 p1_random_count\n",
      "81 p1_mewtwo_count\n",
      "82 p2_fox_count\n",
      "83 p2_falco_count\n",
      "84 p2_marth_count\n",
      "85 p2_sheik_count\n",
      "86 p2_captainfalcon_count\n",
      "87 p2_jigglypuff_count\n",
      "88 p2_peach_count\n",
      "89 p2_luigi_count\n",
      "90 p2_samus_count\n",
      "91 p2_ganondorf_count\n",
      "92 p2_iceclimbers_count\n",
      "93 p2_drmario_count\n",
      "94 p2_yoshi_count\n",
      "95 p2_pikachu_count\n",
      "96 p2_link_count\n",
      "97 p2_mrgameandwatch_count\n",
      "98 p2_donkeykong_count\n",
      "99 p2_mario_count\n",
      "100 p2_zelda_count\n",
      "101 p2_roy_count\n",
      "102 p2_younglink_count\n",
      "103 p2_kirby_count\n",
      "104 p2_ness_count\n",
      "105 p2_bowser_count\n",
      "106 p2_pichu_count\n",
      "107 p2_random_count\n",
      "108 p2_mewtwo_count\n",
      "109 p1_m1_usage\n",
      "110 p2_m1_usage\n",
      "111 p1/m1/m1_alt2_elo\n",
      "112 p1/m1/m1_alt2_rd\n",
      "113 p1/m1/m1_alt2_updates\n",
      "114 p2/m1/m1_alt2_elo\n",
      "115 p2/m1/m1_alt2_rd\n",
      "116 p2/m1/m1_alt2_updates\n",
      "117 p1/m1_alt3_elo\n",
      "118 p1/m1_alt3_rd\n",
      "119 p1/m1_alt3_updates\n",
      "120 p2/m1_alt3_elo\n",
      "121 p2/m1_alt3_rd\n",
      "122 p2/m1_alt3_updates\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(dataset_df.columns):\n",
    "    print(i, col)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the features of player one and player two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matchup_1', 'matchup_2', 'matchup_3', 'matchup_4', 'matchup_5', 'matchup_6', 'matchup_7', 'matchup_8', 'matchup_9', 'matchup_10', 'p1_default_elo', 'p2_default_elo', 'p1_default_rd', 'p2_default_rd', 'p1_default_updates', 'p2_default_updates', 'p1_fox_count', 'p1_falco_count', 'p1_marth_count', 'p1_sheik_count', 'p1_captainfalcon_count', 'p1_jigglypuff_count', 'p1_peach_count', 'p1_luigi_count', 'p1_samus_count', 'p1_ganondorf_count', 'p1_iceclimbers_count', 'p1_drmario_count', 'p1_yoshi_count', 'p1_pikachu_count', 'p1_link_count', 'p1_mrgameandwatch_count', 'p1_donkeykong_count', 'p1_mario_count', 'p1_zelda_count', 'p1_roy_count', 'p1_younglink_count', 'p1_kirby_count', 'p1_ness_count', 'p1_bowser_count', 'p1_pichu_count', 'p1_random_count', 'p1_mewtwo_count', 'p2_fox_count', 'p2_falco_count', 'p2_marth_count', 'p2_sheik_count', 'p2_captainfalcon_count', 'p2_jigglypuff_count', 'p2_peach_count', 'p2_luigi_count', 'p2_samus_count', 'p2_ganondorf_count', 'p2_iceclimbers_count', 'p2_drmario_count', 'p2_yoshi_count', 'p2_pikachu_count', 'p2_link_count', 'p2_mrgameandwatch_count', 'p2_donkeykong_count', 'p2_mario_count', 'p2_zelda_count', 'p2_roy_count', 'p2_younglink_count', 'p2_kirby_count', 'p2_ness_count', 'p2_bowser_count', 'p2_pichu_count', 'p2_random_count', 'p2_mewtwo_count', 'p1_m1_usage', 'p2_m1_usage', 'p1/m1/m1_alt2_elo', 'p1/m1/m1_alt2_rd', 'p1/m1/m1_alt2_updates', 'p2/m1/m1_alt2_elo', 'p2/m1/m1_alt2_rd', 'p2/m1/m1_alt2_updates', 'p1/m1_alt3_elo', 'p1/m1_alt3_rd', 'p1/m1_alt3_updates', 'p2/m1_alt3_elo', 'p2/m1_alt3_rd', 'p2/m1_alt3_updates']\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "features = (\n",
    "    list(dataset_df.columns[36:46]) +\n",
    "    list(dataset_df.columns[47:53]) +\n",
    "    list(dataset_df.columns[55:])\n",
    ").copy()\n",
    "target = 'winner'\n",
    "\n",
    "print(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the 'expected_winner' column\n",
    "dataset_df['expected_winner'] = np.where(\n",
    "    dataset_df['p1_default_elo'] > dataset_df['p2_default_elo'], 1,\n",
    "    np.where(dataset_df['p1_default_elo'] < dataset_df['p2_default_elo'], 0, np.nan)\n",
    ")\n",
    "\n",
    "# 2. Define 'upset' only when 'expected_winner' is not NaN\n",
    "dataset_df['upset'] = np.where(\n",
    "    dataset_df['expected_winner'].notna() & (dataset_df['winner'] != dataset_df['expected_winner']), 1, 0\n",
    ")\n",
    "\n",
    "# 3. Remove matches where ELOs are equal\n",
    "dataset_df = dataset_df[dataset_df['expected_winner'].notna()].reset_index(drop=True)\n",
    "\n",
    "# 4. Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(dataset_df, test_size=0.2, random_state=42, stratify=dataset_df['upset'])\n",
    "\n",
    "# 5. Reset index for test_data\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# 6. Separate features and target\n",
    "X_train_full = train_data[features].reset_index(drop=True)\n",
    "y_train_full = train_data[target].reset_index(drop=True)\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n",
    "\n",
    "# Define the upset mask for the test set\n",
    "upset_mask_test = test_data['upset'] == 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Base Models\n",
    "\n",
    "We train multiple XGBoost models with upset weights ranging from **1.0** to **3.5** (incrementing by **0.5**). For each upset weight:\n",
    "\n",
    "- **Cross-Validation**: We perform K-fold cross-validation to generate out-of-fold predictions for the meta-model training.\n",
    "- **Out-of-Fold Predictions**: Predictions on validation folds are stored for training the meta-model.\n",
    "- **Test Predictions**: After training on the full training data, predictions on the test set are stored for final evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with upset weight: 1.0\n",
      "Training models with upset weight: 1.5\n",
      "Training models with upset weight: 2.0\n",
      "Training models with upset weight: 2.5\n",
      "Training models with upset weight: 3.0\n",
      "Training models with upset weight: 3.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Upset weights to consider\n",
    "upset_weights = np.arange(1.0, 3.6, .5)  # Adjust increments as needed\n",
    "best_params = {'n_estimators': 332,\n",
    "  'max_depth': 13,\n",
    "  'learning_rate': 0.0329014414333458,\n",
    "  'min_child_weight': 3,\n",
    "  'gamma': 0.024318270664498532,\n",
    "  'subsample': 0.8478652099231178,\n",
    "  'colsample_bytree': 0.6737054254112979,\n",
    "  'reg_alpha': 3.090492668111583e-05,\n",
    "  'reg_lambda': 6.748516964647809e-06,\n",
    "  'tree_method': 'hist'}\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Prepare arrays to hold out-of-fold predictions and test predictions\n",
    "oof_predictions = pd.DataFrame(np.zeros((len(X_train_full), len(upset_weights))), columns=[f'weight_{w}' for w in upset_weights])\n",
    "test_predictions = pd.DataFrame(np.zeros((len(X_test), len(upset_weights))), columns=[f'weight_{w}' for w in upset_weights])\n",
    "\n",
    "# KFold cross-validation\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for idx, weight in enumerate(upset_weights):\n",
    "    oof_pred = np.zeros(len(X_train_full))\n",
    "    test_pred = np.zeros(len(X_test))\n",
    "    \n",
    "    print(f\"Training models with upset weight: {weight}\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full, y_train_full)):\n",
    "        X_train = X_train_full.loc[train_idx]\n",
    "        y_train = y_train_full.loc[train_idx]\n",
    "        X_val = X_train_full.loc[val_idx]\n",
    "        y_val = y_train_full.loc[val_idx]\n",
    "        \n",
    "        # Sample weights for training data\n",
    "        sample_weight = np.ones(len(y_train))\n",
    "        sample_weight[train_data.iloc[train_idx]['upset'] == 1] = weight\n",
    "        \n",
    "        # Train the model\n",
    "        model = xgb.XGBClassifier(**best_params,  eval_metric='logloss')\n",
    "        # model = xgb.XGBClassifier(tree_method='hist', eval_metric='logloss')\n",
    "        model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        \n",
    "        # Predict on validation fold\n",
    "        oof_pred[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "    # Store out-of-fold predictions\n",
    "    oof_predictions.iloc[:, idx] = oof_pred\n",
    "    \n",
    "    # Retrain model on full training data\n",
    "    sample_weight_full = np.ones(len(y_train_full))\n",
    "    sample_weight_full[train_data['upset'] == 1] = weight\n",
    "    model = xgb.XGBClassifier(**best_params, eval_metric='logloss')\n",
    "    # model = xgb.XGBClassifier(tree_method='hist', eval_metric='logloss')\n",
    "    model.fit(X_train_full, y_train_full, sample_weight=sample_weight_full)\n",
    "    \n",
    "    # Predict on test data\n",
    "    test_predictions.iloc[:, idx] = model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Overall Accuracy: 0.7784\n",
      "Ensemble Model Upset Accuracy: 0.3614\n",
      "Ensemble Model Non-Upset Accuracy: 0.9208\n"
     ]
    }
   ],
   "source": [
    "# Meta-model training data\n",
    "X_meta_train = oof_predictions\n",
    "y_meta_train = y_train_full\n",
    "\n",
    "# Meta-model test data\n",
    "X_meta_test = test_predictions\n",
    "\n",
    "# Train logistic regression as meta-model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "meta_model = LogisticRegression(max_iter=10_000)\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_meta = meta_model.predict(X_meta_test)\n",
    "y_pred_proba_meta = meta_model.predict_proba(X_meta_test)[:, 1]\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_meta = accuracy_score(y_test, y_pred_meta)\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_meta = accuracy_score(\n",
    "    y_test[upset_mask_test], y_pred_meta[upset_mask_test]\n",
    ")\n",
    "accuracy_non_upsets_meta = accuracy_score(\n",
    "    y_test[~upset_mask_test], y_pred_meta[~upset_mask_test]\n",
    ")\n",
    "\n",
    "print(f\"Ensemble Model Overall Accuracy: {overall_accuracy_meta:.4f}\")\n",
    "print(f\"Ensemble Model Upset Accuracy: {accuracy_upsets_meta:.4f}\")\n",
    "print(f\"Ensemble Model Non-Upset Accuracy: {accuracy_non_upsets_meta:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Overall Accuracy: 0.7788\n",
      "Ensemble Model Upset Accuracy: 0.3179\n",
      "Ensemble Model Non-Upset Accuracy: 0.9362\n"
     ]
    }
   ],
   "source": [
    "meta_model = xgb.XGBClassifier(tree_method='hist', eval_metric='error')\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_meta = meta_model.predict(X_meta_test)\n",
    "y_pred_proba_meta = meta_model.predict_proba(X_meta_test)[:, 1]\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_meta = accuracy_score(y_test, y_pred_meta)\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_meta = accuracy_score(\n",
    "    y_test[upset_mask_test], y_pred_meta[upset_mask_test]\n",
    ")\n",
    "accuracy_non_upsets_meta = accuracy_score(\n",
    "    y_test[~upset_mask_test], y_pred_meta[~upset_mask_test]\n",
    ")\n",
    "\n",
    "print(f\"Ensemble Model Overall Accuracy: {overall_accuracy_meta:.4f}\")\n",
    "print(f\"Ensemble Model Upset Accuracy: {accuracy_upsets_meta:.4f}\")\n",
    "print(f\"Ensemble Model Non-Upset Accuracy: {accuracy_non_upsets_meta:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Overall Accuracy: 0.7788\n",
      "Ensemble Model Upset Accuracy: 0.3179\n",
      "Ensemble Model Non-Upset Accuracy: 0.9362\n"
     ]
    }
   ],
   "source": [
    "meta_model = xgb.XGBClassifier(eval_metric='error')\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_meta = meta_model.predict(X_meta_test)\n",
    "y_pred_proba_meta = meta_model.predict_proba(X_meta_test)[:, 1]\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_meta = accuracy_score(y_test, y_pred_meta)\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_meta = accuracy_score(\n",
    "    y_test[upset_mask_test], y_pred_meta[upset_mask_test]\n",
    ")\n",
    "accuracy_non_upsets_meta = accuracy_score(\n",
    "    y_test[~upset_mask_test], y_pred_meta[~upset_mask_test]\n",
    ")\n",
    "\n",
    "print(f\"Ensemble Model Overall Accuracy: {overall_accuracy_meta:.4f}\")\n",
    "print(f\"Ensemble Model Upset Accuracy: {accuracy_upsets_meta:.4f}\")\n",
    "print(f\"Ensemble Model Non-Upset Accuracy: {accuracy_non_upsets_meta:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Meta-Model\n",
    "\n",
    "We use the out-of-fold predictions from the base models as features to train a logistic regression meta-model.\n",
    "\n",
    "- **Features**: Predicted probabilities from base models with different upset weights.\n",
    "- **Target**: Actual match outcomes (`winner` column).\n",
    "- **Meta-Model**: Logistic regression that learns to combine base model predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model ROC AUC: 0.8573\n",
      "Confusion Matrix:\n",
      "[[139074  39698]\n",
      " [ 39450 139550]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_meta)\n",
    "\n",
    "print(f\"Ensemble Model ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_meta)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Ensemble\n",
    "\n",
    "We evaluate the ensemble model's performance:\n",
    "\n",
    "- **Overall Accuracy**: *[Insert Overall Accuracy]*\n",
    "- **Upset Accuracy**: *[Insert Upset Accuracy]*\n",
    "- **Non-Upset Accuracy**: *[Insert Non-Upset Accuracy]*\n",
    "- **ROC AUC Score**: Measures the model's ability to distinguish between classes.\n",
    "- **Confusion Matrix**: Provides detailed insight into true positives, false positives, etc.\n",
    "\n",
    "The ensemble model shows improved performance in predicting upsets while maintaining overall accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
