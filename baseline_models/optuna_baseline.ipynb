{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Trials For Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GPU is detected.\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import datetime\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'\n",
    "    \n",
    "if torch.cuda.is_available() == False:\n",
    "    RuntimeError(\"GPU detected: False\")\n",
    "else:\n",
    "    print(\"The GPU is detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "We make a basic NN for binary classification that takes as input a list of integers that correspond to the out_features of each linear layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"Initializes the model layers.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): The number of input features of the dataset.\n",
    "            out_features (list): The number of units in each linear layer.\n",
    "        \"\"\"\n",
    "        # Call the parent class (nn.Module) initializer first\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        input_dropout = 0.2\n",
    "        dropout = 0.3\n",
    "\n",
    "        # Input dropout layer\n",
    "        layers.append(nn.Dropout(input_dropout))\n",
    "    \n",
    "        # Build layers dynamically\n",
    "        for out_feature in out_features:\n",
    "            layers.append(nn.Linear(in_features, out_feature))\n",
    "            layers.append(nn.BatchNorm1d(out_feature))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_features = out_feature\n",
    "        \n",
    "        # Final output layer for binary classification (with 1 output node)\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        \n",
    "        # Store the sequence of layers\n",
    "        self.sequential = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        return self.sequential(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "features = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "X = df[features].values  # Convert to numpy array\n",
    "y = df['label'].astype(float).values  # Convert to numpy array\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, train_size=.5)\n",
    "\n",
    "# Convert the splits to PyTorch tensors and reshape y to be 2D\n",
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 16\n",
    "batch_size = 8\n",
    "\n",
    "def prepare_data_loaders(X_train, y_train, X_test, y_test, X_val, y_val, batch_size=batch_size, num_workers=num_workers):\n",
    "    # Convert datasets to TensorDataset (pairs features and labels)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    # Create DataLoader objects for train, test, and validation datasets\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(train_dataset, batch_size=batch_size,drop_last=True, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "        \"test\": DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "        \"val\": DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "    }\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Functions\n",
    "Here we have basic train and test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loaders, criterion, optimizer, num_epochs, epoch, device):\n",
    "    model.train()\n",
    "    \n",
    "    # Use tqdm to display progress bar for the training loop\n",
    "    train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    \n",
    "    running_loss = deque(maxlen=10000)\n",
    "    \n",
    "    # Train epoch\n",
    "    for X_train, y_train in train_loader_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_train_gpu = X_train.to(device)\n",
    "        y_train_gpu = y_train.to(device)\n",
    "        \n",
    "        output_gpu = model(X_train_gpu)\n",
    "        \n",
    "        loss = criterion(output_gpu, y_train_gpu)\n",
    "        running_loss.append(loss.item())  # Store loss for averaging\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate and set the average loss for the tqdm progress bar\n",
    "        avg_loss = sum(running_loss) / len(running_loss) if len(running_loss) > 0 else 0\n",
    "        train_loader_tqdm.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "\n",
    "    return\n",
    "\n",
    "def test_model(model, loaders, criterion, device, num_epochs, epoch, loader='test'):\n",
    "    # Validate epoch:\n",
    "    model.eval()\n",
    "    test_loader_tqdm = tqdm(loaders[loader], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    test_loss = []\n",
    "    num_tested = []\n",
    "    correct_pred = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader_tqdm:\n",
    "            X_test_gpu = X_test.to(device)\n",
    "            y_test_gpu = y_test.to(device)\n",
    "            \n",
    "            output_gpu = model(X_test_gpu)\n",
    "            \n",
    "            # Accumulate test loss\n",
    "            test_loss.append(criterion(output_gpu, y_test_gpu).item() * X_test.shape[0])\n",
    "            num_tested.append(X_test.shape[0])\n",
    "            \n",
    "            # Calculate number of correct predictions for binary classification\n",
    "            correct_pred += torch.sum(((nn.Sigmoid()(output_gpu) > 0.5) == y_test_gpu).float()).item()\n",
    "            \n",
    "            test_loader_tqdm.set_postfix(loss=f\"{sum(test_loss) / sum(num_tested):.4f}\", acc=f\"{correct_pred / sum(num_tested):.4f}\")\n",
    "        \n",
    "        # Calculate average loss and accuracy\n",
    "        avg_loss = sum(test_loss) / sum(num_tested)\n",
    "        accuracy = correct_pred / sum(num_tested)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Functions\n",
    "Here we have the same train and test functions as above, but without the progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loaders, criterion, optimizer, num_epochs, epoch, device):\n",
    "    model.train()\n",
    "    \n",
    "    # Use tqdm to display progress bar for the training loop\n",
    "    # train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    \n",
    "    running_loss = deque(maxlen=10000)\n",
    "    \n",
    "    # Train epoch\n",
    "    for X_train, y_train in loaders['train']:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_train_gpu = X_train.to(device)\n",
    "        y_train_gpu = y_train.to(device)\n",
    "        \n",
    "        output_gpu = model(X_train_gpu)\n",
    "        \n",
    "        loss = criterion(output_gpu, y_train_gpu)\n",
    "        running_loss.append(loss.item())  # Store loss for averaging\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate and set the average loss for the tqdm progress bar\n",
    "        avg_loss = sum(running_loss) / len(running_loss) if len(running_loss) > 0 else 0\n",
    "        # train_loader_tqdm.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "\n",
    "    return\n",
    "\n",
    "def test_model(model, loaders, criterion, device, num_epochs, epoch, loader='test'):\n",
    "    # Validate epoch:\n",
    "    model.eval()\n",
    "    # test_loader_tqdm = tqdm(loaders[loader], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    test_loss = []\n",
    "    num_tested = []\n",
    "    correct_pred = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in loaders[loader]:\n",
    "            X_test_gpu = X_test.to(device)\n",
    "            y_test_gpu = y_test.to(device)\n",
    "            \n",
    "            output_gpu = model(X_test_gpu)\n",
    "            \n",
    "            # Accumulate test loss\n",
    "            test_loss.append(criterion(output_gpu, y_test_gpu).item() * X_test.shape[0])  # Fix here\n",
    "            num_tested.append(X_test.shape[0])  # Fix here\n",
    "            \n",
    "            # Calculate number of correct predictions for binary classification\n",
    "            correct_pred += torch.sum(((nn.Sigmoid()(output_gpu) > 0.5) == y_test_gpu).float()).item()\n",
    "            \n",
    "            # test_loader_tqdm.set_postfix(loss=f\"{sum(test_loss) / sum(num_tested):.4f}\", acc=f\"{correct_pred / sum(num_tested):.4f}\")\n",
    "        \n",
    "        # Calculate average loss and accuracy\n",
    "        avg_loss = sum(test_loss) / sum(num_tested)\n",
    "        accuracy = correct_pred / sum(num_tested)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, num_layers, min_out, max_out, in_features, loaders, study_name):\n",
    "    # Generate the output features for each layer using trial suggestions\n",
    "    out_features = []\n",
    "    for i in range(num_layers):\n",
    "        out_features.append(trial.suggest_int(f\"out_features_layer_{i}\", min_out, max_out))\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create model and move to device\n",
    "    model = Model(in_features, out_features).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    num_epochs = 100\n",
    "    # Training loop for num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(model, loaders, criterion, optimizer, num_epochs, epoch, device)\n",
    "        test_loss, test_accuracy = test_model(model, loaders, criterion, device, num_epochs, epoch, loader='test')\n",
    "\n",
    "    # Return the test loss to be minimized\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-20 15:57:53,545] A new study created in memory with name: Baseline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659f4bc4ac8142d48930bfce360aff7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-10-20 15:58:05,133] Trial 0 finished with value: 0.5822103241215582 and parameters: {'out_features_layer_0': 449, 'out_features_layer_1': 108, 'out_features_layer_2': 135, 'out_features_layer_3': 44, 'out_features_layer_4': 419}. Best is trial 0 with value: 0.5822103241215582.\n",
      "[I 2024-10-20 15:58:15,732] Trial 1 finished with value: 0.541307588763859 and parameters: {'out_features_layer_0': 340, 'out_features_layer_1': 381, 'out_features_layer_2': 370, 'out_features_layer_3': 220, 'out_features_layer_4': 447}. Best is trial 1 with value: 0.541307588763859.\n",
      "[I 2024-10-20 15:58:26,412] Trial 2 finished with value: 0.5368761085945627 and parameters: {'out_features_layer_0': 241, 'out_features_layer_1': 372, 'out_features_layer_2': 372, 'out_features_layer_3': 365, 'out_features_layer_4': 179}. Best is trial 2 with value: 0.5368761085945627.\n",
      "[I 2024-10-20 15:58:37,101] Trial 3 finished with value: 0.5360770987427753 and parameters: {'out_features_layer_0': 231, 'out_features_layer_1': 98, 'out_features_layer_2': 490, 'out_features_layer_3': 81, 'out_features_layer_4': 183}. Best is trial 3 with value: 0.5360770987427753.\n",
      "[I 2024-10-20 15:58:47,633] Trial 4 finished with value: 0.5623642651931099 and parameters: {'out_features_layer_0': 382, 'out_features_layer_1': 149, 'out_features_layer_2': 234, 'out_features_layer_3': 359, 'out_features_layer_4': 358}. Best is trial 3 with value: 0.5360770987427753.\n",
      "Best parameters: {'out_features_layer_0': 231, 'out_features_layer_1': 98, 'out_features_layer_2': 490, 'out_features_layer_3': 81, 'out_features_layer_4': 183}\n",
      "Best trial: FrozenTrial(number=3, state=TrialState.COMPLETE, values=[0.5360770987427753], datetime_start=datetime.datetime(2024, 10, 20, 15, 58, 26, 413153), datetime_complete=datetime.datetime(2024, 10, 20, 15, 58, 37, 101782), params={'out_features_layer_0': 231, 'out_features_layer_1': 98, 'out_features_layer_2': 490, 'out_features_layer_3': 81, 'out_features_layer_4': 183}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'out_features_layer_0': IntDistribution(high=512, log=False, low=32, step=1), 'out_features_layer_1': IntDistribution(high=512, log=False, low=32, step=1), 'out_features_layer_2': IntDistribution(high=512, log=False, low=32, step=1), 'out_features_layer_3': IntDistribution(high=512, log=False, low=32, step=1), 'out_features_layer_4': IntDistribution(high=512, log=False, low=32, step=1)}, trial_id=3, value=None)\n"
     ]
    }
   ],
   "source": [
    "loaders = prepare_data_loaders(X_train, y_train, X_test, y_test, X_val, y_val, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# Define the parameters for the study\n",
    "study_name = \"Baseline\"               # Name of your study\n",
    "num_layers = 5                        # Example number of layers\n",
    "min_out = 32                          # Minimum number of units per layer\n",
    "max_out = 512                         # Maximum number of units per layer\n",
    "in_features = X_train.shape[1]        # Set in_features to match the actual number of features in your data\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, direction='minimize')\n",
    "\n",
    "# Define the objective function and run the optimization\n",
    "study.optimize(lambda trial: objective(trial, num_layers, min_out, max_out, in_features, loaders, study_name), \n",
    "               n_trials=5, show_progress_bar=True)  # You can specify how many trials you want\n",
    "\n",
    "# Print the best parameters found by the study\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best trial: {study.best_trial}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
