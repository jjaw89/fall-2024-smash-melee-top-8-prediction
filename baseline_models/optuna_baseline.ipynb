{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Trials For Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime \n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pymysql\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"Initializes the model layers.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): The number of input features of the dataset.\n",
    "            out_features (list): The number of units in each linear layer.\n",
    "        \"\"\"\n",
    "        num_layers = len(out_features)\n",
    "        input_dropout = .2\n",
    "        dropout = .3\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(nn.Dropout(input_dropout))\n",
    "    \n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(in_features, out_features[i]))\n",
    "            layers.append(nn.BatchNorm1d(out_features[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_features = out_features[i]\n",
    "        \n",
    "        # Binary classification with loss function BCEWithLogitsLoss\n",
    "        layers.append(nn.Linear(in_features,1))\n",
    "        \n",
    "        self.sequential = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 16\n",
    "batch_size = 8\n",
    "\n",
    "def prepare_data_loaders(batch_size=batch_size, num_workers=num_workers):\n",
    "    \n",
    "    loaders = {\n",
    "        \"train\": DataLoader(batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "        \"test\": DataLoader(batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "        \"val\": DataLoader(batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "    }\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loaders, criterion, optimizer, num_epochs, epoch, device):\n",
    "    model.train()\n",
    "    train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    running_loss = deque(maxlen=10000)\n",
    "    \n",
    "    # Train epoch\n",
    "    for X_train, y_train in train_loader_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_train_gpu = X_train.to(device)\n",
    "        y_train_gpu = y_train.to(device)\n",
    "        \n",
    "        output_gpu = model(X_train_gpu)\n",
    "        \n",
    "        loss = criterion(output_gpu, y_train_gpu)\n",
    "        running_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loader_tqdm.set_postfix(f\"loss={running_loss / len(running_loss)}\")\n",
    "        \n",
    "    return\n",
    "    \n",
    "\n",
    "def test_model(model, loaders, criterion, device, num_epochs, epoch, loader='test'):\n",
    "    # Validate epoch:\n",
    "    model.eval()\n",
    "    test_loader_tqdm = tqdm(loaders[loader], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    test_loss = []\n",
    "    num_tested = []\n",
    "    correct_pred = []\n",
    "    \n",
    "    for X_test, y_test in test_loader_tqdm:\n",
    "        X_test_gpu = X_test.to(device)\n",
    "        y_test_gpu = y_test.to(device)\n",
    "        \n",
    "        output_gpu = model(X_test_gpu)\n",
    "        \n",
    "        test_loss.append(criterion(output_gpu, y_test_gpu).item() * X_test.shape[0])\n",
    "        num_tested.append(X_test.shape(0))\n",
    "        \n",
    "        correct_pred += torch.sum(((nn.Sigmoid(output_gpu) > .5) == y_test_gpu))\n",
    "        \n",
    "        test_loader_tqdm.set_postfix(f\"loss={sum(test_loss) / sum(num_tested)}, acc={correct_pred / sum(num_tested)}\")\n",
    "    \n",
    "    return sum(test_loss) / sum(num_tested)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, num_layers, min_out, max_out, in_features, loaders, study_name):\n",
    "    out_features = []\n",
    "    for i in range(num_layers):\n",
    "        out_features.append(trial.suggest_int(f\"out_features_layer_{i}\"), min_out, max_out)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = Model(in_features, out_features).to(device)\n",
    "    model.compile()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(model, loaders, criterion, optimizer, num_epochs, epoch, device)\n",
    "        test_model(model, loaders, criterion, device, num_epochs, epoch, loader='test')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility with a unique(ish) string\n",
    "seed = int(datetime.now().strftime('%Y%m%d%H%M%S')) % (2**32 - 1)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "sampler = optuna.samplers.TPESampler(seed = seed)\n",
    "\n",
    "# Set Some Variables\n",
    "# study_name = current_datetime_string + \"Basic CNN - Classify All Characters\"\n",
    "study_name = \"Baseline\"\n",
    "batch_size = 8\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
