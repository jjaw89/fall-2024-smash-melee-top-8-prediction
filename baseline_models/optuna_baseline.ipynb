{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Trials For Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import datetime\n",
    "import os\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'\n",
    "    \n",
    "if torch.cuda.is_available() == False:\n",
    "    RuntimeError(\"GPU detected: False\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"The GPU is detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "We make a basic NN for binary classification that takes as input a list of integers that correspond to the out_features of each linear layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, out_features, input_dropout=.2, hidden_dropout=.3):\n",
    "        \"\"\"Initializes the model layers.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): The number of input features of the dataset.\n",
    "            out_features (list): The number of units in each linear layer.\n",
    "        \"\"\"\n",
    "        # Call the parent class (nn.Module) initializer first\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "\n",
    "        # Input dropout layer\n",
    "        layers.append(nn.Dropout(input_dropout))\n",
    "    \n",
    "        # Build layers dynamically\n",
    "        for out_feature in out_features:\n",
    "            layers.append(nn.Linear(in_features, out_feature))\n",
    "            layers.append(nn.BatchNorm1d(out_feature))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(hidden_dropout))\n",
    "            in_features = out_feature\n",
    "        \n",
    "        # Final output layer for binary classification (with 1 output node)\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        \n",
    "        # Store the sequence of layers\n",
    "        self.sequential = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        return self.sequential(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "We create the dataset we are going to train the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small dataset to test the code before our dataset is ready.\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "features = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "X = df[features].values  # Convert to numpy array\n",
    "y = df['label'].astype(float).values  # Convert to numpy array\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, train_size=.5, random_state=103)\n",
    "\n",
    "# Convert the splits to PyTorch tensors and reshape y to be 2D\n",
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loaders(X_train, y_train, X_test, y_test, X_val, y_val, batch_size=8, num_workers=16):\n",
    "    # Convert datasets to TensorDataset (pairs features and labels)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    # Create DataLoader objects for train, test, and validation datasets\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(train_dataset, batch_size=batch_size,drop_last=True, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "        \"test\": DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "        \"val\": DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "    }\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Functions\n",
    "Here we have basic train and test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_progress(model, loaders, criterion, optimizer, num_epochs, epoch, device):\n",
    "    model.train()\n",
    "    \n",
    "    # Use tqdm to display progress bar for the training loop\n",
    "    leave = True\n",
    "    train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch', leave=leave)\n",
    "    \n",
    "    # Our training dataset is has well over a million examples.\n",
    "    # We expect the loss to change a lot over a single epoch,\n",
    "    # so we only show the loss of the 10_000 most recent batches.\n",
    "    running_loss = deque(maxlen=10000)\n",
    "    \n",
    "    # Train epoch\n",
    "    for X_train, y_train in train_loader_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_train_gpu = X_train.to(device)\n",
    "        y_train_gpu = y_train.to(device)\n",
    "        \n",
    "        output_gpu = model(X_train_gpu)\n",
    "        \n",
    "        loss = criterion(output_gpu, y_train_gpu)\n",
    "        running_loss.append(loss.item())  # Store loss for averaging\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate and set the average loss for the tqdm progress bar\n",
    "        avg_loss = sum(running_loss) / len(running_loss) if len(running_loss) > 0 else 0\n",
    "        train_loader_tqdm.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "\n",
    "    return\n",
    "\n",
    "def test_model_progress(model, loaders, criterion, device, num_epochs, epoch, loader='test'):\n",
    "    # Validate epoch:\n",
    "    model.eval()\n",
    "    leave = True\n",
    "    test_loader_tqdm = tqdm(loaders[loader], desc=f'Test {epoch+1}/{num_epochs}', unit='batch', leave=leave)\n",
    "    test_loss = []\n",
    "    num_tested = []\n",
    "    correct_pred = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader_tqdm:\n",
    "            X_test_gpu = X_test.to(device)\n",
    "            y_test_gpu = y_test.to(device)\n",
    "            \n",
    "            output_gpu = model(X_test_gpu)\n",
    "            \n",
    "            # Accumulate test loss\n",
    "            test_loss.append(criterion(output_gpu, y_test_gpu).item() * X_test.shape[0])\n",
    "            num_tested.append(X_test.shape[0])\n",
    "            \n",
    "            # Calculate number of correct predictions for binary classification\n",
    "            correct_pred += torch.sum(((nn.Sigmoid()(output_gpu) > 0.5) == y_test_gpu).float()).item()\n",
    "            \n",
    "            test_loader_tqdm.set_postfix(loss=f\"{sum(test_loss) / sum(num_tested):.4f}\", acc=f\"{correct_pred / sum(num_tested):.1%}\")\n",
    "        \n",
    "        # Calculate average loss and accuracy\n",
    "        avg_loss = sum(test_loss) / sum(num_tested)\n",
    "        accuracy = correct_pred / sum(num_tested)\n",
    "        \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the same train and test functions as above, but without the progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loaders, criterion, optimizer, num_epochs, epoch, device):\n",
    "    model.train()\n",
    "    \n",
    "    # Train epoch\n",
    "    for X_train, y_train in loaders['train']:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_train_gpu = X_train.to(device)\n",
    "        y_train_gpu = y_train.to(device)\n",
    "        \n",
    "        output_gpu = model(X_train_gpu)\n",
    "        \n",
    "        loss = criterion(output_gpu, y_train_gpu)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return \n",
    "\n",
    "def test_model(model, loaders, criterion, device, num_epochs, epoch, loader='test'):\n",
    "    # Validate epoch:\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    num_tested = []\n",
    "    correct_pred = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in loaders[loader]:\n",
    "            X_test_gpu = X_test.to(device)\n",
    "            y_test_gpu = y_test.to(device)\n",
    "            \n",
    "            output_gpu = model(X_test_gpu)\n",
    "            \n",
    "            # Accumulate test loss.\n",
    "            test_loss.append(criterion(output_gpu, y_test_gpu).item() * X_test.shape[0])\n",
    "            num_tested.append(X_test.shape[0])\n",
    "            \n",
    "            # Calculate number of correct predictions for binary classification.\n",
    "            correct_pred += torch.sum(((nn.Sigmoid()(output_gpu) > 0.5) == y_test_gpu).float()).item()\n",
    "        \n",
    "        # Calculate average loss and accuracy\n",
    "        avg_loss = sum(test_loss) / sum(num_tested)\n",
    "        accuracy = correct_pred / sum(num_tested)\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Study\n",
    "We create a simple optuna study to find a good model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, num_layers, min_out, max_out, in_features, loaders, study_name):\n",
    "    input_dropout = trial.suggest_float(\"input_dropout\", 0, .9)\n",
    "    hidden_dropout = trial.suggest_float(\"hidden_dropout\", 0, .9)\n",
    "    \n",
    "    # Generate the output features for each layer using trial suggestions\n",
    "    out_features = []\n",
    "    for i in range(num_layers):\n",
    "        out_features.append(trial.suggest_int(f\"out_features_layer_{i}\", min_out, max_out))\n",
    "    \n",
    "    # Create model and move to device\n",
    "    model = Model(in_features, out_features, input_dropout, hidden_dropout).to(device)\n",
    "    \n",
    "    # Compile the model (not always worth it)\n",
    "    # model.compile()\n",
    "    \n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    num_epochs = 10\n",
    "    \n",
    "    # Training loop for num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(model, loaders, criterion, optimizer, num_epochs, epoch, device)\n",
    "        test_loss, test_accuracy = test_model(model, loaders, criterion, device, num_epochs, epoch, loader='test')\n",
    "    \n",
    "    ## Print results if we want \n",
    "    # print(f\"Loss={test_loss:0.5f}, Accuracy={test_accuracy:0.1%}\")\n",
    "    \n",
    "    # Return the test loss to be minimized\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = prepare_data_loaders(X_train, y_train, X_test, y_test, X_val, y_val, batch_size=8, num_workers=1)\n",
    "\n",
    "# Define the parameters for the study\n",
    "study_name = \"Baseline\"\n",
    "num_layers = 5\n",
    "min_out = 64\n",
    "max_out = 4096\n",
    "in_features = X_train.shape[1]\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(study_name=study_name, direction='minimize')\n",
    "\n",
    "# Define the objective function and run the optimization\n",
    "study.optimize(lambda trial: objective(trial, num_layers, min_out, max_out, in_features, loaders, study_name), \n",
    "               n_trials=5, show_progress_bar=True)  # You can specify how many trials you want\n",
    "\n",
    "# Print the best parameters found by the study\n",
    "print()\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best trial: {study.best_trial}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters of the best study\n",
    "out_features = list(study.best_params.values())[2:]  # Adjust indexing as needed\n",
    "input_dropout = study.best_params['input_dropout']   # Example key name\n",
    "hidden_dropout = study.best_params['hidden_dropout'] # Example key name\n",
    "\n",
    "# Build the model\n",
    "# model = Model(in_features, out_features, input_dropout, hidden_dropout)\n",
    "model = Model(in_features, [4096*2], 0, .85)\n",
    "\n",
    "loaders = prepare_data_loaders(X_train, y_train, X_test, y_test, X_val, y_val, batch_size=4, num_workers=1)\n",
    "\n",
    "## Compiling might not be worth it (Cannot save the model if we do.)\n",
    "# model = torch.compile(model)#, mode = 'max-autotune')\n",
    "\n",
    "# Move model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = 15\n",
    "# Training loop for num_epochs\n",
    "for epoch in range(num_epochs):\n",
    "    train_epoch_progress(model, loaders, criterion, optimizer, num_epochs, epoch, device)\n",
    "    test_loss, test_accuracy = test_model_progress(model, loaders, criterion, device, num_epochs, epoch, loader='test')\n",
    "\n",
    "test_loss, test_accuracy = test_model_progress(model, loaders, criterion, device, num_epochs, epoch, loader='val')\n",
    "print(f\"Val: Loss={test_loss:0.5f}, Accuracy={test_accuracy:0.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
