{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Trials For Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime \n",
    "\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"Initializes the model layers.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): The number of input features of the dataset.\n",
    "            out_features (list): The number of units in each linear layer.\n",
    "        \"\"\"\n",
    "        num_layers = len(out_features)\n",
    "        input_dropout = .2\n",
    "        dropout = .3\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(nn.Dropout(input_dropout))\n",
    "    \n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(in_features, out_features[i]))\n",
    "            layers.append(nn.BatchNorm1d(out_features[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_features = out_features[i]\n",
    "        \n",
    "        # Binary classification with loss function BCEWithLogitsLoss\n",
    "        layers.append(nn.Linear(in_features,1))\n",
    "        \n",
    "        self.sequential = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 16\n",
    "batch_size = 8\n",
    "\n",
    "def prepare_data_loaders(batch_size=batch_size, num_workers=num_workers):\n",
    "    \n",
    "    loaders = {\n",
    "        \"train\": DataLoader(batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "        \"test\": DataLoader(batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "        \"val\": DataLoader(batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "    }\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loaders, criterion, optimizer, num_epochs, epoch, device):\n",
    "    model.train()\n",
    "    train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    running_loss = deque(maxlen=10000)\n",
    "    \n",
    "    # Train epoch\n",
    "    for X_train, y_train in train_loader_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_train_gpu = X_train.to(device)\n",
    "        y_train_gpu = y_train.to(device)\n",
    "        \n",
    "        output_gpu = model(X_train_gpu)\n",
    "        \n",
    "        loss = criterion(output_gpu, y_train_gpu)\n",
    "        running_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loader_tqdm.set_postfix(f\"loss={running_loss / len(running_loss)}\")\n",
    "        \n",
    "    return\n",
    "    \n",
    "\n",
    "def test_model(model, loaders, criterion, device, num_epochs, epoch, loader='test'):\n",
    "    # Validate epoch:\n",
    "    model.eval()\n",
    "    test_loader_tqdm = tqdm(loaders[loader], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    test_loss = []\n",
    "    num_tested = []\n",
    "    correct_pred = []\n",
    "    \n",
    "    for X_test, y_test in test_loader_tqdm:\n",
    "        X_test_gpu = X_test.to(device)\n",
    "        y_test_gpu = y_test.to(device)\n",
    "        \n",
    "        output_gpu = model(X_test_gpu)\n",
    "        \n",
    "        test_loss.append(criterion(output_gpu, y_test_gpu).item() * X_test.shape[0])\n",
    "        num_tested.append(X_test.shape(0))\n",
    "        \n",
    "        correct_pred += torch.sum(((nn.Sigmoid(output_gpu) > .5) == y_test_gpu))\n",
    "        \n",
    "        test_loader_tqdm.set_postfix(f\"loss={sum(test_loss) / sum(num_tested)}, acc={correct_pred / sum(num_tested)}\")\n",
    "    \n",
    "    return sum(test_loss) / sum(num_tested), correct_pred / sum(num_tested)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, num_layers, min_out, max_out, in_features, loaders, study_name):\n",
    "    # Generate the output features for each layer using trial suggestions\n",
    "    out_features = []\n",
    "    for i in range(num_layers):\n",
    "        out_features.append(trial.suggest_int(f\"out_features_layer_{i}\", min_out, max_out))\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create model and move to device\n",
    "    model = Model(in_features, out_features).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    num_epochs = 3\n",
    "    # Training loop for num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(model, loaders, criterion, optimizer, num_epochs, epoch, device)\n",
    "        test_loss, test_accuracy = test_model(model, loaders, criterion, device, num_epochs, epoch, loader='test')\n",
    "\n",
    "    # Return the test loss to be minimized\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = prepare_data_loaders()\n",
    "\n",
    "# Define the parameters for the study\n",
    "study_name = \"Baseline\"               # Name of your study\n",
    "num_layers = 5                        # Example number of layers\n",
    "min_out = 32                          # Minimum number of units per layer\n",
    "max_out = 512                         # Maximum number of units per layer\n",
    "in_features = 128                     # Example input feature size\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, direction='minimize')\n",
    "\n",
    "# Define the objective function and run the optimization\n",
    "study.optimize(lambda trial: objective(trial, num_layers, min_out, max_out, in_features, loaders, study_name), \n",
    "               n_trials=5, show_progress_bar=True)  # You can specify how many trials you want\n",
    "\n",
    "# Print the best parameters found by the study\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best trial: {study.best_trial}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
