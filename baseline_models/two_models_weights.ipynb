{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Match Winners with XGBoost Classifier\n",
    "\n",
    "In this notebook, we aim to predict the winner of a match using an XGBoost classifier. We want to give higher importance to correctly predicting upsets, defined as instances where a player with a lower default ELO wins the match.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Preparation](#data-preparation)\n",
    "2. [Baseline Model](#baseline-model)\n",
    "3. [Weighted Model](#weighted-model)\n",
    "4. [Hyperparameter Optimization with Optuna](#hyperparameter-optimization)\n",
    "5. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import datetime\n",
    "import os\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import optuna\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if os.path.exists('/workspace/data_2'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data_2/'\n",
    "else:\n",
    "    data_path = '../data/'\n",
    "    \n",
    "# if torch.cuda.is_available() == False:\n",
    "#     RuntimeError(\"GPU detected: False\")\n",
    "#     print(\"GPU detected: False\")\n",
    "# else:\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"The GPU is detected.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_pickle(data_path + 'full_dataset_df.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify columns for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 key_x\n",
      "1 game\n",
      "2 tournament_key\n",
      "3 winner_id\n",
      "4 loser_id\n",
      "5 p1_id\n",
      "6 p2_id\n",
      "7 p1_score\n",
      "8 p2_score\n",
      "9 valid_score\n",
      "10 best_of\n",
      "11 location_names\n",
      "12 bracket_name\n",
      "13 bracket_order\n",
      "14 set_order\n",
      "15 game_data\n",
      "16 top_8\n",
      "17 top_8_location_names\n",
      "18 valid_top_8_bracket\n",
      "19 top_8_bracket_location_names\n",
      "20 major\n",
      "21 key_y\n",
      "22 start\n",
      "23 end\n",
      "24 start_week\n",
      "25 p1_characters\n",
      "26 p2_characters\n",
      "27 p1_consistent\n",
      "28 p2_consistent\n",
      "29 matchup_strings\n",
      "30 end_week\n",
      "31 players_have_history\n",
      "32 (p1/p2)_sorted\n",
      "33 (p1/p2)_was_sorted\n",
      "34 results_sorted\n",
      "35 results\n",
      "36 matchup_1\n",
      "37 matchup_2\n",
      "38 matchup_3\n",
      "39 matchup_4\n",
      "40 matchup_5\n",
      "41 matchup_6\n",
      "42 matchup_7\n",
      "43 matchup_8\n",
      "44 matchup_9\n",
      "45 matchup_10\n",
      "46 winner\n",
      "47 p1_default_elo\n",
      "48 p2_default_elo\n",
      "49 p1_default_rd\n",
      "50 p2_default_rd\n",
      "51 p1_default_updates\n",
      "52 p2_default_updates\n",
      "53 start_index\n",
      "54 start_date\n",
      "55 p1_fox_count\n",
      "56 p1_falco_count\n",
      "57 p1_marth_count\n",
      "58 p1_sheik_count\n",
      "59 p1_captainfalcon_count\n",
      "60 p1_jigglypuff_count\n",
      "61 p1_peach_count\n",
      "62 p1_luigi_count\n",
      "63 p1_samus_count\n",
      "64 p1_ganondorf_count\n",
      "65 p1_iceclimbers_count\n",
      "66 p1_drmario_count\n",
      "67 p1_yoshi_count\n",
      "68 p1_pikachu_count\n",
      "69 p1_link_count\n",
      "70 p1_mrgameandwatch_count\n",
      "71 p1_donkeykong_count\n",
      "72 p1_mario_count\n",
      "73 p1_zelda_count\n",
      "74 p1_roy_count\n",
      "75 p1_younglink_count\n",
      "76 p1_kirby_count\n",
      "77 p1_ness_count\n",
      "78 p1_bowser_count\n",
      "79 p1_pichu_count\n",
      "80 p1_random_count\n",
      "81 p1_mewtwo_count\n",
      "82 p2_fox_count\n",
      "83 p2_falco_count\n",
      "84 p2_marth_count\n",
      "85 p2_sheik_count\n",
      "86 p2_captainfalcon_count\n",
      "87 p2_jigglypuff_count\n",
      "88 p2_peach_count\n",
      "89 p2_luigi_count\n",
      "90 p2_samus_count\n",
      "91 p2_ganondorf_count\n",
      "92 p2_iceclimbers_count\n",
      "93 p2_drmario_count\n",
      "94 p2_yoshi_count\n",
      "95 p2_pikachu_count\n",
      "96 p2_link_count\n",
      "97 p2_mrgameandwatch_count\n",
      "98 p2_donkeykong_count\n",
      "99 p2_mario_count\n",
      "100 p2_zelda_count\n",
      "101 p2_roy_count\n",
      "102 p2_younglink_count\n",
      "103 p2_kirby_count\n",
      "104 p2_ness_count\n",
      "105 p2_bowser_count\n",
      "106 p2_pichu_count\n",
      "107 p2_random_count\n",
      "108 p2_mewtwo_count\n",
      "109 p1_m1_usage\n",
      "110 p2_m1_usage\n",
      "111 p1/m1/m1_alt2_elo\n",
      "112 p1/m1/m1_alt2_rd\n",
      "113 p1/m1/m1_alt2_updates\n",
      "114 p2/m1/m1_alt2_elo\n",
      "115 p2/m1/m1_alt2_rd\n",
      "116 p2/m1/m1_alt2_updates\n",
      "117 p1/m1_alt3_elo\n",
      "118 p1/m1_alt3_rd\n",
      "119 p1/m1_alt3_updates\n",
      "120 p2/m1_alt3_elo\n",
      "121 p2/m1_alt3_rd\n",
      "122 p2/m1_alt3_updates\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(dataset_df.columns):\n",
    "    print(i, col)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the features of player one and player two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matchup_1', 'matchup_2', 'matchup_3', 'matchup_4', 'matchup_5', 'matchup_6', 'matchup_7', 'matchup_8', 'matchup_9', 'matchup_10', 'p1_default_elo', 'p2_default_elo', 'p1_default_rd', 'p2_default_rd', 'p1_default_updates', 'p2_default_updates', 'p1_fox_count', 'p1_falco_count', 'p1_marth_count', 'p1_sheik_count', 'p1_captainfalcon_count', 'p1_jigglypuff_count', 'p1_peach_count', 'p1_luigi_count', 'p1_samus_count', 'p1_ganondorf_count', 'p1_iceclimbers_count', 'p1_drmario_count', 'p1_yoshi_count', 'p1_pikachu_count', 'p1_link_count', 'p1_mrgameandwatch_count', 'p1_donkeykong_count', 'p1_mario_count', 'p1_zelda_count', 'p1_roy_count', 'p1_younglink_count', 'p1_kirby_count', 'p1_ness_count', 'p1_bowser_count', 'p1_pichu_count', 'p1_random_count', 'p1_mewtwo_count', 'p2_fox_count', 'p2_falco_count', 'p2_marth_count', 'p2_sheik_count', 'p2_captainfalcon_count', 'p2_jigglypuff_count', 'p2_peach_count', 'p2_luigi_count', 'p2_samus_count', 'p2_ganondorf_count', 'p2_iceclimbers_count', 'p2_drmario_count', 'p2_yoshi_count', 'p2_pikachu_count', 'p2_link_count', 'p2_mrgameandwatch_count', 'p2_donkeykong_count', 'p2_mario_count', 'p2_zelda_count', 'p2_roy_count', 'p2_younglink_count', 'p2_kirby_count', 'p2_ness_count', 'p2_bowser_count', 'p2_pichu_count', 'p2_random_count', 'p2_mewtwo_count', 'p1_m1_usage', 'p2_m1_usage', 'p1/m1/m1_alt2_elo', 'p1/m1/m1_alt2_rd', 'p1/m1/m1_alt2_updates', 'p2/m1/m1_alt2_elo', 'p2/m1/m1_alt2_rd', 'p2/m1/m1_alt2_updates', 'p1/m1_alt3_elo', 'p1/m1_alt3_rd', 'p1/m1_alt3_updates', 'p2/m1_alt3_elo', 'p2/m1_alt3_rd', 'p2/m1_alt3_updates']\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "features = (\n",
    "    list(dataset_df.columns[36:46]) +\n",
    "    list(dataset_df.columns[47:53]) +\n",
    "    list(dataset_df.columns[55:])\n",
    ").copy()\n",
    "target = 'winner'\n",
    "\n",
    "print(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the 'expected_winner' column\n",
    "# An expected winner is defined only when ELOs are strictly different\n",
    "# If ELOs are equal, expected_winner is set to NaN\n",
    "dataset_df['expected_winner'] = np.where(\n",
    "    dataset_df['p1_default_elo'] > dataset_df['p2_default_elo'], 1,\n",
    "    np.where(dataset_df['p1_default_elo'] < dataset_df['p2_default_elo'], 0, np.nan)\n",
    ")\n",
    "\n",
    "# 2. Define 'upset' only when 'expected_winner' is not NaN\n",
    "dataset_df['upset'] = np.where(\n",
    "    dataset_df['expected_winner'].notna() & (dataset_df['winner'] != dataset_df['expected_winner']), 1, 0\n",
    ")\n",
    "\n",
    "# 3. Remove matches where ELOs are equal (expected_winner is NaN)\n",
    "dataset_df = dataset_df[dataset_df['expected_winner'].notna()].reset_index(drop=True)\n",
    "\n",
    "# 4. Split the data into upsets and non-upsets\n",
    "upsets_df = dataset_df[dataset_df['upset'] == 1]\n",
    "non_upsets_df = dataset_df[dataset_df['upset'] == 0]\n",
    "\n",
    "# 5. Split each into train and test sets\n",
    "upsets_train, upsets_test = train_test_split(\n",
    "    upsets_df, test_size=0.2, random_state=42\n",
    ")\n",
    "non_upsets_train, non_upsets_test = train_test_split(\n",
    "    non_upsets_df, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 6. Combine the train and test sets\n",
    "train_data = pd.concat([upsets_train, non_upsets_train]).sample(\n",
    "    frac=1, random_state=42\n",
    ").reset_index(drop=True)\n",
    "test_data = pd.concat([upsets_test, non_upsets_test]).sample(\n",
    "    frac=1, random_state=42\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 7. Separate features and target\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['matchup_1', 'matchup_2', 'matchup_3', 'matchup_4', 'matchup_5',\n",
       "       'matchup_6', 'matchup_7', 'matchup_8', 'matchup_9', 'matchup_10',\n",
       "       'p1_default_elo', 'p2_default_elo', 'p1_default_rd', 'p2_default_rd',\n",
       "       'p1_default_updates', 'p2_default_updates', 'p1_fox_count',\n",
       "       'p1_falco_count', 'p1_marth_count', 'p1_sheik_count',\n",
       "       'p1_captainfalcon_count', 'p1_jigglypuff_count', 'p1_peach_count',\n",
       "       'p1_luigi_count', 'p1_samus_count', 'p1_ganondorf_count',\n",
       "       'p1_iceclimbers_count', 'p1_drmario_count', 'p1_yoshi_count',\n",
       "       'p1_pikachu_count', 'p1_link_count', 'p1_mrgameandwatch_count',\n",
       "       'p1_donkeykong_count', 'p1_mario_count', 'p1_zelda_count',\n",
       "       'p1_roy_count', 'p1_younglink_count', 'p1_kirby_count', 'p1_ness_count',\n",
       "       'p1_bowser_count', 'p1_pichu_count', 'p1_random_count',\n",
       "       'p1_mewtwo_count', 'p2_fox_count', 'p2_falco_count', 'p2_marth_count',\n",
       "       'p2_sheik_count', 'p2_captainfalcon_count', 'p2_jigglypuff_count',\n",
       "       'p2_peach_count', 'p2_luigi_count', 'p2_samus_count',\n",
       "       'p2_ganondorf_count', 'p2_iceclimbers_count', 'p2_drmario_count',\n",
       "       'p2_yoshi_count', 'p2_pikachu_count', 'p2_link_count',\n",
       "       'p2_mrgameandwatch_count', 'p2_donkeykong_count', 'p2_mario_count',\n",
       "       'p2_zelda_count', 'p2_roy_count', 'p2_younglink_count',\n",
       "       'p2_kirby_count', 'p2_ness_count', 'p2_bowser_count', 'p2_pichu_count',\n",
       "       'p2_random_count', 'p2_mewtwo_count', 'p1_m1_usage', 'p2_m1_usage',\n",
       "       'p1/m1/m1_alt2_elo', 'p1/m1/m1_alt2_rd', 'p1/m1/m1_alt2_updates',\n",
       "       'p2/m1/m1_alt2_elo', 'p2/m1/m1_alt2_rd', 'p2/m1/m1_alt2_updates',\n",
       "       'p1/m1_alt3_elo', 'p1/m1_alt3_rd', 'p1/m1_alt3_updates',\n",
       "       'p2/m1_alt3_elo', 'p2/m1_alt3_rd', 'p2/m1_alt3_updates'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2546142541139352\n"
     ]
    }
   ],
   "source": [
    "print(dataset_df['upset'].sum()/dataset_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Overall Accuracy: 0.7763\n",
      "Baseline Model Upset Accuracy: 0.3059\n",
      "Baseline Model Non-Upset Accuracy: 0.9369\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost classifier without weights\n",
    "baseline_model = xgb.XGBClassifier( eval_metric='error')\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "# Add predictions to test_data\n",
    "test_data['prediction'] = y_pred_baseline\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "upset_mask = test_data['upset'] == 1\n",
    "accuracy_upsets = accuracy_score(\n",
    "    y_test[upset_mask], y_pred_baseline[upset_mask]\n",
    ")\n",
    "accuracy_non_upsets = accuracy_score(\n",
    "    y_test[~upset_mask], y_pred_baseline[~upset_mask]\n",
    ")\n",
    "\n",
    "print(f\"Baseline Model Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"Baseline Model Upset Accuracy: {accuracy_upsets:.4f}\")\n",
    "print(f\"Baseline Model Non-Upset Accuracy: {accuracy_non_upsets:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "We train a baseline XGBoost classifier without any weighting to see how well it predicts both upsets and non-upsets.\n",
    "\n",
    "- **Overall Accuracy**: Measures the model's performance across all matches.\n",
    "- **Upset Accuracy**: How often the model correctly predicts upsets.\n",
    "- **Non-Upset Accuracy**: How often the model correctly predicts matches without upsets.\n",
    "\n",
    "The baseline accuracies are:\n",
    "\n",
    "- **Overall Accuracy**: 0.7763\n",
    "- **Upset Accuracy**: 0.3059\n",
    "- **Non-Upset Accuracy**: 0.9369\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Model Overall Accuracy: 0.6830\n",
      "Weighted Model Upset Accuracy: 0.6912\n",
      "Weighted Model Non-Upset Accuracy: 0.6802\n"
     ]
    }
   ],
   "source": [
    "# Create sample weights for the training data\n",
    "sample_weight = np.ones(len(y_train))\n",
    "sample_weight[train_data['upset'] == 1] = 3.5  # Upsets get a weight of 3.5\n",
    "\n",
    "# Train XGBoost classifier with sample weights\n",
    "weighted_model = xgb.XGBClassifier( eval_metric='logloss')\n",
    "weighted_model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_weighted = weighted_model.predict(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
    "\n",
    "# Add predictions to test_data\n",
    "test_data['prediction_weighted'] = y_pred_weighted\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_weighted = accuracy_score(\n",
    "    y_test[upset_mask], y_pred_weighted[upset_mask]\n",
    ")\n",
    "accuracy_non_upsets_weighted = accuracy_score(\n",
    "    y_test[~upset_mask], y_pred_weighted[~upset_mask]\n",
    ")\n",
    "\n",
    "print(f\"Weighted Model Overall Accuracy: {overall_accuracy_weighted:.4f}\")\n",
    "print(f\"Weighted Model Upset Accuracy: {accuracy_upsets_weighted:.4f}\")\n",
    "print(f\"Weighted Model Non-Upset Accuracy: {accuracy_non_upsets_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Model\n",
    "\n",
    "To improve the model's ability to predict upsets, we assign a higher weight (2x) to upset instances in the loss function during training.\n",
    "\n",
    "The weighted model accuracies are:\n",
    "\n",
    "- **Overall Accuracy**: 0.7619\n",
    "- **Upset Accuracy**: 0.4726\n",
    "- **Non-Upset Accuracy**: 0.8607\n",
    "\n",
    "Comparing these results with the baseline model helps us understand the impact of weighting on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-27 18:23:11,137] A new study created in memory with name: no-name-89c6227f-d9b1-4fdb-bee7-fad24905c877\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ebfa7984dd45abbf9a89ca68ca8b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-27 18:23:19,144] Trial 0 finished with value: 0.7763939044978366 and parameters: {'upset_weight': 0.9015182568659634}. Best is trial 0 with value: 0.7763939044978366.\n",
      "[I 2024-11-27 18:23:32,754] Trial 1 finished with value: 0.7748398421340966 and parameters: {'upset_weight': 0.7195488286093267}. Best is trial 0 with value: 0.7763939044978366.\n",
      "[I 2024-11-27 18:23:39,684] Trial 2 finished with value: 0.7690009279652963 and parameters: {'upset_weight': 0.5408382256180367}. Best is trial 0 with value: 0.7763939044978366.\n",
      "[I 2024-11-27 18:23:50,259] Trial 3 finished with value: 0.7749236944199099 and parameters: {'upset_weight': 0.7579044366072831}. Best is trial 0 with value: 0.7763939044978366.\n",
      "[I 2024-11-27 18:23:59,360] Trial 4 finished with value: 0.7756448240779044 and parameters: {'upset_weight': 1.1876425995058697}. Best is trial 0 with value: 0.7763939044978366.\n",
      "[I 2024-11-27 18:24:11,957] Trial 5 finished with value: 0.7751333251344432 and parameters: {'upset_weight': 0.776885375772495}. Best is trial 0 with value: 0.7763939044978366.\n",
      "[I 2024-11-27 18:24:24,707] Trial 6 finished with value: 0.7707338752054381 and parameters: {'upset_weight': 0.599546057015333}. Best is trial 0 with value: 0.7763939044978366.\n",
      "[I 2024-11-27 18:24:34,610] Trial 7 finished with value: 0.7765112976979752 and parameters: {'upset_weight': 1.021123912653107}. Best is trial 7 with value: 0.7765112976979752.\n",
      "[I 2024-11-27 18:24:46,398] Trial 8 finished with value: 0.769744418232841 and parameters: {'upset_weight': 0.5727259624048974}. Best is trial 7 with value: 0.7765112976979752.\n",
      "[I 2024-11-27 18:24:55,988] Trial 9 finished with value: 0.7760892411927149 and parameters: {'upset_weight': 1.106940086760131}. Best is trial 7 with value: 0.7765112976979752.\n",
      "[I 2024-11-27 18:25:03,589] Trial 10 finished with value: 0.7767628545554152 and parameters: {'upset_weight': 0.9650483011024468}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:25:15,486] Trial 11 finished with value: 0.7763044620596358 and parameters: {'upset_weight': 0.988125936794715}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:25:22,299] Trial 12 finished with value: 0.7762988719072482 and parameters: {'upset_weight': 1.0080813961484447}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:25:32,970] Trial 13 finished with value: 0.7765504287646882 and parameters: {'upset_weight': 1.0207460638225085}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:25:50,904] Trial 14 finished with value: 0.7764078798788056 and parameters: {'upset_weight': 0.9065611990694267}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:26:05,242] Trial 15 finished with value: 0.775424013058596 and parameters: {'upset_weight': 1.1847924608314715}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:26:11,990] Trial 16 finished with value: 0.7766426662790827 and parameters: {'upset_weight': 1.0809532747088049}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:26:19,965] Trial 17 finished with value: 0.7760892411927149 and parameters: {'upset_weight': 1.095646504134512}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:26:30,554] Trial 18 finished with value: 0.7763547734311237 and parameters: {'upset_weight': 0.8820675522142235}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:26:41,247] Trial 19 finished with value: 0.7763072571358295 and parameters: {'upset_weight': 1.109270744421282}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:26:47,995] Trial 20 finished with value: 0.7764805518598437 and parameters: {'upset_weight': 0.9493647067358518}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:26:58,818] Trial 21 finished with value: 0.7764498060217121 and parameters: {'upset_weight': 1.0572370133946138}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:27:06,022] Trial 22 finished with value: 0.7766734121172143 and parameters: {'upset_weight': 0.95197737521727}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:27:17,395] Trial 23 finished with value: 0.7758879957067629 and parameters: {'upset_weight': 0.8239050723162616}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:27:29,247] Trial 24 finished with value: 0.7761032165736838 and parameters: {'upset_weight': 0.9343195547961256}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:27:38,971] Trial 25 finished with value: 0.7764917321646188 and parameters: {'upset_weight': 0.9573736349432489}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:27:48,673] Trial 26 finished with value: 0.775837684335275 and parameters: {'upset_weight': 0.8424520292628994}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:28:00,593] Trial 27 finished with value: 0.7760584953545834 and parameters: {'upset_weight': 1.1406206227703535}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:28:13,626] Trial 28 finished with value: 0.7763994946502242 and parameters: {'upset_weight': 1.0594996037643463}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:28:20,678] Trial 29 finished with value: 0.7762206097738225 and parameters: {'upset_weight': 0.8730371268835732}. Best is trial 10 with value: 0.7767628545554152.\n",
      "[I 2024-11-27 18:28:28,135] Trial 30 finished with value: 0.776852296993616 and parameters: {'upset_weight': 0.9732982628261874}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:28:40,413] Trial 31 finished with value: 0.7763603635835112 and parameters: {'upset_weight': 0.9431155822952872}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:28:51,771] Trial 32 finished with value: 0.7765112976979752 and parameters: {'upset_weight': 1.057687062066722}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:28:58,669] Trial 33 finished with value: 0.7763575685073175 and parameters: {'upset_weight': 0.9845199037640753}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:29:08,987] Trial 34 finished with value: 0.7762150196214349 and parameters: {'upset_weight': 0.9098777874314234}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:29:16,474] Trial 35 finished with value: 0.7738923113044062 and parameters: {'upset_weight': 0.6987923487186758}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:29:24,019] Trial 36 finished with value: 0.776251355611954 and parameters: {'upset_weight': 1.0529614868745787}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:29:34,306] Trial 37 finished with value: 0.7755721520968661 and parameters: {'upset_weight': 0.8054907813157517}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:29:50,082] Trial 38 finished with value: 0.7758712252496003 and parameters: {'upset_weight': 1.1513189689852115}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:30:07,749] Trial 39 finished with value: 0.7762038393166598 and parameters: {'upset_weight': 0.9761838430097974}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:30:18,433] Trial 40 finished with value: 0.7743339333430229 and parameters: {'upset_weight': 0.7230643047738985}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:30:28,980] Trial 41 finished with value: 0.7764414207931308 and parameters: {'upset_weight': 1.0245379268136963}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:30:35,454] Trial 42 finished with value: 0.7762709211453104 and parameters: {'upset_weight': 1.0211643780223376}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:30:51,252] Trial 43 finished with value: 0.7764218552597744 and parameters: {'upset_weight': 1.0872531177716285}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:31:06,951] Trial 44 finished with value: 0.776326822669186 and parameters: {'upset_weight': 0.9246281694778065}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:31:17,347] Trial 45 finished with value: 0.7760696756593585 and parameters: {'upset_weight': 0.8831580556534787}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:31:30,622] Trial 46 finished with value: 0.7764581912502935 and parameters: {'upset_weight': 1.0062666228670154}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:31:41,513] Trial 47 finished with value: 0.7762206097738225 and parameters: {'upset_weight': 0.9770752028146832}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:31:53,208] Trial 48 finished with value: 0.7759047661639257 and parameters: {'upset_weight': 1.1499164047526356}. Best is trial 30 with value: 0.776852296993616.\n",
      "[I 2024-11-27 18:32:07,032] Trial 49 finished with value: 0.7761032165736838 and parameters: {'upset_weight': 1.0370036247663939}. Best is trial 30 with value: 0.776852296993616.\n",
      "Best Upset Weight: 0.9733\n",
      "Final Model Overall Accuracy: 0.7769\n",
      "Final Model Upset Accuracy: 0.3027\n",
      "Final Model Non-Upset Accuracy: 0.9388\n"
     ]
    }
   ],
   "source": [
    "# Function to optimize upset weight using Optuna\n",
    "def objective(trial):\n",
    "    # Suggest a weight between 1 and 5\n",
    "    upset_weight = trial.suggest_float('upset_weight', 0.8, 1.2)\n",
    "    \n",
    "    # Create sample weights\n",
    "    sample_weight = np.ones(len(y_train))\n",
    "    sample_weight[train_data['upset'] == 1] = upset_weight\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.XGBClassifier( eval_metric='logloss')\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy  # Optuna minimizes the objective\n",
    "\n",
    "# Create a study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True, timeout=600)\n",
    "\n",
    "# Best upset weight\n",
    "best_upset_weight = study.best_params['upset_weight']\n",
    "print(f\"Best Upset Weight: {best_upset_weight:.4f}\")\n",
    "\n",
    "# Train final model with the best upset weight\n",
    "sample_weight = np.ones(len(y_train))\n",
    "sample_weight[train_data['upset'] == 1] = best_upset_weight\n",
    "\n",
    "final_model = xgb.XGBClassifier(eval_metric='logloss')\n",
    "final_model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_final = accuracy_score(\n",
    "    y_test[upset_mask], y_pred_final[upset_mask]\n",
    ")\n",
    "accuracy_non_upsets_final = accuracy_score(\n",
    "    y_test[~upset_mask], y_pred_final[~upset_mask]\n",
    ")\n",
    "\n",
    "print(f\"Final Model Overall Accuracy: {overall_accuracy_final:.4f}\")\n",
    "print(f\"Final Model Upset Accuracy: {accuracy_upsets_final:.4f}\")\n",
    "print(f\"Final Model Non-Upset Accuracy: {accuracy_non_upsets_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-27 20:33:20,498] A new study created in memory with name: no-name-62347ecd-dc02-4a6b-b716-4eff476d12be\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02b68340b624f6889d1002b2eb143d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0: Log Loss = 0.63623, Accuracy = 72.70832%\n",
      "[I 2024-11-27 20:34:37,769] Trial 0 finished with value: 0.6362348842351547 and parameters: {'n_estimators': 433, 'max_depth': 15, 'learning_rate': 0.1672704480182783, 'min_child_weight': 9, 'gamma': 0.0028772981386906405, 'subsample': 0.6645397320322507, 'colsample_bytree': 0.7986937021243965, 'reg_alpha': 0.15497263510813206, 'reg_lambda': 0.02607990868191984}. Best is trial 0 with value: 0.6362348842351547.\n",
      "Trial 1: Log Loss = 0.58967, Accuracy = 73.22652%\n",
      "[I 2024-11-27 20:34:56,693] Trial 1 finished with value: 0.5896680557446415 and parameters: {'n_estimators': 424, 'max_depth': 6, 'learning_rate': 0.06821367025722622, 'min_child_weight': 7, 'gamma': 2.523286511401221e-07, 'subsample': 0.9653928452412004, 'colsample_bytree': 0.9877056996992711, 'reg_alpha': 7.2462564594556784e-06, 'reg_lambda': 6.846985080569071e-06}. Best is trial 1 with value: 0.5896680557446415.\n",
      "Trial 2: Log Loss = 0.58916, Accuracy = 73.63181%\n",
      "[I 2024-11-27 20:35:34,850] Trial 2 finished with value: 0.589157617514999 and parameters: {'n_estimators': 400, 'max_depth': 8, 'learning_rate': 0.022066921050237148, 'min_child_weight': 5, 'gamma': 8.191800964405627e-06, 'subsample': 0.7376585148457124, 'colsample_bytree': 0.722616417871682, 'reg_alpha': 8.887415131124209e-06, 'reg_lambda': 0.08493496183972747}. Best is trial 2 with value: 0.589157617514999.\n",
      "Trial 3: Log Loss = 0.58955, Accuracy = 71.15593%\n",
      "[I 2024-11-27 20:36:18,427] Trial 3 finished with value: 0.5895512682165013 and parameters: {'n_estimators': 651, 'max_depth': 8, 'learning_rate': 0.232524420576211, 'min_child_weight': 5, 'gamma': 0.05691059524914634, 'subsample': 0.8819910027446516, 'colsample_bytree': 0.6518346915484534, 'reg_alpha': 0.00987115435540548, 'reg_lambda': 1.5736921647147413e-07}. Best is trial 2 with value: 0.589157617514999.\n",
      "Trial 4: Log Loss = 0.59532, Accuracy = 74.03430%\n",
      "[I 2024-11-27 20:36:59,777] Trial 4 finished with value: 0.595320169006774 and parameters: {'n_estimators': 638, 'max_depth': 3, 'learning_rate': 0.16027109027712744, 'min_child_weight': 9, 'gamma': 2.673113902391568e-05, 'subsample': 0.7175430648633282, 'colsample_bytree': 0.7455348888350422, 'reg_alpha': 0.014783003698704587, 'reg_lambda': 0.01323797955880093}. Best is trial 2 with value: 0.589157617514999.\n",
      "Trial 5: Log Loss = 0.58822, Accuracy = 72.25719%\n",
      "[I 2024-11-27 20:37:49,571] Trial 5 finished with value: 0.5882190831667994 and parameters: {'n_estimators': 685, 'max_depth': 4, 'learning_rate': 0.14521452967640883, 'min_child_weight': 5, 'gamma': 0.01336079586824964, 'subsample': 0.9674395406195772, 'colsample_bytree': 0.8720849190732629, 'reg_alpha': 0.0009870239592648263, 'reg_lambda': 0.00013503827830873094}. Best is trial 5 with value: 0.5882190831667994.\n",
      "Trial 6: Log Loss = 0.58293, Accuracy = 72.25440%\n",
      "[I 2024-11-27 20:38:40,422] Trial 6 finished with value: 0.5829325417370838 and parameters: {'n_estimators': 313, 'max_depth': 9, 'learning_rate': 0.07321146832437425, 'min_child_weight': 3, 'gamma': 8.432004789350832e-08, 'subsample': 0.8168309876434064, 'colsample_bytree': 0.679834556038858, 'reg_alpha': 0.023208949853956707, 'reg_lambda': 0.027798989989102487}. Best is trial 6 with value: 0.5829325417370838.\n",
      "Trial 7: Log Loss = 0.59212, Accuracy = 74.03458%\n",
      "[I 2024-11-27 20:40:01,569] Trial 7 finished with value: 0.5921182782965113 and parameters: {'n_estimators': 778, 'max_depth': 6, 'learning_rate': 0.020708769610168803, 'min_child_weight': 3, 'gamma': 1.3266066004022385e-05, 'subsample': 0.8742852325095732, 'colsample_bytree': 0.8458332016310646, 'reg_alpha': 1.4490600605769925e-08, 'reg_lambda': 0.36291169116545646}. Best is trial 6 with value: 0.5829325417370838.\n",
      "Trial 8: Log Loss = 0.62655, Accuracy = 73.68017%\n",
      "[I 2024-11-27 20:40:22,357] Trial 8 finished with value: 0.6265464836328116 and parameters: {'n_estimators': 59, 'max_depth': 13, 'learning_rate': 0.011033391712552021, 'min_child_weight': 10, 'gamma': 8.113352131143843e-06, 'subsample': 0.7331935228805873, 'colsample_bytree': 0.8071103069417518, 'reg_alpha': 1.2461887092600142e-05, 'reg_lambda': 0.2606915948064592}. Best is trial 6 with value: 0.5829325417370838.\n",
      "Trial 9: Log Loss = 0.59128, Accuracy = 72.47996%\n",
      "[I 2024-11-27 20:40:42,798] Trial 9 finished with value: 0.5912844306744228 and parameters: {'n_estimators': 568, 'max_depth': 3, 'learning_rate': 0.2930619882300359, 'min_child_weight': 5, 'gamma': 0.0006733835976809869, 'subsample': 0.713159748312707, 'colsample_bytree': 0.9446261911855423, 'reg_alpha': 0.0020358853258982505, 'reg_lambda': 0.00010789102061072482}. Best is trial 6 with value: 0.5829325417370838.\n",
      "Trial 10: Log Loss = 0.58632, Accuracy = 72.62363%\n",
      "[I 2024-11-27 20:43:13,767] Trial 10 finished with value: 0.5863180139144373 and parameters: {'n_estimators': 983, 'max_depth': 11, 'learning_rate': 0.06507040493170192, 'min_child_weight': 1, 'gamma': 1.0414770812744176e-08, 'subsample': 0.8202004426181608, 'colsample_bytree': 0.6006093285061557, 'reg_alpha': 0.7546490029388501, 'reg_lambda': 0.0008475413902463942}. Best is trial 6 with value: 0.5829325417370838.\n",
      "Trial 11: Log Loss = 0.58644, Accuracy = 72.53055%\n",
      "[I 2024-11-27 20:44:53,176] Trial 11 finished with value: 0.5864394423260724 and parameters: {'n_estimators': 895, 'max_depth': 11, 'learning_rate': 0.06829329533402853, 'min_child_weight': 1, 'gamma': 1.8127673174732127e-08, 'subsample': 0.8377111243578351, 'colsample_bytree': 0.6004097120086265, 'reg_alpha': 0.5188353172041266, 'reg_lambda': 0.001581172585802277}. Best is trial 6 with value: 0.5829325417370838.\n",
      "Trial 12: Log Loss = 0.58009, Accuracy = 72.18033%\n",
      "[I 2024-11-27 20:45:28,376] Trial 12 finished with value: 0.5800894912810792 and parameters: {'n_estimators': 251, 'max_depth': 11, 'learning_rate': 0.04091958863635254, 'min_child_weight': 1, 'gamma': 1.4449817139870211e-08, 'subsample': 0.800587255770686, 'colsample_bytree': 0.6599479880522005, 'reg_alpha': 0.7123390273245238, 'reg_lambda': 0.0021170882416073913}. Best is trial 12 with value: 0.5800894912810792.\n",
      "Trial 13: Log Loss = 0.58428, Accuracy = 72.59176%\n",
      "[I 2024-11-27 20:45:57,619] Trial 13 finished with value: 0.5842752089522695 and parameters: {'n_estimators': 184, 'max_depth': 10, 'learning_rate': 0.03712111258546007, 'min_child_weight': 3, 'gamma': 3.7916697563376774e-07, 'subsample': 0.7968664842511349, 'colsample_bytree': 0.6841046877211402, 'reg_alpha': 0.040256519866548726, 'reg_lambda': 3.893058209546627e-06}. Best is trial 12 with value: 0.5800894912810792.\n",
      "Trial 14: Log Loss = 0.57836, Accuracy = 72.15992%\n",
      "[I 2024-11-27 20:46:47,839] Trial 14 finished with value: 0.5783637654006983 and parameters: {'n_estimators': 227, 'max_depth': 13, 'learning_rate': 0.039301546213719424, 'min_child_weight': 3, 'gamma': 3.0157603359733366e-07, 'subsample': 0.7825578811252158, 'colsample_bytree': 0.673875422041449, 'reg_alpha': 0.000421811963206714, 'reg_lambda': 0.00415766161276772}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 15: Log Loss = 0.57995, Accuracy = 72.34608%\n",
      "[I 2024-11-27 20:47:45,128] Trial 15 finished with value: 0.5799490954063427 and parameters: {'n_estimators': 237, 'max_depth': 13, 'learning_rate': 0.036069970513119116, 'min_child_weight': 2, 'gamma': 1.086640470111303e-06, 'subsample': 0.6018468077848805, 'colsample_bytree': 0.7436447247480367, 'reg_alpha': 0.00020724968106713607, 'reg_lambda': 0.001201858285440972}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 16: Log Loss = 0.59184, Accuracy = 72.93164%\n",
      "[I 2024-11-27 20:48:20,863] Trial 16 finished with value: 0.5918374182315977 and parameters: {'n_estimators': 71, 'max_depth': 15, 'learning_rate': 0.023002506168674018, 'min_child_weight': 3, 'gamma': 6.847285622825601e-07, 'subsample': 0.6286147765809417, 'colsample_bytree': 0.7465438153422385, 'reg_alpha': 0.00010247848365173844, 'reg_lambda': 7.974734581011337e-06}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 17: Log Loss = 0.58034, Accuracy = 72.48164%\n",
      "[I 2024-11-27 20:49:06,232] Trial 17 finished with value: 0.58033664332273 and parameters: {'n_estimators': 181, 'max_depth': 13, 'learning_rate': 0.037473949140417864, 'min_child_weight': 2, 'gamma': 0.00018969935140171454, 'subsample': 0.6023523013030206, 'colsample_bytree': 0.7943131441937403, 'reg_alpha': 1.1544331710208953e-07, 'reg_lambda': 0.004016020218365571}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 18: Log Loss = 0.58194, Accuracy = 72.69798%\n",
      "[I 2024-11-27 20:50:21,908] Trial 18 finished with value: 0.5819434222085194 and parameters: {'n_estimators': 288, 'max_depth': 13, 'learning_rate': 0.013582625249699911, 'min_child_weight': 4, 'gamma': 1.5733380814702187e-06, 'subsample': 0.6706967471797064, 'colsample_bytree': 0.7218361816053934, 'reg_alpha': 0.0006759188516698617, 'reg_lambda': 2.8323372877621892e-08}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 19: Log Loss = 0.58143, Accuracy = 72.11744%\n",
      "[I 2024-11-27 20:51:02,494] Trial 19 finished with value: 0.5814250795093965 and parameters: {'n_estimators': 163, 'max_depth': 14, 'learning_rate': 0.09736604799162422, 'min_child_weight': 7, 'gamma': 2.7846144255467607e-06, 'subsample': 0.9974280103803848, 'colsample_bytree': 0.8700345321701853, 'reg_alpha': 9.056970803754838e-05, 'reg_lambda': 0.00028655713540009565}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 20: Log Loss = 0.57880, Accuracy = 72.12443%\n",
      "[I 2024-11-27 20:52:47,919] Trial 20 finished with value: 0.578804354312254 and parameters: {'n_estimators': 513, 'max_depth': 12, 'learning_rate': 0.044391002447051374, 'min_child_weight': 2, 'gamma': 0.00011214678371438712, 'subsample': 0.7653055545097938, 'colsample_bytree': 0.6359404706268583, 'reg_alpha': 9.900548859175293e-05, 'reg_lambda': 1.852284099544264e-05}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 21: Log Loss = 0.58049, Accuracy = 72.48890%\n",
      "[I 2024-11-27 20:54:53,258] Trial 21 finished with value: 0.5804915240513008 and parameters: {'n_estimators': 497, 'max_depth': 13, 'learning_rate': 0.04555978453152613, 'min_child_weight': 2, 'gamma': 9.359306398023311e-05, 'subsample': 0.7630801352412528, 'colsample_bytree': 0.6341652802100415, 'reg_alpha': 8.174525047263722e-05, 'reg_lambda': 1.9072657486971178e-05}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 22: Log Loss = 0.57881, Accuracy = 72.21806%\n",
      "[I 2024-11-27 20:55:51,063] Trial 22 finished with value: 0.5788053242129224 and parameters: {'n_estimators': 370, 'max_depth': 12, 'learning_rate': 0.02905014732211795, 'min_child_weight': 2, 'gamma': 0.5470424191855715, 'subsample': 0.8835357988833437, 'colsample_bytree': 0.7005205467482813, 'reg_alpha': 1.8576276210457052e-06, 'reg_lambda': 1.1054079811778613e-06}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 23: Log Loss = 0.57900, Accuracy = 72.23092%\n",
      "[I 2024-11-27 20:56:51,033] Trial 23 finished with value: 0.5789994821700986 and parameters: {'n_estimators': 340, 'max_depth': 12, 'learning_rate': 0.02851207796602668, 'min_child_weight': 4, 'gamma': 0.43458933378196507, 'subsample': 0.9083652239615126, 'colsample_bytree': 0.6966269608712697, 'reg_alpha': 1.3974988753038593e-06, 'reg_lambda': 8.559079775112665e-07}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 24: Log Loss = 0.58329, Accuracy = 72.74885%\n",
      "[I 2024-11-27 20:58:00,944] Trial 24 finished with value: 0.5832928271764645 and parameters: {'n_estimators': 533, 'max_depth': 10, 'learning_rate': 0.01831147629332587, 'min_child_weight': 2, 'gamma': 0.6085755942174557, 'subsample': 0.7680818679200393, 'colsample_bytree': 0.6316335730283126, 'reg_alpha': 6.177862468750836e-07, 'reg_lambda': 6.82129907208789e-07}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 25: Log Loss = 0.57901, Accuracy = 72.24042%\n",
      "[I 2024-11-27 20:59:09,509] Trial 25 finished with value: 0.5790127744167375 and parameters: {'n_estimators': 364, 'max_depth': 12, 'learning_rate': 0.028342581007730707, 'min_child_weight': 4, 'gamma': 0.09989412352555728, 'subsample': 0.8732881858594215, 'colsample_bytree': 0.7032338692195831, 'reg_alpha': 1.6113868589384063e-06, 'reg_lambda': 1.4453474933014752e-08}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 26: Log Loss = 0.61503, Accuracy = 73.46830%\n",
      "[I 2024-11-27 21:01:10,602] Trial 26 finished with value: 0.6150313678315936 and parameters: {'n_estimators': 484, 'max_depth': 14, 'learning_rate': 0.09916181573265001, 'min_child_weight': 1, 'gamma': 0.003906169028926695, 'subsample': 0.91307956718602, 'colsample_bytree': 0.6360700406990157, 'reg_alpha': 2.224437269577954e-05, 'reg_lambda': 3.612573895255795e-05}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 27: Log Loss = 0.58043, Accuracy = 72.23846%\n",
      "[I 2024-11-27 21:02:27,036] Trial 27 finished with value: 0.5804339824537712 and parameters: {'n_estimators': 130, 'max_depth': 12, 'learning_rate': 0.05177909607338223, 'min_child_weight': 6, 'gamma': 0.0009439328458289363, 'subsample': 0.843525603106026, 'colsample_bytree': 0.6650453826729078, 'reg_alpha': 1.4373698798673922e-07, 'reg_lambda': 1.716750613996571e-07}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 28: Log Loss = 0.58376, Accuracy = 72.81621%\n",
      "[I 2024-11-27 21:06:53,590] Trial 28 finished with value: 0.5837602746605823 and parameters: {'n_estimators': 588, 'max_depth': 10, 'learning_rate': 0.016091540599206006, 'min_child_weight': 2, 'gamma': 8.436231260219836e-05, 'subsample': 0.7715967965074598, 'colsample_bytree': 0.774000319640053, 'reg_alpha': 0.0030531125898826906, 'reg_lambda': 1.7387669876353077e-06}. Best is trial 14 with value: 0.5783637654006983.\n",
      "Trial 29: Log Loss = 0.57812, Accuracy = 72.49813%\n",
      "[I 2024-11-27 21:10:56,561] Trial 29 finished with value: 0.5781220676599927 and parameters: {'n_estimators': 444, 'max_depth': 14, 'learning_rate': 0.029199798706149832, 'min_child_weight': 4, 'gamma': 0.013972167418181627, 'subsample': 0.6936468527228805, 'colsample_bytree': 0.7087418141599743, 'reg_alpha': 0.0003598248123504426, 'reg_lambda': 4.2776866888190774e-05}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 30: Log Loss = 0.64460, Accuracy = 73.73299%\n",
      "[I 2024-11-27 21:13:28,900] Trial 30 finished with value: 0.6445994374139026 and parameters: {'n_estimators': 752, 'max_depth': 15, 'learning_rate': 0.09668014704044135, 'min_child_weight': 4, 'gamma': 0.011831722670952631, 'subsample': 0.6757059627575709, 'colsample_bytree': 0.7696722056252835, 'reg_alpha': 0.0002173523874436518, 'reg_lambda': 0.00028458456708377295}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 31: Log Loss = 0.57882, Accuracy = 72.59232%\n",
      "[I 2024-11-27 21:15:28,102] Trial 31 finished with value: 0.578816698018238 and parameters: {'n_estimators': 478, 'max_depth': 14, 'learning_rate': 0.02827038063166041, 'min_child_weight': 3, 'gamma': 0.12721026338941174, 'subsample': 0.6440476315100636, 'colsample_bytree': 0.7063175149347943, 'reg_alpha': 4.08754765045876e-05, 'reg_lambda': 3.014613991309179e-05}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 32: Log Loss = 0.57969, Accuracy = 72.87127%\n",
      "[I 2024-11-27 21:17:22,322] Trial 32 finished with value: 0.579690198109233 and parameters: {'n_estimators': 430, 'max_depth': 15, 'learning_rate': 0.030411872004139213, 'min_child_weight': 3, 'gamma': 0.02246462735613565, 'subsample': 0.7509186895920524, 'colsample_bytree': 0.6269984349465877, 'reg_alpha': 0.0005356735239736905, 'reg_lambda': 1.3999254447275496e-05}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 33: Log Loss = 0.58081, Accuracy = 72.38996%\n",
      "[I 2024-11-27 21:18:42,779] Trial 33 finished with value: 0.5808125646350668 and parameters: {'n_estimators': 393, 'max_depth': 14, 'learning_rate': 0.05270739859632639, 'min_child_weight': 6, 'gamma': 0.0037491424260398826, 'subsample': 0.7019230126204764, 'colsample_bytree': 0.6718414579753611, 'reg_alpha': 2.628623769419535e-06, 'reg_lambda': 2.7887253109361393e-06}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 34: Log Loss = 0.57873, Accuracy = 72.16943%\n",
      "[I 2024-11-27 21:20:19,600] Trial 34 finished with value: 0.5787273119057189 and parameters: {'n_estimators': 442, 'max_depth': 12, 'learning_rate': 0.024429999451915346, 'min_child_weight': 4, 'gamma': 0.0007143658587388006, 'subsample': 0.7849787177271392, 'colsample_bytree': 0.7151858019207592, 'reg_alpha': 0.0045089430261782025, 'reg_lambda': 3.2512284145023435e-07}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 35: Log Loss = 0.58018, Accuracy = 72.22896%\n",
      "[I 2024-11-27 21:21:51,738] Trial 35 finished with value: 0.5801757968377029 and parameters: {'n_estimators': 454, 'max_depth': 11, 'learning_rate': 0.024875048256479872, 'min_child_weight': 4, 'gamma': 0.0004632384420657392, 'subsample': 0.6930519850486665, 'colsample_bytree': 0.7245149412237806, 'reg_alpha': 0.005927868421825237, 'reg_lambda': 2.747545136337555e-07}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 36: Log Loss = 0.59245, Accuracy = 74.22520%\n",
      "[I 2024-11-27 21:23:03,229] Trial 36 finished with value: 0.5924460654622101 and parameters: {'n_estimators': 602, 'max_depth': 7, 'learning_rate': 0.01577980267935774, 'min_child_weight': 6, 'gamma': 3.727382661128464e-05, 'subsample': 0.7891706325213189, 'colsample_bytree': 0.6522745292721857, 'reg_alpha': 0.05596575360927652, 'reg_lambda': 4.706756994513733e-08}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 37: Log Loss = 0.58067, Accuracy = 72.00284%\n",
      "[I 2024-11-27 21:24:13,022] Trial 37 finished with value: 0.5806730912433895 and parameters: {'n_estimators': 536, 'max_depth': 9, 'learning_rate': 0.04715517211224495, 'min_child_weight': 5, 'gamma': 0.0017048211607469347, 'subsample': 0.7413435024894894, 'colsample_bytree': 0.7299091288821982, 'reg_alpha': 0.0030124279868596333, 'reg_lambda': 0.016166678915064018}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 38: Log Loss = 0.58839, Accuracy = 72.76673%\n",
      "[I 2024-11-27 21:26:13,574] Trial 38 finished with value: 0.5883870861875833 and parameters: {'n_estimators': 698, 'max_depth': 14, 'learning_rate': 0.06069654442971014, 'min_child_weight': 7, 'gamma': 0.00032758881870258887, 'subsample': 0.7232731522041301, 'colsample_bytree': 0.6171535106793139, 'reg_alpha': 0.0003358024716854287, 'reg_lambda': 0.0066874988651204805}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 39: Log Loss = 0.59237, Accuracy = 74.20173%\n",
      "[I 2024-11-27 21:26:48,209] Trial 39 finished with value: 0.5923709251574746 and parameters: {'n_estimators': 284, 'max_depth': 8, 'learning_rate': 0.02002925856374216, 'min_child_weight': 4, 'gamma': 3.361152970164297e-05, 'subsample': 0.7844328185874342, 'colsample_bytree': 0.7693993810142163, 'reg_alpha': 0.0014370906424554875, 'reg_lambda': 5.3202051994310567e-05}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 40: Log Loss = 0.58292, Accuracy = 72.08837%\n",
      "[I 2024-11-27 21:28:04,944] Trial 40 finished with value: 0.5829178292680858 and parameters: {'n_estimators': 437, 'max_depth': 12, 'learning_rate': 0.08056333832066478, 'min_child_weight': 5, 'gamma': 0.009879117109435375, 'subsample': 0.8203999636788639, 'colsample_bytree': 0.9741199842798299, 'reg_alpha': 0.008125787850081825, 'reg_lambda': 0.053546125928287724}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 41: Log Loss = 0.57842, Accuracy = 72.15294%\n",
      "[I 2024-11-27 21:29:36,340] Trial 41 finished with value: 0.57842339563773 and parameters: {'n_estimators': 384, 'max_depth': 12, 'learning_rate': 0.03499480561739134, 'min_child_weight': 3, 'gamma': 0.04748071885535444, 'subsample': 0.8556625187222452, 'colsample_bytree': 0.6870454435744907, 'reg_alpha': 4.7756946180313856e-05, 'reg_lambda': 5.871600086066106e-07}. Best is trial 29 with value: 0.5781220676599927.\n",
      "Trial 42: Log Loss = 0.57799, Accuracy = 72.21722%\n",
      "[I 2024-11-27 21:30:55,022] Trial 42 finished with value: 0.5779923247550582 and parameters: {'n_estimators': 332, 'max_depth': 13, 'learning_rate': 0.0329014414333458, 'min_child_weight': 3, 'gamma': 0.024318270664498532, 'subsample': 0.8478652099231178, 'colsample_bytree': 0.6737054254112979, 'reg_alpha': 3.090492668111583e-05, 'reg_lambda': 6.748516964647809e-06}. Best is trial 42 with value: 0.5779923247550582.\n",
      "Trial 43: Log Loss = 0.57823, Accuracy = 72.19682%\n",
      "[I 2024-11-27 21:32:04,466] Trial 43 finished with value: 0.5782329792206619 and parameters: {'n_estimators': 312, 'max_depth': 13, 'learning_rate': 0.033056965416389826, 'min_child_weight': 3, 'gamma': 0.042709319300738985, 'subsample': 0.8594890405731384, 'colsample_bytree': 0.6764728832913339, 'reg_alpha': 7.280002993450569e-06, 'reg_lambda': 8.873202589954335e-08}. Best is trial 42 with value: 0.5779923247550582.\n",
      "Trial 44: Log Loss = 0.57845, Accuracy = 72.27620%\n",
      "[I 2024-11-27 21:33:10,098] Trial 44 finished with value: 0.5784478337344071 and parameters: {'n_estimators': 235, 'max_depth': 13, 'learning_rate': 0.037203701395787804, 'min_child_weight': 3, 'gamma': 0.03851203489982358, 'subsample': 0.8521392030711777, 'colsample_bytree': 0.6847784304829606, 'reg_alpha': 9.894477416506958e-06, 'reg_lambda': 6.259164290234499e-08}. Best is trial 42 with value: 0.5779923247550582.\n",
      "Trial 45: Log Loss = 0.57911, Accuracy = 72.81257%\n",
      "[I 2024-11-27 21:34:59,498] Trial 45 finished with value: 0.5791083889821208 and parameters: {'n_estimators': 323, 'max_depth': 15, 'learning_rate': 0.033551523335381785, 'min_child_weight': 3, 'gamma': 0.18575582416626732, 'subsample': 0.9269498861859303, 'colsample_bytree': 0.6514212680396934, 'reg_alpha': 4.887560326854557e-06, 'reg_lambda': 1.0883456731908904e-07}. Best is trial 42 with value: 0.5779923247550582.\n",
      "Best Hyperparameters:\n",
      "  n_estimators: 332\n",
      "  max_depth: 13\n",
      "  learning_rate: 0.0329014414333458\n",
      "  min_child_weight: 3\n",
      "  gamma: 0.024318270664498532\n",
      "  subsample: 0.8478652099231178\n",
      "  colsample_bytree: 0.6737054254112979\n",
      "  reg_alpha: 3.090492668111583e-05\n",
      "  reg_lambda: 6.748516964647809e-06\n",
      "Final Model Overall Accuracy: 0.7224\n",
      "Final Model Upset Accuracy: 0.6489\n",
      "Final Model Non-Upset Accuracy: 0.7475\n"
     ]
    }
   ],
   "source": [
    "# Function to optimize hyperparameters using Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters for XGBoost\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    \n",
    "    upset_weight = 3\n",
    "    \n",
    "    # Create sample weights for training data\n",
    "    sample_weight = np.ones(len(y_train))\n",
    "    sample_weight[train_data['upset'] == 1] = upset_weight\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    \n",
    "    # Predict probabilities on test data\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Create sample weights for test data\n",
    "    sample_weight_test = np.ones(len(y_test))\n",
    "    sample_weight_test[test_data['upset'] == 1] = upset_weight\n",
    "    \n",
    "    # Calculate log loss\n",
    "    loss = log_loss(y_test, y_pred_proba, sample_weight=sample_weight_test)\n",
    "    \n",
    "    # Optionally, print accuracy for monitoring\n",
    "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"Trial {trial.number}: Log Loss = {loss:.5f}, Accuracy = {accuracy:.5%}\")\n",
    "    \n",
    "    return loss  # Optuna will minimize this metric\n",
    "\n",
    "# Create a study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=300, timeout=3600, show_progress_bar=True)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "# best_upset_weight = best_params.pop('upset_weight')\n",
    "# print(f\"Best Upset Weight: {best_upset_weight:.4f}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Train final model with the best hyperparameters and upset weight\n",
    "sample_weight = np.ones(len(y_train))\n",
    "sample_weight[train_data['upset'] == 1] = 3\n",
    "\n",
    "final_model = xgb.XGBClassifier(**best_params, eval_metric='logloss')\n",
    "final_model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_final = accuracy_score(\n",
    "    y_test[upset_mask], y_pred_final[upset_mask]\n",
    ")\n",
    "accuracy_non_upsets_final = accuracy_score(\n",
    "    y_test[~upset_mask], y_pred_final[~upset_mask]\n",
    ")\n",
    "\n",
    "print(f\"Final Model Overall Accuracy: {overall_accuracy_final:.4f}\")\n",
    "print(f\"Final Model Upset Accuracy: {accuracy_upsets_final:.4f}\")\n",
    "print(f\"Final Model Non-Upset Accuracy: {accuracy_non_upsets_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Overall Accuracy: 0.7618\n",
      "Final Model Upset Accuracy: 0.5089\n",
      "Final Model Non-Upset Accuracy: 0.8481\n"
     ]
    }
   ],
   "source": [
    "# Train final model with the best hyperparameters and upset weight\n",
    "sample_weight = np.ones(len(y_train))\n",
    "sample_weight[train_data['upset'] == 1] = 2\n",
    "\n",
    "final_model = xgb.XGBClassifier(**best_params, eval_metric='logloss')\n",
    "final_model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_final = accuracy_score(\n",
    "    y_test[upset_mask], y_pred_final[upset_mask]\n",
    ")\n",
    "accuracy_non_upsets_final = accuracy_score(\n",
    "    y_test[~upset_mask], y_pred_final[~upset_mask]\n",
    ")\n",
    "\n",
    "print(f\"Final Model Overall Accuracy: {overall_accuracy_final:.4f}\")\n",
    "print(f\"Final Model Upset Accuracy: {accuracy_upsets_final:.4f}\")\n",
    "print(f\"Final Model Non-Upset Accuracy: {accuracy_non_upsets_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Overall Accuracy: 0.7738\n",
      "Final Model Upset Accuracy: 0.4226\n",
      "Final Model Non-Upset Accuracy: 0.8937\n"
     ]
    }
   ],
   "source": [
    "# Train final model with the best hyperparameters and upset weight\n",
    "sample_weight = np.ones(len(y_train))\n",
    "sample_weight[train_data['upset'] == 1] = 1.5\n",
    "\n",
    "final_model = xgb.XGBClassifier(**best_params, eval_metric='logloss')\n",
    "final_model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_final = accuracy_score(\n",
    "    y_test[upset_mask], y_pred_final[upset_mask]\n",
    ")\n",
    "accuracy_non_upsets_final = accuracy_score(\n",
    "    y_test[~upset_mask], y_pred_final[~upset_mask]\n",
    ")\n",
    "\n",
    "print(f\"Final Model Overall Accuracy: {overall_accuracy_final:.4f}\")\n",
    "print(f\"Final Model Upset Accuracy: {accuracy_upsets_final:.4f}\")\n",
    "print(f\"Final Model Non-Upset Accuracy: {accuracy_non_upsets_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Overall Accuracy: 0.7783\n",
      "Final Model Upset Accuracy: 0.3237\n",
      "Final Model Non-Upset Accuracy: 0.9336\n"
     ]
    }
   ],
   "source": [
    "best = {'n_estimators': 332,\n",
    "  'max_depth': 13,\n",
    "  'learning_rate': 0.0329014414333458,\n",
    "  'min_child_weight': 3,\n",
    "  'gamma': 0.024318270664498532,\n",
    "  'subsample': 0.8478652099231178,\n",
    "  'colsample_bytree': 0.6737054254112979,\n",
    "  'reg_alpha': 3.090492668111583e-05,\n",
    "  'reg_lambda': 6.748516964647809e-06,\n",
    "  'tree_method': 'hist'}\n",
    "\n",
    "# Train final model with the best hyperparameters and upset weight\n",
    "sample_weight = np.ones(len(y_train))\n",
    "sample_weight[train_data['upset'] == 1] = 1\n",
    "\n",
    "final_model = xgb.XGBClassifier(**best, eval_metric='logloss')\n",
    "final_model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_final = accuracy_score(\n",
    "    y_test[upset_mask], y_pred_final[upset_mask]\n",
    ")\n",
    "accuracy_non_upsets_final = accuracy_score(\n",
    "    y_test[~upset_mask], y_pred_final[~upset_mask]\n",
    ")\n",
    "\n",
    "print(f\"Final Model Overall Accuracy: {overall_accuracy_final:.4f}\")\n",
    "print(f\"Final Model Upset Accuracy: {accuracy_upsets_final:.4f}\")\n",
    "print(f\"Final Model Non-Upset Accuracy: {accuracy_non_upsets_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "Final Model Overall Accuracy: 0.7750\n",
      "Final Model Upset Accuracy: 0.3116\n",
      "Final Model Non-Upset Accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters:\")\n",
    "best = {'n_estimators': 864,\n",
    "        'max_depth': 3,\n",
    "        'learning_rate': 0.01610478829322539,\n",
    "        'min_child_weight': 8,\n",
    "        'gamma': 5.003832819230665e-08,\n",
    "        'subsample': 0.6645683689341554,\n",
    "        'colsample_bytree': 0.6581693927090808,\n",
    "        'reg_alpha': 9.956839465089495e-06,\n",
    "        'reg_lambda': 0.0365646384855557,\n",
    "        'tree_method': 'hist'}\n",
    "\n",
    "# Train final model with the best hyperparameters and upset weight\n",
    "sample_weight = np.ones(len(y_train))\n",
    "sample_weight[train_data['upset'] == 1] = 1\n",
    "\n",
    "final_model = xgb.XGBClassifier(**best, eval_metric='logloss')\n",
    "final_model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_final = accuracy_score(\n",
    "    y_test[upset_mask], y_pred_final[upset_mask]\n",
    ")\n",
    "accuracy_non_upsets_final = accuracy_score(\n",
    "    y_test[~upset_mask], y_pred_final[~upset_mask]\n",
    ")\n",
    "\n",
    "print(f\"Final Model Overall Accuracy: {overall_accuracy_final:.4f}\")\n",
    "print(f\"Final Model Upset Accuracy: {accuracy_upsets_final:.4f}\")\n",
    "print(f\"Final Model Non-Upset Accuracy: {accuracy_non_upsets_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-27 20:10:43,653] A new study created in memory with name: no-name-18223380-6276-4a0f-b3e5-b2d1020bd4cc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9b09de5071498fa553a9e23b3d46d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-11-27 20:12:07,126] Trial 0 finished with value: 0.7280893977169818 and parameters: {'n_estimators': 299, 'max_depth': 14, 'learning_rate': 0.01729852492241346, 'min_child_weight': 9, 'gamma': 1.4529302114839138e-05, 'subsample': 0.6149531125392811, 'colsample_bytree': 0.7779283234663867, 'reg_alpha': 0.0006729737342550955, 'reg_lambda': 7.369018959922341e-06, 'upset_weight': 2.9296157385156523}. Best is trial 0 with value: 0.7280893977169818.\n",
      "[W 2024-11-27 20:12:29,641] Trial 1 failed with parameters: {'n_estimators': 975, 'max_depth': 6, 'learning_rate': 0.14536026601462412, 'min_child_weight': 1, 'gamma': 0.0003386691737871086, 'subsample': 0.9687918033792976, 'colsample_bytree': 0.8355601056224924, 'reg_alpha': 3.174130591698756e-06, 'reg_lambda': 3.338061874586353e-05, 'upset_weight': 2.6317921749361326} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_368669/2402501208.py\", line 29, in objective\n",
      "    model.fit(X_train, y_train, sample_weight=sample_weight)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1531, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 2101, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2024-11-27 20:12:29,642] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Create a study and optimize\u001b[39;00m\n\u001b[1;32m     40\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Best hyperparameters\u001b[39;00m\n\u001b[1;32m     44\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[112], line 29\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Predict on test data\u001b[39;00m\n\u001b[1;32m     32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to optimize hyperparameters using Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters for XGBoost\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    \n",
    "    # Suggest a weight for upsets between 0.8 and 1.2\n",
    "    upset_weight = trial.suggest_float('upset_weight', 2.5, 3.5)\n",
    "    \n",
    "    # Create sample weights\n",
    "    sample_weight = np.ones(len(y_train))\n",
    "    sample_weight[train_data['upset'] == 1] = upset_weight\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    loss = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return loss  # Optuna will maximize this metric\n",
    "\n",
    "# Create a study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=300, timeout=3600*2, show_progress_bar=True)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_upset_weight = best_params.pop('upset_weight')\n",
    "print(f\"Best Upset Weight: {best_upset_weight:.4f}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Train final model with the best hyperparameters and upset weight\n",
    "sample_weight = np.ones(len(y_train))\n",
    "sample_weight[train_data['upset'] == 1] = best_upset_weight\n",
    "\n",
    "final_model = xgb.XGBClassifier(**best_params, eval_metric='logloss')\n",
    "final_model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "\n",
    "# Compute accuracies for upsets and non-upsets\n",
    "accuracy_upsets_final = accuracy_score(\n",
    "    y_test[upset_mask], y_pred_final[upset_mask]\n",
    ")\n",
    "accuracy_non_upsets_final = accuracy_score(\n",
    "    y_test[~upset_mask], y_pred_final[~upset_mask]\n",
    ")\n",
    "\n",
    "print(f\"Final Model Overall Accuracy: {overall_accuracy_final:.4f}\")\n",
    "print(f\"Final Model Upset Accuracy: {accuracy_upsets_final:.4f}\")\n",
    "print(f\"Final Model Non-Upset Accuracy: {accuracy_non_upsets_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
