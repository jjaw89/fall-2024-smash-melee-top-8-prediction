{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "#import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from glicko2 import Player\n",
    "import multiprocessing\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "dataset_df = pd.read_pickle(data_path + 'dataset_full.pkl')\n",
    "dataset_df.sort_index(inplace=True) # For convenience, mostly. Not really necessary, we use .loc[] anyways\n",
    "\n",
    "# Note that there is no real reason to keep separately computing the individual probabilities of winning each individual set.\n",
    "# Let's just compute them all at once here.\n",
    "\n",
    "single_set_model = None\n",
    "with open(data_path + 'single_set_model.pkl', 'rb') as f:\n",
    "    single_set_model = pickle.load(f)\n",
    "\n",
    "# Make sure these match up with what features the model was trained on.\n",
    "features_all_everything = ['p1_default_elo', 'p2_default_elo', 'p1_default_rd', 'p2_default_rd',\n",
    "       'p1_default_updates', 'p2_default_updates', 'matchup_1', 'matchup_2',\n",
    "       'matchup_3', 'matchup_4', 'matchup_5', 'matchup_6', 'matchup_7',\n",
    "       'matchup_8', 'matchup_9', 'matchup_10', 'p1_m1_usage', 'p2_m1_usage',\n",
    "       'p1/m1/m1_alt2_elo', 'p1/m1/m1_alt2_rd', 'p1/m1/m1_alt2_updates',\n",
    "       'p2/m1/m1_alt2_elo', 'p2/m1/m1_alt2_rd', 'p2/m1/m1_alt2_updates',\n",
    "       'p1/m1_alt3_elo', 'p1/m1_alt3_rd', 'p1/m1_alt3_updates',\n",
    "       'p2/m1_alt3_elo', 'p2/m1_alt3_rd', 'p2/m1_alt3_updates']\n",
    "\n",
    "dataset_df['p1_win_prob'] = single_set_model.predict_proba(dataset_df[features_all_everything])[:,1]\n",
    "\n",
    "# As a sanity check, let's verify the accuracy and log loss on 2024 data\n",
    "# Total accuracy and log loss (including lots of data that the model was trained on)\n",
    "date_filter = (dataset_df['start'] >= datetime.datetime(2024,1,1)) & (dataset_df['end'] <= datetime.datetime(2024,12,31))\n",
    "print(\"2024 single-set performance metrics\")\n",
    "print(\"Log loss: \", round(log_loss(dataset_df[date_filter]['winner'], dataset_df[date_filter]['p1_win_prob']), 3))\n",
    "print(\"Accuracy: \", round(100.0 * accuracy_score(dataset_df[date_filter]['winner'], dataset_df[date_filter]['p1_win_prob'] >= 0.5), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_df = pd.read_pickle(data_path + 'top_8_tournament_previous_sets_and_results_with_winners_df')\n",
    "\n",
    "# Filter down to tournaments which actually have valid top 8 data, and previous data on getting there.\n",
    "tournament_df = tournament_df.loc[tournament_df[['LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2',\n",
    "                                                 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2',\n",
    "                                                 'LN_A_p1_non_top_8_sets', 'LN_A_p2_non_top_8_sets',\n",
    "                                                 'LN_B_p1_non_top_8_sets', 'LN_B_p2_non_top_8_sets',\n",
    "                                                 'WSF_A_p1_non_top_8_sets', 'WSF_A_p2_non_top_8_sets',\n",
    "                                                 'WSF_B_p1_non_top_8_sets', 'WSF_B_p2_non_top_8_sets']].dropna().index]\n",
    "\n",
    "\n",
    "# Very rarely (not sure where the problem is) you get something not actually in the single-set dataframe\n",
    "# It is not actually that common though, so let's just delete those instances.\n",
    "def references_valid_sets(prev_sets):\n",
    "    for x in prev_sets:\n",
    "        if x[0] not in dataset_df.index:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "filter = tournament_df[['LN_A_p1_non_top_8_sets', 'LN_A_p2_non_top_8_sets',\n",
    "                        'LN_B_p1_non_top_8_sets', 'LN_B_p2_non_top_8_sets',\n",
    "                        'WSF_A_p1_non_top_8_sets', 'WSF_A_p2_non_top_8_sets',\n",
    "                        'WSF_B_p1_non_top_8_sets', 'WSF_B_p2_non_top_8_sets']].map(references_valid_sets).all(axis=1)\n",
    "\n",
    "tournament_df = tournament_df[filter]\n",
    "\n",
    "# Likewise, some of these sets don't seem to have a valid winner\n",
    "tournament_df = tournament_df[~tournament_df['winner_id'].isna()]\n",
    "\n",
    "# A bit more cleanup, for sanity\n",
    "min_date = datetime.datetime(2015,1,1)\n",
    "max_date = datetime.datetime(2024,12,31)\n",
    "\n",
    "tournament_df = tournament_df[(tournament_df['start'] >= min_date) &\n",
    "                              (tournament_df['end'] >= min_date) &\n",
    "                              (tournament_df['start'] <= max_date) &\n",
    "                              (tournament_df['end'] <= max_date)]\n",
    "\n",
    "tournament_df.sort_values(by=['end', 'start'], inplace=True)\n",
    "\n",
    "tournament_df[['winner_id', 'LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2', 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a score based on how likely it was that they actually made it to the top.\n",
    "# A sort of sum of negative log probabilities, where wins are positive and losses are negative\n",
    "def prev_set_score(prev_sets):\n",
    "    result = 0\n",
    "\n",
    "    for x in prev_sets:\n",
    "        data = dataset_df.loc[x[0], ['winner', 'p1_win_prob']]\n",
    "        outcome = x[1]\n",
    "\n",
    "        # We don't know if this player is p1 or p2 in this list, but this can determine it without looking at player id\n",
    "        # Compare if (player we are interested in wins) vs (did p1 win)\n",
    "        if outcome == (data['winner'] == 1.0): # The player is p1\n",
    "            if outcome: # player wins, as p1\n",
    "                result += (-np.log(data['p1_win_prob']))\n",
    "            else:       # player loses, as p1\n",
    "                result -= (-np.log(1-data['p1_win_prob']))\n",
    "        else:                                  # The player is p2\n",
    "            if outcome: # player wins, as p2\n",
    "                result += (-np.log(1-data['p1_win_prob']))\n",
    "            else:       # player loses, as p2\n",
    "                result -= (-np.log(data['p1_win_prob']))\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example data (note that there is a consistent ELO throughout this entire dataset)\n",
    "prev_sets = tournament_df.iloc[10010]['LN_A_p1_non_top_8_sets']\n",
    "print(prev_sets)\n",
    "print()\n",
    "print(dataset_df.loc[[x[0] for x in prev_sets], ['p1_default_elo', 'p2_default_elo', 'winner', 'p1_win_prob']])\n",
    "print()\n",
    "print(\"Previous set score: \", prev_set_score(tournament_df.iloc[10010]['LN_A_p1_non_top_8_sets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_8_pos = ['LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2', 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2']\n",
    "top_8_prevs = [x + '_non_top_8_sets' for x in top_8_pos]\n",
    "top_8_prevs_lengths = [x + \"_len\" for x in top_8_prevs] # These columns will just keep track of how many sets the player went through to get to the top 8\n",
    "top_8_prevs_scores = [x + \"_score\" for x in top_8_prevs] # These will keep track of their \"score\" that shows how \"well\" they are performing relative to their predicted odds.\n",
    "\n",
    "tournament_df[top_8_prevs_lengths] = tournament_df[top_8_prevs].map(lambda x: len(x)).to_numpy()\n",
    "tournament_df[top_8_prevs_scores] = tournament_df[top_8_prevs].map(prev_set_score).to_numpy()\n",
    "tournament_df[top_8_prevs + top_8_prevs_lengths + top_8_prevs_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to have all of the features we've engineered for each of the players that made it to the top 8\n",
    "# We can be clever and pull most of them (elo-based features) from the previous sets in the dataframe\n",
    "# The rest (player vs player stats, also called \"matchup\", but might be renamed) have to be pulled in manually\n",
    "\n",
    "features_elo = ['p1_default_elo', 'p2_default_elo', 'p1_default_rd', 'p2_default_rd',\n",
    "       'p1_default_updates', 'p2_default_updates', 'p1_m1_usage', 'p2_m1_usage',\n",
    "       'p1/m1/m1_alt2_elo', 'p1/m1/m1_alt2_rd', 'p1/m1/m1_alt2_updates',\n",
    "       'p2/m1/m1_alt2_elo', 'p2/m1/m1_alt2_rd', 'p2/m1/m1_alt2_updates',\n",
    "       'p1/m1_alt3_elo', 'p1/m1_alt3_rd', 'p1/m1_alt3_updates',\n",
    "       'p2/m1_alt3_elo', 'p2/m1_alt3_rd', 'p2/m1_alt3_updates']\n",
    "\n",
    "features_matchup = ['matchup_1', 'matchup_2', 'matchup_3', 'matchup_4', 'matchup_5',\n",
    "                    'matchup_6', 'matchup_7', 'matchup_8', 'matchup_9', 'matchup_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to avoid an organizational nightmare, each of the following:\n",
    "#     'LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2', 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2'\n",
    "# will have their data stored in separate dataframes, each held in the following dictionary\n",
    "\n",
    "top_8_stats = {}\n",
    "\n",
    "def pull_data_from_set(loc, outcome):\n",
    "    set_data = dataset_df.loc[loc]\n",
    "    player_num = 'p1' if outcome == (set_data['winner'] == 1.0) else 'p2' # Sneaky way of getting the player number\n",
    "\n",
    "    features_to_pull = [x for x in features_elo if player_num in x]\n",
    "    pulled_data = set_data[features_to_pull].copy()\n",
    "    pulled_data.index = [x.replace(player_num, '') for x in pulled_data.index] # We will add player numbers on an as-needed basis later\n",
    "\n",
    "    return pulled_data\n",
    "\n",
    "for top_8_position in tqdm(top_8_pos):\n",
    "    # First, pull in player 1 data from a previous match.\n",
    "    # Note that the player might NOT be player 1 in the match that we are pulling from\n",
    "    top_8_stats[top_8_position] = tournament_df[top_8_position + '_non_top_8_sets'].apply(lambda x: pull_data_from_set(x[0][0], x[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_path_prob(row):\n",
    "    # First, compute the probabilities of any player in this top 8 winning a set against any other player.\n",
    "    # Row represents p1, column represents p2 (or specifically, the index in top_8_pos)\n",
    "    pairwise_probs = np.zeros(shape=(8,8))\n",
    "    \n",
    "    # For convenience, put everything into one dataframe and then run the single set model\n",
    "    # This is probably more efficient than doing things line by line\n",
    "    combination_stats = []\n",
    "\n",
    "    for r in range(0,8):\n",
    "        for c in range(0,8):\n",
    "            p1_data = top_8_stats[top_8_pos[r]].loc[row.name]\n",
    "            p1_data.index = ['p1' + x for x in p1_data.index]\n",
    "\n",
    "            p2_data = top_8_stats[top_8_pos[c]].loc[row.name]\n",
    "            p2_data.index = ['p2' + x for x in p2_data.index]\n",
    "\n",
    "            #TODO: Actually populate this with proper data!\n",
    "            #      This is currently only just placeholder data,\n",
    "            #      indicating that the players have never played together before (0.5).\n",
    "            matchup_data = pd.Series(0.5, index=['matchup_' + str(n) for n in range(1,10+1)])\n",
    "\n",
    "            total_data = pd.concat([p1_data, p2_data, matchup_data])\n",
    "            total_data = total_data[features_all_everything] # Entries need to be in the correct order\n",
    "\n",
    "            combination_stats.append(total_data)\n",
    "\n",
    "    combination_stats = pd.DataFrame(combination_stats)\n",
    "\n",
    "    y_prob = single_set_model.predict_proba(combination_stats)\n",
    "\n",
    "    # Now actually populate this probability matrix with data.\n",
    "    # Note that we can just use the same nested loop and read off the entries of the 1D probability array one at a time.\n",
    "    # This will put things in the correct order.\n",
    "    i = 0\n",
    "    for r in range(0,8):\n",
    "        for c in range(0,8):\n",
    "            pairwise_probs[r,c] = y_prob[i,1]\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    # NOTE: Some models that we train are highly non-symmetric, even though they very much should be,\n",
    "    #       given that we have randomized the players. We can fix that issue here.\n",
    "    pairwise_probs = 0.5 * (pairwise_probs + (1 - pairwise_probs.T))\n",
    "    pairwise_probs_zero_diagonal = pairwise_probs - 0.5 * np.identity(8) # Used for janky computations\n",
    "\n",
    "    # Now start building the tree structure of how the tournament can play out.\n",
    "    # Each \"cell\" will represent some set in the tournament played by some p1 and p2.\n",
    "    # The cell will have to keep track of all of the probabilities of each player making it to that point.\n",
    "    #\n",
    "    # Links should have the form (cell, 'winner') or (cell, 'loser'),\n",
    "    # describing if it is the winner or the loser of the previous that gets to this one\n",
    "    class cell:\n",
    "        def __init__(self, p1=None, p2=None, p1_link=None, p2_link=None):\n",
    "            if p1==None:\n",
    "                self.p1_probs = None\n",
    "            else:\n",
    "                self.p1_probs = np.zeros(8)\n",
    "                self.p1_probs[p1] = 1.0\n",
    "\n",
    "            if p2==None:\n",
    "                self.p2_probs = None\n",
    "            else:\n",
    "                self.p2_probs = np.zeros(8)\n",
    "                self.p2_probs[p2] = 1.0\n",
    "\n",
    "            # Links to previous cells\n",
    "            self.p1_link = p1_link\n",
    "            self.p2_link = p2_link\n",
    "\n",
    "            # Used for a (hopefully) temporary patch on the fact that these computations are not entirely accurate\n",
    "            self.pairwise_probs_zero_diagonal = pairwise_probs - 0.5 * np.identity(8)\n",
    "\n",
    "        # Get the probabilities from the previous cell.\n",
    "        # Should not be called if there are no links to previous cells.\n",
    "        def fetch_probs(self):\n",
    "            self.p1_probs = self.p1_link[0].compute_winner_probs() if self.p1_link[1] == 'winner' else self.p1_link[0].compute_loser_probs()\n",
    "            self.p2_probs = self.p2_link[0].compute_winner_probs() if self.p2_link[1] == 'winner' else self.p2_link[0].compute_loser_probs()\n",
    "        \n",
    "        # Probability of making it to this cell, and then proceeding to win\n",
    "        def compute_winner_probs(self):\n",
    "            if self.p1_probs is None or self.p2_probs is None:\n",
    "                self.fetch_probs()\n",
    "\n",
    "            probs = np.zeros(8)\n",
    "\n",
    "            # Old code, far less efficient. Might make the numpy operations make sense though.\n",
    "            '''\n",
    "            for p1 in range(0,8):\n",
    "                # Save a result for p1.\n",
    "                # It will be the sum over all p2 of\n",
    "                # (probability that p1 got there) * (probability that p2 got there) * (probability p1 beats p2)\n",
    "                for p2 in range(0,8):\n",
    "                    probs[p1] += self.p1_probs[p1] * self.p2_probs[p2] * pairwise_probs[p1, p2]\n",
    "                    probs[p2] += self.p1_probs[p1] * self.p2_probs[p2] * (1.0 - pairwise_probs[p1, p2])\n",
    "            '''\n",
    "            # Just remember that 1-pairwise_probs is the transpose of pairwise_probs, by symmetry\n",
    "            #\n",
    "            # TODO: I just realized that the probability of a certain player becoming p1 and another becoming p2 are NOT independent.\n",
    "            #       In particular, these probabilities become correlated when you could potentially have the same player as p1 or p2.\n",
    "            #       This is a bit of a janky patch that hopefully gives accurate enough probabilities, but we should come up with a proper fix.       \n",
    "            probs += self.p1_probs * (pairwise_probs_zero_diagonal @ self.p2_probs) # Probability that (specific p1) wins\n",
    "            probs += self.p2_probs * (pairwise_probs_zero_diagonal @ self.p1_probs) # Same but p2\n",
    "\n",
    "            probs /= probs.sum() # Purely due to zeroing out the diagonal of pairwise_probs\n",
    "\n",
    "            return probs\n",
    "\n",
    "        # Probability of making it to this cell, and then proceeding to lose\n",
    "        def compute_loser_probs(self):\n",
    "            if self.p1_probs is None or self.p2_probs is None:\n",
    "                self.fetch_probs()\n",
    "\n",
    "            probs = np.zeros(8)\n",
    "\n",
    "            '''\n",
    "            for p1 in range(0,8):\n",
    "                # Same, except use probability of p1 losing\n",
    "                for p2 in range(0,8):\n",
    "                    probs[p1] += self.p1_probs[p1] * self.p2_probs[p2] * (1.0 - pairwise_probs[p1, p2])\n",
    "                    probs[p2] += self.p1_probs[p1] * self.p2_probs[p2] * pairwise_probs[p1, p2]\n",
    "            '''\n",
    "            # TODO: Same janky patch as in winners case here.\n",
    "            probs += self.p1_probs * (pairwise_probs_zero_diagonal.T @ self.p2_probs)\n",
    "            probs += self.p2_probs * (pairwise_probs_zero_diagonal.T @ self.p1_probs)\n",
    "\n",
    "            probs /= probs.sum()\n",
    "             \n",
    "            return probs\n",
    "        \n",
    "    # 'LN_A_p1', 'LN_A_p2', 'LN_B_p1', 'LN_B_p2', 'WSF_A_p1', 'WSF_A_p2', 'WSF_B_p1', 'WSF_B_p2'\n",
    "    WSFA = cell(p1=4, p2=5)\n",
    "    WSFB = cell(p1=6, p2=7)\n",
    "    LNA  = cell(p1=0, p2=1)\n",
    "    LNB  = cell(p1=2, p2=3)\n",
    "\n",
    "    WF = cell(p1_link=(WSFA, 'winner'), p2_link=(WSFB, 'winner'))\n",
    "\n",
    "    LQFA = cell(p1_link=(WSFA, 'loser'), p2_link=(LNA, 'winner'))\n",
    "    LQFB = cell(p1_link=(WSFB, 'loser'), p2_link=(LNB, 'winner'))\n",
    "\n",
    "    LSF = cell(p1_link=(LQFA, 'winner'), p2_link=(LQFB, 'winner'))\n",
    "\n",
    "    LF = cell(p1_link=(WF, 'loser'), p2_link=(LSF, 'winner'))\n",
    "\n",
    "    GF = cell(p1_link=(WF, 'winner'), p2_link=(LF, 'winner'))\n",
    "\n",
    "    # From the Grand Final onwards, some special cases are required, due to how the Grand Final Reset works\n",
    "    GF.fetch_probs()\n",
    "\n",
    "    # TODO: Again, same janky fix as before, \"removing\" correlation between p1 and p2\n",
    "    win_as_p1_probs = GF.p1_probs * (pairwise_probs_zero_diagonal @ GF.p2_probs) # direct win as p1 (WF winner)\n",
    "    win_as_p1_probs += GF.p1_probs * ((pairwise_probs_zero_diagonal.T * pairwise_probs_zero_diagonal) @ GF.p2_probs) # p2 win, then p1 win in GFR\n",
    "\n",
    "    win_as_p2_probs = GF.p2_probs * ((pairwise_probs_zero_diagonal ** 2) @ GF.p1_probs) # win by 2 required for LF winner\n",
    "\n",
    "    probs = win_as_p1_probs + win_as_p2_probs\n",
    "    probs /= probs.sum() # Again due to that janky fix\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "n = 10000\n",
    "result = tournament_df.iloc[n:n+10].apply(compute_path_prob, axis=1)\n",
    "print(pd.DataFrame(np.stack(result.to_numpy()),\n",
    "      index=result.index, columns=[x + '_winprob' for x in top_8_pos]))\n",
    "#top_8_stats[top_8_pos[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_df['winner_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline of \"who has the higher elo\"\n",
    "\n",
    "test_df = tournament_df[tournament_df['start'] >= datetime.datetime(2024,1,1)].copy()\n",
    "print(test_df.shape)\n",
    "\n",
    "def pull_elo_from_set(loc, outcome):\n",
    "    set_data = dataset_df.loc[loc]\n",
    "    player_num = 'p1' if outcome == (set_data['winner'] == 1.0) else 'p2' # Sneaky way of getting the player number\n",
    "\n",
    "    feature_to_pull = player_num + '_default_elo'\n",
    "    pulled_data = set_data[feature_to_pull]\n",
    "\n",
    "    return pulled_data\n",
    "\n",
    "# First, pull in player 1 data from a previous match.\n",
    "# Note that the player might NOT be player 1 in the match that we are pulling from\n",
    "test_df[[x + '_elo' for x in top_8_pos]] = test_df[[x + '_non_top_8_sets' for x in top_8_pos]].map(lambda x: pull_elo_from_set(x[0][0], x[0][1])).to_numpy()\n",
    "\n",
    "test_df['elo_prediction'] = test_df[[x + '_elo' for x in top_8_pos]].idxmax(axis=1).apply(lambda x: x.replace('_elo', ''))\n",
    "test_df['elo_prediction'] = test_df.apply(lambda row: row[row['elo_prediction']], axis=1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor of \"who has the highest path probability, taking into account all possible paths on how the top 8 will play out\"\n",
    "\n",
    "result = test_df.apply(compute_path_prob, axis=1)\n",
    "test_df = pd.concat([test_df, pd.DataFrame(np.stack(result.to_numpy()), index=result.index, columns=[x + '_winprob' for x in top_8_pos])], axis=1)\n",
    "\n",
    "test_df['path_prediction'] = test_df[[x + '_winprob' for x in top_8_pos]].idxmax(axis=1).apply(lambda x: x.replace('_winprob', ''))\n",
    "test_df['path_prediction'] = test_df.apply(lambda row: row[row['path_prediction']], axis=1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['winner_id', 'elo_prediction', 'path_prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ELO-only accuracy: \", round(100.0 * (test_df['winner_id'] == test_df['elo_prediction']).astype(float).mean(), 1))\n",
    "print(\"Pathprob accuracy: \", round(100.0 * (test_df['winner_id'] == test_df['path_prediction']).astype(float).mean(), 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
