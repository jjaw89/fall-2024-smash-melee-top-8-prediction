{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "#import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from glicko2 import Player\n",
    "import multiprocessing\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SQLite Database into Pandas DataFrames\n",
    "\n",
    "The following code connects to an SQLite database (`melee_player_database.db`) and converts each table within the database into a pandas DataFrame. The DataFrames will be stored in a dictionary, where each key corresponds to the table name with `_df` appended, and the values are the respective DataFrames.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Database Connection**: We use the `sqlite3` library to connect to the SQLite database file.\n",
    "2. **Retrieve Table Names**: A query retrieves all the table names in the database.\n",
    "3. **Convert Tables to DataFrames**: For each table:\n",
    "   - The table is loaded into a pandas DataFrame using `pd.read_sql()`.\n",
    "   - We check each column to see if any data is JSON-formatted (lists or dictionaries). If so, we convert these columns from strings into their corresponding Python objects using `json.loads()`.\n",
    "4. **Store DataFrames**: The DataFrames are stored in a dictionary, where the key is the table name with a `_df` suffix, and the value is the DataFrame.\n",
    "5. **Database Connection Closed**: Once all tables are loaded into DataFrames, the database connection is closed.\n",
    "\n",
    "### Example:\n",
    "If the database contains a table named `players`, the corresponding DataFrame will be stored in the dictionary with the key `players_df`, and can be accessed as:\n",
    "\n",
    "```python\n",
    "players_df = dfs['players_df']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the table names\n",
    "def get_table_names(conn):\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    return pd.read_sql(query, conn)['name'].tolist()\n",
    "\n",
    "# Function to load tables into DataFrames\n",
    "def load_tables_to_dfs(conn):\n",
    "    table_names = get_table_names(conn)\n",
    "    dataframes = {}\n",
    "    \n",
    "    for table in table_names:\n",
    "        # Load table into a DataFrame\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "        \n",
    "        # Detect and convert JSON formatted columns (if any)\n",
    "        for col in df.columns:\n",
    "            # Check if any entry in the column is a valid JSON (list or dictionary)\n",
    "            if df[col].apply(lambda x: isinstance(x, str)).all():\n",
    "                try:\n",
    "                    # Try parsing the column as JSON\n",
    "                    df[col] = df[col].apply(lambda x: json.loads(x) if pd.notnull(x) else x)\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    # If it fails, skip the column\n",
    "                    pass\n",
    "        \n",
    "        # Store the DataFrame with table name + '_df'\n",
    "        dataframes[f\"{table}_df\"] = df\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "if os.path.exists(data_path + 'dfs_dict.pkl'):\n",
    "    cell_has_run = True\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    with open(data_path + 'dfs_dict.pkl', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n",
    "# Check if the flag variable exists in the global scope so that this code does not run twice\n",
    "if 'cell_has_run' not in globals():\n",
    "    path = data_path + \"melee_player_database.db\"\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(path)\n",
    "\n",
    "    # Convert each table into a DataFrame\n",
    "    dfs = load_tables_to_dfs(conn)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Now, you have a dictionary 'dfs' where each key is the table name with '_df' suffix and value is the corresponding DataFrame.\n",
    "    # For example, to access the DataFrame for a table called 'players':\n",
    "    # players_df = dfs['players_df']\n",
    "\n",
    "    dfs['tournament_info_df']['start'] = pd.to_datetime(dfs['tournament_info_df']['start'], unit='s')\n",
    "    dfs['tournament_info_df']['end'] = pd.to_datetime(dfs['tournament_info_df']['end'], unit='s')\n",
    "\n",
    "    \n",
    "    # Set the flag to indicate that the cell has been run\n",
    "    cell_has_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we adjust the data types of the dataframes so that they are the correct type. (This will be updated as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['sets_df']['best_of'] = dfs['sets_df']['best_of'].fillna(0).astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we make dataframes that we will use and print the head.\n",
    "\n",
    "The integers in 'characters' count the number of games the player has played that character. (We verify this for Zain below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = dfs['players_df']\n",
    "players_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df = dfs['ranking_df']\n",
    "ranking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_seasons_df = dfs['ranking_seasons_df']\n",
    "ranking_seasons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_df = dfs['sets_df']\n",
    "print(f\"{sets_df[sets_df['game_data'].apply(lambda x: len(x) > 0)].shape[0] / sets_df.shape[0]:0.01%} percent of sets have some game data)\")\n",
    "\n",
    "sets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_info_df = dfs['tournament_info_df']\n",
    "tournament_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code optimization by Dan\n",
    "# Basically we want to replace this line in process_tournament with something more efficient:\n",
    "#\n",
    "#      tournament_sets_df = sets_df[sets_df['tournament_key'] == tournament_key]\n",
    "#\n",
    "# Instead, we can\n",
    "# - Merge the tournament date info into ``sets_df``\n",
    "# - Sort by date\n",
    "# - Store the start/end positions of each tournament in a separate dictionary\n",
    "# - Use tournament_sets_df = sets_df.iloc[start:end+1] instead.\n",
    "\n",
    "sets_df = sets_df.merge(tournament_info_df[['key', 'start', 'end']], left_on='tournament_key', right_on='key', how='left')\n",
    "sets_df = sets_df.drop(labels=['key_y'], axis='columns')\n",
    "sets_df = sets_df.rename(columns={\"key_x\": \"key\"})\n",
    "sets_df = sets_df.sort_values(by=['end', 'tournament_key']) # Just in case there are tournaments with the exact same end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of data cleanup\n",
    "# TODO: Rerun!\n",
    "min_date = datetime.datetime(2015, 1, 1)\n",
    "max_date = datetime.datetime(2024, 12, 31)\n",
    "\n",
    "sets_df = sets_df[(sets_df['start'] >= min_date) & (sets_df['end'] >= min_date) & (sets_df['start'] <= max_date) & (sets_df['end'] <= max_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we ignore the L{n} location name.\n",
    "top_8_locations = [                                   \n",
    "        ['WSF', 'Winners Semis', 'Winners Semi-Final'],\n",
    "        ['LQF', 'Losers Quarters', 'Losers Quarter-Final'],\n",
    "        ['WF', 'Winners Final', 'Winners Final'],\n",
    "        ['LSF', 'Losers Semis', 'Losers Semi-Final'],\n",
    "        ['LF', 'Losers Final', 'Losers Final'],\n",
    "        ['GF', 'Grand Final', 'Grand Final'],\n",
    "        ['GFR', 'GF Reset', 'Grand Final Reset']\n",
    "    ] \n",
    "\n",
    "top_8 = sets_df['location_names'].isin(top_8_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mini_df = pd.read_pickle(data_path + 'dataset_mini.pkl')\n",
    "\n",
    "# Temporary bugfix, might have added stuff twice at some point\n",
    "# dataset_mini_df = dataset_mini_df.loc[:,~dataset_mini_df.columns.duplicated()].copy()\n",
    "\n",
    "#minier_cols = [x for x in dataset_mini_df.columns if \"m2\" not in x and \"_alt_\" not in x]\n",
    "minier_cols = [x for x in dataset_mini_df.columns if \"m2\" not in x and \"_alt_\" not in x]\n",
    "#minier_cols = [x for x in dataset_mini_df.columns if \"m2\" not in x]\n",
    "dataset_minier_df = dataset_mini_df[minier_cols]\n",
    "\n",
    "dataset_df = dataset_minier_df\n",
    "\n",
    "features_default_elo = ['p1_elo', 'p2_elo']\n",
    "features_all_elo = ['p1_elo', 'p2_elo', 'p1/m1/m1_elo', 'p2/m1/m1_elo', 'p1/m1_elo', 'p2/m1_elo']\n",
    "features_all_rd = [x.replace('elo', 'rd') for x in features_all_elo]\n",
    "features_all_eru = features_all_elo + features_all_rd + [x.replace('elo', 'updates') for x in features_all_elo]\n",
    "features_all_everything = list(dataset_df.columns[:-1])\n",
    "\n",
    "# Filter by elos that actually have nontrivial data\n",
    "quality_filter = pd.Series(True, index=dataset_df.index)\n",
    "for elo_col in features_all_elo:\n",
    "    quality_filter = quality_filter & (dataset_df[elo_col] != 1500.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models that only use the ELO scores\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2016, 2023+1)\n",
    "\n",
    "# These models work best with normally distributed data, which appears to be the case for the various ELOs\n",
    "models = {'lr': LogisticRegression(penalty=None, max_iter=10000),\n",
    "          'lda': LinearDiscriminantAnalysis(),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "# Just using default ELOs\n",
    "for i, y in enumerate(years):\n",
    "    # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "    # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "    dataset_train_df = dataset_df[(sets_df['start'] <= datetime.datetime(2016,1,1)) &\n",
    "                                  (sets_df['end'] <= datetime.datetime(y,12,31))]\n",
    "    dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                 (sets_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "\n",
    "    for j, name in enumerate(models):\n",
    "        models[name].fit(dataset_train_df[features_default_elo], dataset_train_df['winner'])\n",
    "        y_prob = models[name].predict_proba(dataset_test_df[features_default_elo])\n",
    "        y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "        ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "        acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "print(\"Scores involving just the default elos\")\n",
    "print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "             index=years,\n",
    "             columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))\n",
    "\n",
    "# Using all ELOs\n",
    "for i, y in enumerate(years):\n",
    "    # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "    # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "    dataset_train_df = dataset_df[(sets_df['start'] <= datetime.datetime(2016,1,1)) &\n",
    "                                  (sets_df['end'] <= datetime.datetime(y,12,31))]\n",
    "    dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                 (sets_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "\n",
    "    for j, name in enumerate(models):\n",
    "        models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'])\n",
    "        y_prob = models[name].predict_proba(dataset_test_df[features_all_elo])\n",
    "        y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "        ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "        acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "print()\n",
    "print(\"Scores involving all elos\")\n",
    "print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "             index=years,\n",
    "             columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))\n",
    "\n",
    "# Using all ELOs, but with only recent years for training\n",
    "for i, y in enumerate(years):\n",
    "    # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "    # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "    dataset_train_df = dataset_df[(sets_df['start'] <= datetime.datetime(y,1,1)) &\n",
    "                                  (sets_df['end'] <= datetime.datetime(y,12,31))]\n",
    "    dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                 (sets_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "\n",
    "    for j, name in enumerate(models):\n",
    "        models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'])\n",
    "        y_prob = models[name].predict_proba(dataset_test_df[features_all_elo])\n",
    "        y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "        ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "        acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "print()\n",
    "print(\"Scores involving all elos, but only one year for training\")\n",
    "print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "             index=years,\n",
    "             columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))\n",
    "\n",
    "\n",
    "# Using all ELOs, but with only recent years for training\n",
    "# AND with the restriction of only looking at quality data for training\n",
    "# (and testing still makes use of all data)\n",
    "for i, y in enumerate(years):\n",
    "    # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "    # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "    dataset_train_df = dataset_df[(sets_df['start'] <= datetime.datetime(y,1,1)) &\n",
    "                                  (sets_df['end'] <= datetime.datetime(y,12,31)) &\n",
    "                                  quality_filter]\n",
    "    dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                 (sets_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "\n",
    "    for j, name in enumerate(models):\n",
    "        models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'])\n",
    "        y_prob = models[name].predict_proba(dataset_test_df[features_all_elo])\n",
    "        y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "        ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "        acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "print()\n",
    "print(\"Scores involving all elos, but only one year for training\")\n",
    "print(\"AND only using quality data for training\")\n",
    "print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "             index=years,\n",
    "             columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced models\n",
    "import xgboost as xgb\n",
    "import errorlda\n",
    "import importlib\n",
    "\n",
    "# Just in case we make changes to this model.\n",
    "importlib.reload(errorlda)\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2016, 2023+1)\n",
    "\n",
    "models = {'errorlda': errorlda.ErrorLDA(),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "for i, y in enumerate(years):\n",
    "    # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "    # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "    dataset_train_df = dataset_df[(sets_df['start'] <= datetime.datetime(y,1,1)) &\n",
    "                                  (sets_df['end'] <= datetime.datetime(y,12,31)) &\n",
    "                                  quality_filter & top_8]\n",
    "    dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                 (sets_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "\n",
    "    for j, name in enumerate(models):\n",
    "        y_prob = None\n",
    "        y_pred = None\n",
    "\n",
    "        if name == 'errorlda': # Special syntax required here.\n",
    "            models[name] = errorlda.ErrorLDA() # Not sure if I've implemented .fit() to reset everything upon every new fit.\n",
    "\n",
    "            models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'],\n",
    "                             X_train_errors=dataset_train_df[features_all_rd].apply(lambda row: np.diag(row.to_numpy() ** 2), axis=1),\n",
    "                             error_scaling=False) # Note that error-scaling is slower by like a factor of 8, unfortunately\n",
    "            \n",
    "            # Literally just for numerical stability, in case some eigenvalues are near zero.\n",
    "            # This will add 1 to each eigenvalue.\n",
    "            models[name].variance += np.identity(len(features_all_rd))\n",
    "            \n",
    "            y_prob = models[name].predict_proba(dataset_test_df[features_all_elo],\n",
    "                                                X_error=dataset_test_df[features_all_rd].apply(lambda row: np.diag(row.to_numpy() ** 2), axis=1))\n",
    "            \n",
    "            print(\"Trained errorlda, year {0}\".format(y))\n",
    "            \n",
    "        else:\n",
    "            models[name].fit(dataset_train_df[features_all_elo + features_all_rd], dataset_train_df['winner'])\n",
    "            y_prob = models[name].predict_proba(dataset_test_df[features_all_elo + features_all_rd])\n",
    "        \n",
    "        y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "        ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "        acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "print(\"Scores involving all ELOs and RDs\")\n",
    "print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "             index=years,\n",
    "             columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
