{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "#import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from glicko2 import Player\n",
    "import multiprocessing\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SQLite Database into Pandas DataFrames\n",
    "\n",
    "The following code connects to an SQLite database (`melee_player_database.db`) and converts each table within the database into a pandas DataFrame. The DataFrames will be stored in a dictionary, where each key corresponds to the table name with `_df` appended, and the values are the respective DataFrames.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Database Connection**: We use the `sqlite3` library to connect to the SQLite database file.\n",
    "2. **Retrieve Table Names**: A query retrieves all the table names in the database.\n",
    "3. **Convert Tables to DataFrames**: For each table:\n",
    "   - The table is loaded into a pandas DataFrame using `pd.read_sql()`.\n",
    "   - We check each column to see if any data is JSON-formatted (lists or dictionaries). If so, we convert these columns from strings into their corresponding Python objects using `json.loads()`.\n",
    "4. **Store DataFrames**: The DataFrames are stored in a dictionary, where the key is the table name with a `_df` suffix, and the value is the DataFrame.\n",
    "5. **Database Connection Closed**: Once all tables are loaded into DataFrames, the database connection is closed.\n",
    "\n",
    "### Example:\n",
    "If the database contains a table named `players`, the corresponding DataFrame will be stored in the dictionary with the key `players_df`, and can be accessed as:\n",
    "\n",
    "```python\n",
    "players_df = dfs['players_df']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the table names\n",
    "def get_table_names(conn):\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    return pd.read_sql(query, conn)['name'].tolist()\n",
    "\n",
    "# Function to load tables into DataFrames\n",
    "def load_tables_to_dfs(conn):\n",
    "    table_names = get_table_names(conn)\n",
    "    dataframes = {}\n",
    "    \n",
    "    for table in table_names:\n",
    "        # Load table into a DataFrame\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "        \n",
    "        # Detect and convert JSON formatted columns (if any)\n",
    "        for col in df.columns:\n",
    "            # Check if any entry in the column is a valid JSON (list or dictionary)\n",
    "            if df[col].apply(lambda x: isinstance(x, str)).all():\n",
    "                try:\n",
    "                    # Try parsing the column as JSON\n",
    "                    df[col] = df[col].apply(lambda x: json.loads(x) if pd.notnull(x) else x)\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    # If it fails, skip the column\n",
    "                    pass\n",
    "        \n",
    "        # Store the DataFrame with table name + '_df'\n",
    "        dataframes[f\"{table}_df\"] = df\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "if os.path.exists(data_path + 'dfs_dict.pkl'):\n",
    "    cell_has_run = True\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    with open(data_path + 'dfs_dict.pkl', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n",
    "# Check if the flag variable exists in the global scope so that this code does not run twice\n",
    "if 'cell_has_run' not in globals():\n",
    "    path = data_path + \"melee_player_database.db\"\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(path)\n",
    "\n",
    "    # Convert each table into a DataFrame\n",
    "    dfs = load_tables_to_dfs(conn)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Now, you have a dictionary 'dfs' where each key is the table name with '_df' suffix and value is the corresponding DataFrame.\n",
    "    # For example, to access the DataFrame for a table called 'players':\n",
    "    # players_df = dfs['players_df']\n",
    "\n",
    "    dfs['tournament_info_df']['start'] = pd.to_datetime(dfs['tournament_info_df']['start'], unit='s')\n",
    "    dfs['tournament_info_df']['end'] = pd.to_datetime(dfs['tournament_info_df']['end'], unit='s')\n",
    "\n",
    "    \n",
    "    # Set the flag to indicate that the cell has been run\n",
    "    cell_has_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we adjust the data types of the dataframes so that they are the correct type. (This will be updated as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['sets_df']['best_of'] = dfs['sets_df']['best_of'].fillna(0).astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we make dataframes that we will use and print the head.\n",
    "\n",
    "The integers in 'characters' count the number of games the player has played that character. (We verify this for Zain below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = dfs['players_df']\n",
    "players_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df = dfs['ranking_df']\n",
    "ranking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_seasons_df = dfs['ranking_seasons_df']\n",
    "ranking_seasons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_df = dfs['sets_df']\n",
    "print(f\"{sets_df[sets_df['game_data'].apply(lambda x: len(x) > 0)].shape[0] / sets_df.shape[0]:0.01%} percent of sets have some game data)\")\n",
    "\n",
    "sets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_info_df = dfs['tournament_info_df']\n",
    "tournament_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code optimization by Dan\n",
    "# Basically we want to replace this line in process_tournament with something more efficient:\n",
    "#\n",
    "#      tournament_sets_df = sets_df[sets_df['tournament_key'] == tournament_key]\n",
    "#\n",
    "# Instead, we can\n",
    "# - Merge the tournament date info into ``sets_df``\n",
    "# - Sort by date\n",
    "# - Store the start/end positions of each tournament in a separate dictionary\n",
    "# - Use tournament_sets_df = sets_df.iloc[start:end+1] instead.\n",
    "\n",
    "sets_df = sets_df.merge(tournament_info_df[['key', 'start', 'end']], left_on='tournament_key', right_on='key', how='left')\n",
    "sets_df = sets_df.drop(labels=['key_y'], axis='columns')\n",
    "sets_df = sets_df.rename(columns={\"key_x\": \"key\"})\n",
    "sets_df = sets_df.sort_values(by=['end', 'tournament_key']) # Just in case there are tournaments with the exact same end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of data cleanup\n",
    "# TODO: Rerun!\n",
    "min_date = datetime.datetime(2015, 1, 1)\n",
    "max_date = datetime.datetime(2024, 12, 31)\n",
    "\n",
    "sets_df = sets_df[(sets_df['start'] >= min_date) & (sets_df['end'] >= min_date) & (sets_df['start'] <= max_date) & (sets_df['end'] <= max_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we ignore the L{n} location name.\n",
    "top_8_locations = [                                   \n",
    "        ['WSF', 'Winners Semis', 'Winners Semi-Final'],\n",
    "        ['LQF', 'Losers Quarters', 'Losers Quarter-Final'],\n",
    "        ['WF', 'Winners Final', 'Winners Final'],\n",
    "        ['LSF', 'Losers Semis', 'Losers Semi-Final'],\n",
    "        ['LF', 'Losers Final', 'Losers Final'],\n",
    "        ['GF', 'Grand Final', 'Grand Final'],\n",
    "        ['GFR', 'GF Reset', 'Grand Final Reset']\n",
    "    ] \n",
    "\n",
    "top_8 = sets_df['location_names'].isin(top_8_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mini_df = pd.read_pickle(data_path + 'dataset_mini.pkl')\n",
    "\n",
    "# Temporary bugfix, might have added stuff twice at some point\n",
    "# dataset_mini_df = dataset_mini_df.loc[:,~dataset_mini_df.columns.duplicated()].copy()\n",
    "\n",
    "#minier_cols = [x for x in dataset_mini_df.columns if \"m2\" not in x and \"_alt_\" not in x]\n",
    "minier_cols = [x for x in dataset_mini_df.columns if \"m2\" not in x and \"_alt_\" not in x]\n",
    "#minier_cols = [x for x in dataset_mini_df.columns if \"m2\" not in x]\n",
    "dataset_minier_df = dataset_mini_df[minier_cols]\n",
    "\n",
    "dataset_df = dataset_minier_df\n",
    "\n",
    "features_default_elo = ['p1_elo', 'p2_elo']\n",
    "features_all_elo = ['p1_elo', 'p2_elo', 'p1/m1/m1_elo', 'p2/m1/m1_elo', 'p1/m1_elo', 'p2/m1_elo']\n",
    "features_all_rd = [x.replace('elo', 'rd') for x in features_all_elo]\n",
    "features_all_updates = [x.replace('elo', 'updates') for x in features_all_elo]\n",
    "features_all_eru = features_all_elo + features_all_rd + features_all_updates\n",
    "features_all_everything = list(dataset_df.columns[:-1])\n",
    "\n",
    "# Filter by elos that actually have nontrivial data\n",
    "quality_filter = pd.Series(True, index=dataset_df.index)\n",
    "for update_col in features_all_updates:\n",
    "    quality_filter = quality_filter & (dataset_df[update_col] >= 10.0)\n",
    "\n",
    "low_quality_filter = pd.Series(True, index=dataset_df.index)\n",
    "for update_col in features_all_updates:\n",
    "    low_quality_filter = low_quality_filter & ((dataset_df[update_col] >= 2.0) & (dataset_df[update_col] <= 10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An observation about the data\n",
    "\n",
    "First, we note the motivation for including ``quality_filter`` as an option for the data. With it, ELO scores appear to (mostly) follow a multivariate normal distribution, while lower-quality data tends to cluster far more around the default elo values of 1500. This suggests, at least in the case of high-quality data, that very simplistic linear models might actually yield the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elos = ['p1_elo', 'p2_elo', 'p1/m1/m1_elo', 'p2/m1/m1_elo', 'p1/m1_elo', 'p2/m1_elo']\n",
    "\n",
    "plt.title('default, alt2, alt3 ELOs for quality data')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.subplot(3,2,2*i+1)\n",
    "    plt.scatter(dataset_df[quality_filter & (dataset_df['winner'] == 1.0)][all_elos[2*i]],\n",
    "                dataset_df[quality_filter & (dataset_df['winner'] == 1.0)][all_elos[2*i+1]],\n",
    "                s=0.3, alpha=0.2, label='p1 wins')\n",
    "    plt.xlim(0, 3000)\n",
    "    plt.ylim(0, 3000)\n",
    "    \n",
    "    if i != 2:\n",
    "        plt.xticks([])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3,2,2*i+2)\n",
    "    plt.scatter(dataset_df[quality_filter & (dataset_df['winner'] == 0.0)][all_elos[2*i]],\n",
    "                dataset_df[quality_filter & (dataset_df['winner'] == 0.0)][all_elos[2*i+1]],\n",
    "                s=0.3, alpha=0.2, label='p2 wins')\n",
    "    plt.xlim(0, 3000)\n",
    "    plt.ylim(0, 3000)\n",
    "    plt.legend()\n",
    "\n",
    "    if i != 2:\n",
    "        plt.xticks([])\n",
    "\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing some basic models\n",
    "\n",
    "Before we begin, some important remarks are in order. In some sense, this is time series data, and in another sense, it is not. It somewhat showcases the evolution of player ELO ratings (and related features) over time, albeit without directly linking them to certain players. However, these ELO scores, in some sense, already take into account all of the player's past performance up to a certain point. Likewise, RD values are meant to indicate the \"uncertainty\" in any player's current ELO score, and at any one point in time it takes into account all of their previous games, and even how long it has been since they've last played.\n",
    "\n",
    "That being said, it is somewhat ill-advised to shuffle the data when training. Especially for more advanced models, it might potentially be able to recognize that a very precise ELO score has shown up in some future match, and conclude that it must have won some previous matches (note that we have updated ELO scores only once a week).\n",
    "\n",
    "Here, we begin by training some basic models that have the goal of predicting the outcome of individual sets, with no ability to look back on any past performance (single-set models). We first want to observe the impact of the following factors, and are not yet interested in serious hyperparameter tuning:\n",
    "\n",
    "* Is it better to only train on more recent data rather than all data up to a certain point (perhaps game meta, average ELO, etc... shifts over time)\n",
    "* Are we actually gaining anything by including all of our engineered ELO scores, rather than just the default ones?\n",
    "* What is the impact of considering only on \"high quality\" data which has received many updates to all ELO scores?\n",
    "\n",
    "We also note that for this single-set predictor, we train on data up to 2022 (and test on 2023) for cross-validation and hyperparameter tuning. The secondary model which takes tournament performance into account (which will use this single-set model), tuned on 2023 data and have its final performance tested on 2024 data. However, for the interest of obtaining a final performance score for the single-set predictor, it should be safely testable on 2024 data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models that only use the ELO scores\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2017, 2022+1)\n",
    "\n",
    "# These models work best with normally distributed data, which appears to be the case for the various ELOs\n",
    "models = {'lr': LogisticRegression(penalty=None, max_iter=10000),\n",
    "          'lda': LinearDiscriminantAnalysis(),\n",
    "          'qda': QuadraticDiscriminantAnalysis(),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "training_modes = ['past_year', 'all_years'] # Train on just the past year, or all years (starting from 2016).\n",
    "elo_modes = ['default_elo', 'all_elo'] # Only the default glicko2 elo scores, or all engineered ones\n",
    "data_modes = ['quality_data', 'all_data'] # Whether or not each elo has had at least 10 updates\n",
    "\n",
    "for training_mode in training_modes:\n",
    "    for elo_mode in elo_modes:\n",
    "        for data_mode in data_modes:\n",
    "            # Just using default ELOs\n",
    "            for i, y in enumerate(tqdm(years)):\n",
    "                # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "                # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "                dataset_train_df = dataset_df[(sets_df['start'] >= datetime.datetime(2016 if training_mode == 'all_years' else y, 1, 1)) &\n",
    "                                            (sets_df['end'] <= datetime.datetime(y,12,31)) &\n",
    "                                            (quality_filter if data_mode == 'quality_data' else True)]\n",
    "                dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                            (sets_df['end'] <= datetime.datetime(y+1,12,31)) &\n",
    "                                            (quality_filter if data_mode == 'quality_data' else True)]\n",
    "                \n",
    "                for j, name in enumerate(models):\n",
    "                    models[name].fit(dataset_train_df[features_default_elo if elo_mode == 'default_elo' else features_all_elo], dataset_train_df['winner'])\n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_default_elo if elo_mode == 'default_elo' else features_all_elo])\n",
    "                    y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "                    ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "                    acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "            print(\"Using \" + training_mode + \" and \" + elo_mode + \" and \" + data_mode)\n",
    "            print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "                        index=years,\n",
    "                        columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of particular interest are the latter few years, because the data is of substantially higher quality, and it will also more closely follow years 2023 and 2024.\n",
    "\n",
    "Without any need to run any statistical tests, a simple side-by-side comparison reveals that\n",
    "* In all instances, training on more recent data is slightly favourable for both linear models and XGBoost. We will stick with that from now on.\n",
    "* LDA appears favourable over QDA.\n",
    "* When dealing with higher-quality data, basic linear models substantially outperform XGBoost, at least without any hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A closer look at linear models\n",
    "\n",
    "Here, we test the following linear models in the case of high-quality data, and explicitly low-quality data. The models we will be testing are LogisticRegression (classification), LDA, and a custom ErrorLDA model which should take into account the various RD values (roughly interpreted as a measurement error on the player ELOs, with the \"true\" ELOs being unknown values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced models\n",
    "import xgboost as xgb\n",
    "import errorlda\n",
    "import importlib\n",
    "\n",
    "# Just in case we make changes to this model.\n",
    "importlib.reload(errorlda)\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2020, 2022+1)\n",
    "\n",
    "models = {'lr': LogisticRegression(penalty=None, max_iter=10000),\n",
    "          'lda': LinearDiscriminantAnalysis(),\n",
    "          'errorlda': errorlda.ErrorLDA(),\n",
    "          'errorlda_scaling': errorlda.ErrorLDA(),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "data_modes = ['low_quality', 'high_quality']\n",
    "\n",
    "for data_mode in data_modes:\n",
    "    for i, y in enumerate(tqdm(years)):\n",
    "        # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "        # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "        dataset_train_df = dataset_df[(sets_df['start'] >= datetime.datetime(y,1,1)) &\n",
    "                                    (sets_df['end'] <= datetime.datetime(y,12,31)) &\n",
    "                                    (low_quality_filter if data_mode == 'low_quality' else quality_filter)]\n",
    "        dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                    (sets_df['end'] <= datetime.datetime(y+1,12,31)) &\n",
    "                                    (low_quality_filter if data_mode == 'low_quality' else quality_filter)]\n",
    "        \n",
    "        for j, name in enumerate(models):\n",
    "            y_prob = None\n",
    "            y_pred = None\n",
    "\n",
    "            # Basically, all of these require slightly different syntax and restriction of features\n",
    "            # * ErrorLDA to include the variances (RD values)\n",
    "            # * LDA to just use the the ELO values without RD values (mainly to compare to ErrorLDA)\n",
    "            match name:\n",
    "                case 'lr' | 'lda':\n",
    "                    models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'])\n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_all_elo])\n",
    "\n",
    "                case 'errorlda' | 'errorlda_scaling':\n",
    "                    models[name] = errorlda.ErrorLDA() # Not sure if I've implemented .fit() to reset everything upon every new fit.\n",
    "\n",
    "                    # Experimental pre-scaling of the RD values\n",
    "                    pre_scaler = np.diag([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "                    models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'],\n",
    "                                    X_train_errors=dataset_train_df[features_all_rd].apply(lambda row: np.diag(row.to_numpy() ** 2) @ pre_scaler, axis=1),\n",
    "                                    error_scaling=(True if name == 'errorlda_scaling' else False)) # Note that error-scaling is slower by like a factor of 8, unfortunately\n",
    "                    \n",
    "                    # Literally just for numerical stability, in case some eigenvalues are near zero.\n",
    "                    # This will add 1 to each eigenvalue.\n",
    "                    models[name].variance += np.identity(len(features_all_rd))\n",
    "                    \n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_all_elo],\n",
    "                                                        X_error=dataset_test_df[features_all_rd].apply(lambda row: np.diag(row.to_numpy() ** 2) @ pre_scaler, axis=1))\n",
    "\n",
    "                case 'xgb': # Special syntax required here.\n",
    "                    models[name].fit(dataset_train_df[features_all_elo + features_all_rd], dataset_train_df['winner'])\n",
    "                    y_prob = models[name].predict_proba(dataset_test_df[features_all_elo + features_all_rd])\n",
    "\n",
    "            # Rest of the prediction code is the same among all models    \n",
    "            y_pred = (y_prob[:,1] >= 0.5)\n",
    "\n",
    "            ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "            acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "    print(\"Scores involving all ELOs and RDs in \" + data_mode + \" mode\")\n",
    "    print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "                index=years,\n",
    "                columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best linear model\n",
    "\n",
    "At least from the looks of things, it does not appear that there is any noticeable difference in accuracy and log loss with the ErrorLDA model, compared to the other two linear models. As such, considering the enormous lack of speed in training these models, we will just stick with LogisticRegression() in such special cases for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data according to the various amount of updates (\"no data/low quality data/high quality data\")\n",
    "\n",
    "def get_quality_encoding(row):\n",
    "    result = ''\n",
    "\n",
    "    for update_col in features_all_updates:\n",
    "        if row[update_col] >= 5.0:  # High quality\n",
    "            result += '2'\n",
    "        #elif row[update_col] >= 2.0: # Low quality\n",
    "        #    result += '1'\n",
    "        else:                        # None (or basically none)\n",
    "            result += '0'\n",
    "    \n",
    "    return result\n",
    "\n",
    "dataset_df = dataset_df.copy() # Fixes \"copy of a slice\" nonsense\n",
    "dataset_df['quality_class'] = dataset_df.apply(get_quality_encoding, axis=1)\n",
    "\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits off the data into different classes and applies distinct models to each\n",
    "class SplitRegression:\n",
    "    # min_data is the minimum number of occurences of a class to split it off\n",
    "    def __init__(self, min_data=10000):\n",
    "        self.features = None # Basically the list of columns of the training data\n",
    "        self.min_data = min_data\n",
    "\n",
    "        self.large_classes = None\n",
    "        self.small_classes = None\n",
    "        self.zero_classes = None\n",
    "\n",
    "        self.large_models = {} # A dictionary of models, one for each class with lots of data\n",
    "        self.small_model = None # Just bundle together all small classes (except zero) into one group and apply a more complex model to them\n",
    "        self.zero_model = None # Apply another separate model to the \"basically no data\" group\n",
    "\n",
    "    # Figure out what features we actually need to pull from (and ignore the ones that have basically zero data)\n",
    "    def class_to_features(self, c):\n",
    "        return [self.features[int(x)] for x in c if x != '0']\n",
    "    \n",
    "    # Jankiness, but it works. If there is *technically* only one class in whatever we've split off here,\n",
    "    # then we should make sure to add the other class. Ideally with fake data.\n",
    "    def patch_missing_outcome(self, X_train, y_train):\n",
    "        old_len = len(X_train.index)\n",
    "\n",
    "        y_unique = list(y_train.unique()) # Could very well be empty, or just one value. This handles both cases.\n",
    "        for y in [y for y in [0.0, 1.0] if y not in y_unique]:\n",
    "            print(\"PATCHED {0}\".format(y))\n",
    "            X_train = pd.concat([X_train, pd.Series([1500.0] * len(X_train.columns), index=X_train.columns, axis=0)]) # Most canonical fake data\n",
    "            y_train = pd.concat([y_train, pd.Series([y])])\n",
    "\n",
    "        if len(X_train.index) > old_len:\n",
    "            print(len(X_train.index))\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    # Kind of assumes these are all dataframes and series-es\n",
    "    # X_class corresponds to a unique id of the form n_1 ... n_k for the class\n",
    "    # A value of n_i = 0 means we will not actually use data from that class\n",
    "    def fit(self, X, y, X_class):\n",
    "        self.features = list(X.columns)\n",
    "\n",
    "        class_counts = X_class.value_counts()\n",
    "\n",
    "        # The zero classes, treated as separate classes. Realistically, there should only be at most one.\n",
    "        self.zero_classes = [c for c in class_counts.index if self.class_to_features(c) == []]\n",
    "        \n",
    "        self.large_classes = [x for x in class_counts[class_counts >= self.min_data].index if x not in self.zero_classes]\n",
    "        self.small_classes = [x for x in class_counts[class_counts < self.min_data].index if x not in self.zero_classes]\n",
    "\n",
    "        # As we've seen, LogisticRegression does quite well on classes with lots of data.\n",
    "        # We kind of assume each class has slightly different means, hence the need to train separate models,\n",
    "        # and also just skip out entirely on passing it info from features with basically no info.\n",
    "        for c in tqdm(self.large_classes):\n",
    "            class_features = self.class_to_features(c)\n",
    "\n",
    "            self.large_models[c] = LogisticRegression(penalty=None, max_iter=10000)\n",
    "\n",
    "            # Just in case the outcome is constant on this training subset.\n",
    "            X_train, y_train = self.patch_missing_outcome(X[X_class == c][class_features], y[X_class == c])\n",
    "\n",
    "            self.large_models[c].fit(X_train, y_train)\n",
    "\n",
    "        # Now the zero classes (realistically at most one of them) get lumped together and have a single model used.\n",
    "        # Same for the small classes.\n",
    "        # Might as well toss all features in there, just in case it feels like extracting *some* kind of info, somehow.\n",
    "        small_class_filter = X_class.apply(lambda c: c in self.small_classes)\n",
    "        zero_class_filter = X_class.apply(lambda c: c in self.zero_classes)\n",
    "        \n",
    "        # TODO: Tuning of hyperparameters\n",
    "        self.small_model = xgb.XGBClassifier()\n",
    "        self.zero_model = xgb.XGBClassifier()        \n",
    "\n",
    "        # These could technically be empty, or have just one outcome\n",
    "        # Small class\n",
    "        X_train, y_train = self.patch_missing_outcome(X[small_class_filter], y[small_class_filter])\n",
    "        self.small_model.fit(X_train, y_train)\n",
    "        # Zero class\n",
    "        X_train, y_train = self.patch_missing_outcome(X[zero_class_filter], y[zero_class_filter])\n",
    "        self.zero_model.fit(X_train, y_train)\n",
    "\n",
    "    def predict_proba(self, X, X_class):\n",
    "\n",
    "        merged_df = pd.concat([X, X_class], axis=1)\n",
    "        merged_df.columns = list(X.columns) + ['quality_class']\n",
    "\n",
    "        # It is substantially more efficient to figure out which model applies to which row,\n",
    "        # use a groupby(), and feed the entire block of data into the model,\n",
    "        # rather than doing this entire operation row by row.\n",
    "\n",
    "        # Also, this breaks if we try to put the models directly in the dataframe.\n",
    "        # Let's instead just assign them numeric values.\n",
    "        model_list = [self.large_models[c] for c in self.large_models] + [self.zero_model, self.small_model]\n",
    "        model_to_num_dict = {}\n",
    "\n",
    "        for i, model in enumerate(model_list):\n",
    "            model_to_num_dict[model] = i\n",
    "\n",
    "        def assign_model(row):\n",
    "            c = row['quality_class']\n",
    "\n",
    "            if c in self.large_classes:\n",
    "                return model_to_num_dict[self.large_models[c]]\n",
    "            \n",
    "            # Not a large model. Perhaps zero?\n",
    "            if self.class_to_features(c) == []:\n",
    "                return model_to_num_dict[self.zero_model]\n",
    "            \n",
    "            # Only possibility is the \"small\" model. Just lump it in with the rest of the data.\n",
    "            return model_to_num_dict[self.small_model]\n",
    "        \n",
    "        merged_df['model'] = merged_df.apply(assign_model, axis=1)\n",
    "        merged_df['model_copy'] = merged_df['model'] # include_groups=True deprecation nonsense\n",
    "\n",
    "        # Run predict_proba on entire blocks of data that use the same model,\n",
    "        # rather than running it row by row (slow)!\n",
    "        def block_proba(df):\n",
    "            model_num = df.iloc[0]['model_copy']\n",
    "            c = df.iloc[0]['quality_class']\n",
    "            c_features = df.columns[:-2] # Ignore 'quality_class' and 'model_copy' columns\n",
    "\n",
    "            # Large model, need to actually restrict features\n",
    "            if model_num < len(self.large_models):\n",
    "                c_features = self.class_to_features(c)\n",
    "            \n",
    "            model = model_list[model_num]\n",
    "            \n",
    "            return pd.DataFrame(model.predict_proba(df[c_features]), index=df.index)\n",
    "        \n",
    "        \n",
    "        result = merged_df.groupby('model').apply(block_proba, include_groups=False)\n",
    "\n",
    "        # Note that this will have a two-layered index now.\n",
    "        # One for the model number, and one for the original index.\n",
    "        # Let's remove it.\n",
    "        result = result.droplevel(0)\n",
    "\n",
    "        # NOTE: The index is NOT the original order anymore, because of the above! Let's fix that\n",
    "        result = result.loc[X.index]\n",
    "\n",
    "        return result.to_numpy()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced models\n",
    "import xgboost as xgb\n",
    "import errorlda\n",
    "import importlib\n",
    "\n",
    "# Just in case we make changes to this model.\n",
    "importlib.reload(errorlda)\n",
    "\n",
    "# Years to train on. We will test on the next year.\n",
    "years = range(2018, 2022+1)\n",
    "\n",
    "models = {'split': SplitRegression(),\n",
    "          'xgb': xgb.XGBClassifier()}\n",
    "\n",
    "ll_scores = np.zeros(shape=(len(years), len(models)))\n",
    "acc_scores = np.zeros(shape=(len(years), len(models)))\n",
    "\n",
    "\n",
    "\n",
    "for i, y in enumerate(tqdm(years)):\n",
    "    # Note that 2015 data is probably not that good. Elo scores barely started getting accurate.\n",
    "    # NOTE: It is assumed that dataset_df and sets_df share the same rows, only with different engineered features in dataset_df\n",
    "    dataset_train_df = dataset_df[(sets_df['start'] >= datetime.datetime(y,1,1)) &\n",
    "                                (sets_df['end'] <= datetime.datetime(y,12,31))]\n",
    "    dataset_test_df = dataset_df[(sets_df['start'] >= datetime.datetime(y+1,1,1)) &\n",
    "                                (sets_df['end'] <= datetime.datetime(y+1,12,31))]\n",
    "    \n",
    "    for j, name in enumerate(models):\n",
    "        y_prob = None\n",
    "        y_pred = None\n",
    "\n",
    "        # Basically, all of these require slightly different syntax and restriction of features\n",
    "        # * ErrorLDA to include the variances (RD values)\n",
    "        # * LDA to just use the the ELO values without RD values (mainly to compare to ErrorLDA)\n",
    "        match name:\n",
    "            case 'split':\n",
    "                models[name].fit(dataset_train_df[features_all_elo], dataset_train_df['winner'], dataset_train_df['quality_class'])\n",
    "                y_prob = models[name].predict_proba(dataset_test_df[features_all_elo], dataset_test_df['quality_class'])\n",
    "            case 'xgb': # Special syntax required here.\n",
    "                models[name].fit(dataset_train_df[features_all_elo + features_all_rd], dataset_train_df['winner'])\n",
    "                y_prob = models[name].predict_proba(dataset_test_df[features_all_elo + features_all_rd])\n",
    "\n",
    "        # Rest of the prediction code is the same among all models    \n",
    "        y_pred = (y_prob[:,-1] >= 0.5)\n",
    "\n",
    "        ll_scores[i,j] = round(log_loss(dataset_test_df['winner'], y_prob), 3)\n",
    "        acc_scores[i,j] = round(100.0 * accuracy_score(dataset_test_df['winner'], y_pred), 1)\n",
    "\n",
    "print(\"Scores involving all ELOs and RDs in \" + data_mode + \" mode\")\n",
    "print(pd.DataFrame(np.concatenate([ll_scores, acc_scores], axis=1),\n",
    "            index=years,\n",
    "            columns=[x + \"_ll\" for x in models] + [x + \"_acc\" for x in models]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset_df['p1_elo']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
