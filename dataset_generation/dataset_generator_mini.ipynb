{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from glicko2 import Player\n",
    "\n",
    "if os.path.exists('/workspace/data'):\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    data_path = '/workspace/data/'\n",
    "else:\n",
    "    data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SQLite Database into Pandas DataFrames\n",
    "\n",
    "The following code connects to an SQLite database (`melee_player_database.db`) and converts each table within the database into a pandas DataFrame. The DataFrames will be stored in a dictionary, where each key corresponds to the table name with `_df` appended, and the values are the respective DataFrames.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Database Connection**: We use the `sqlite3` library to connect to the SQLite database file.\n",
    "2. **Retrieve Table Names**: A query retrieves all the table names in the database.\n",
    "3. **Convert Tables to DataFrames**: For each table:\n",
    "   - The table is loaded into a pandas DataFrame using `pd.read_sql()`.\n",
    "   - We check each column to see if any data is JSON-formatted (lists or dictionaries). If so, we convert these columns from strings into their corresponding Python objects using `json.loads()`.\n",
    "4. **Store DataFrames**: The DataFrames are stored in a dictionary, where the key is the table name with a `_df` suffix, and the value is the DataFrame.\n",
    "5. **Database Connection Closed**: Once all tables are loaded into DataFrames, the database connection is closed.\n",
    "\n",
    "### Example:\n",
    "If the database contains a table named `players`, the corresponding DataFrame will be stored in the dictionary with the key `players_df`, and can be accessed as:\n",
    "\n",
    "```python\n",
    "players_df = dfs['players_df']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the table names\n",
    "def get_table_names(conn):\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    return pd.read_sql(query, conn)['name'].tolist()\n",
    "\n",
    "# Function to load tables into DataFrames\n",
    "def load_tables_to_dfs(conn):\n",
    "    table_names = get_table_names(conn)\n",
    "    dataframes = {}\n",
    "    \n",
    "    for table in table_names:\n",
    "        # Load table into a DataFrame\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "        \n",
    "        # Detect and convert JSON formatted columns (if any)\n",
    "        for col in df.columns:\n",
    "            # Check if any entry in the column is a valid JSON (list or dictionary)\n",
    "            if df[col].apply(lambda x: isinstance(x, str)).all():\n",
    "                try:\n",
    "                    # Try parsing the column as JSON\n",
    "                    df[col] = df[col].apply(lambda x: json.loads(x) if pd.notnull(x) else x)\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    # If it fails, skip the column\n",
    "                    pass\n",
    "        \n",
    "        # Store the DataFrame with table name + '_df'\n",
    "        dataframes[f\"{table}_df\"] = df\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "if os.path.exists(data_path + 'dfs_dict.pkl'):\n",
    "    cell_has_run = True\n",
    "    # Load the dictionary of DataFrames from the pickle\n",
    "    with open(data_path + 'dfs_dict.pkl', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n",
    "# Check if the flag variable exists in the global scope so that this code does not run twice\n",
    "if 'cell_has_run' not in globals():\n",
    "    path = data_path + \"melee_player_database.db\"\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(path)\n",
    "\n",
    "    # Convert each table into a DataFrame\n",
    "    dfs = load_tables_to_dfs(conn)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Now, you have a dictionary 'dfs' where each key is the table name with '_df' suffix and value is the corresponding DataFrame.\n",
    "    # For example, to access the DataFrame for a table called 'players':\n",
    "    # players_df = dfs['players_df']\n",
    "\n",
    "    dfs['tournament_info_df']['start'] = pd.to_datetime(dfs['tournament_info_df']['start'], unit='s')\n",
    "    dfs['tournament_info_df']['end'] = pd.to_datetime(dfs['tournament_info_df']['end'], unit='s')\n",
    "\n",
    "    \n",
    "    # Set the flag to indicate that the cell has been run\n",
    "    cell_has_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we adjust the data types of the dataframes so that they are the correct type. (This will be updated as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['sets_df']['best_of'] = dfs['sets_df']['best_of'].fillna(0).astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we make dataframes that we will use and print the head.\n",
    "\n",
    "The integers in 'characters' count the number of games the player has played that character. (We verify this for Zain below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = dfs['players_df']\n",
    "players_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df = dfs['ranking_df']\n",
    "ranking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_seasons_df = dfs['ranking_seasons_df']\n",
    "ranking_seasons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_df = dfs['sets_df']\n",
    "print(f\"{sets_df[sets_df['game_data'].apply(lambda x: len(x) > 0)].shape[0] / sets_df.shape[0]:0.01%} percent of sets have some game data)\")\n",
    "\n",
    "sets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_info_df = dfs['tournament_info_df']\n",
    "tournament_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code optimization by Dan\n",
    "# Basically we want to replace this line in process_tournament with something more efficient:\n",
    "#\n",
    "#      tournament_sets_df = sets_df[sets_df['tournament_key'] == tournament_key]\n",
    "#\n",
    "# Instead, we can\n",
    "# - Merge the tournament date info into ``sets_df``\n",
    "# - Sort by date\n",
    "# - Store the start/end positions of each tournament in a separate dictionary\n",
    "# - Use tournament_sets_df = sets_df.iloc[start:end+1] instead.\n",
    "\n",
    "sets_df = sets_df.merge(tournament_info_df[['key', 'start', 'end']], left_on='tournament_key', right_on='key', how='left')\n",
    "sets_df = sets_df.drop(labels=['key_y'], axis='columns')\n",
    "sets_df = sets_df.rename(columns={\"key_x\": \"key\"})\n",
    "sets_df = sets_df.sort_values(by=['end', 'tournament_key']) # Just in case there are tournaments with the exact same end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of data cleanup\n",
    "min_date = datetime.datetime(2015, 1, 1)\n",
    "max_date = datetime.datetime(2024, 12, 31)\n",
    "\n",
    "sets_df = sets_df[(sets_df['start'] >= min_date) & (sets_df['end'] >= min_date) & (sets_df['start'] <= max_date) & (sets_df['end'] <= max_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to this list evey time you add features!\n",
    "# For convenience, it is blank here, and you should append in each feature-adding cell\n",
    "features = []\n",
    "output = 'winner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_df[output] = sets_df.apply(lambda row: 1.0 if row['winner_id'] == row['p1_id'] else 0.0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Default\" ELO/RD features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall ELO/RD as features\n",
    "player_ratings_df = pd.read_pickle(data_path + 'overall_players_ranking_new_weekly.pkl')\n",
    "player_rds_df = pd.read_pickle(data_path + 'overall_players_rds_new_weekly.pkl')\n",
    "\n",
    "# Might as well compute a (mostly accurate) number of updates from the ratings dataframe\n",
    "# This isn't 100% accurate, as sometimes (especially if default elo playing default elo)\n",
    "# the elo might not change from round to round.\n",
    "player_updates_df = pd.DataFrame(0, index=player_ratings_df.index, columns=player_ratings_df.columns)\n",
    "\n",
    "for i in range(1, len(player_ratings_df.index)):\n",
    "    row_updates = (player_ratings_df.iloc[i] != player_ratings_df.iloc[i-1]).astype(int)\n",
    "    player_updates_df.iloc[i] = row_updates\n",
    "\n",
    "player_updates_df = player_updates_df.cumsum()\n",
    "\n",
    "\n",
    "features_elo_rd = ['p1_elo', 'p2_elo', 'p1_rd', 'p2_rd', 'p1_updates', 'p2_updates']\n",
    "dates = list(player_ratings_df.index)\n",
    "\n",
    "# Efficiency purposes. We are assuming that constant intervals are used.\n",
    "initial_date = dates[0]\n",
    "interval = dates[1] - dates[0]\n",
    "\n",
    "# start_date is the start date of the specific set (tournament)\n",
    "def get_info(df, player_id, start_date):\n",
    "    # Far more efficient than searching the index manually every time\n",
    "    week_num = int((start_date - initial_date) / interval)\n",
    "    newest_date = initial_date + week_num * interval\n",
    "    if dates[-1] < newest_date:\n",
    "        newest_date = dates[-1]\n",
    "    return df.loc[newest_date, player_id]\n",
    "\n",
    "sets_df['p1_elo'] = sets_df.apply(lambda row: get_info(player_ratings_df, row['p1_id'], row['start']), axis=1)\n",
    "sets_df['p2_elo'] = sets_df.apply(lambda row: get_info(player_ratings_df, row['p2_id'], row['start']), axis=1)\n",
    "\n",
    "sets_df['p1_rd'] = sets_df.apply(lambda row: get_info(player_rds_df, row['p1_id'], row['start']), axis=1)\n",
    "sets_df['p2_rd'] = sets_df.apply(lambda row: get_info(player_rds_df, row['p2_id'], row['start']), axis=1)\n",
    "\n",
    "sets_df['p1_updates'] = sets_df.apply(lambda row: get_info(player_updates_df, row['p1_id'], row['start']), axis=1)\n",
    "sets_df['p2_updates'] = sets_df.apply(lambda row: get_info(player_updates_df, row['p2_id'], row['start']), axis=1)\n",
    "\n",
    "# Might as well clear if these aren't needed. They are huge in memory.\n",
    "del player_ratings_df\n",
    "del player_rds_df\n",
    "del player_updates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of times each player has played each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_date = datetime.datetime(2015,1,1)\n",
    "interval = datetime.timedelta(weeks=1)\n",
    "\n",
    "end_date = initial_date + math.ceil((sets_df['end'].max() - initial_date) / interval) * interval\n",
    "\n",
    "# Assumes game_data_extractor.ipynb was run\n",
    "game_data = pd.read_pickle(data_path + 'individual_game_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually get a list of characters and their individual popularities\n",
    "# In the end, a series with index as the character and value as the number of times played.\n",
    "# Probably sorted in the end.\n",
    "all_characters = pd.concat([game_data['winner_char'], game_data['loser_char']], ignore_index=True)\n",
    "all_characters = all_characters.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Player/character combos\n",
    "game_data['winner_pc'] = game_data.apply(lambda row: row['winner_id'] + '/' + row['winner_char'], axis=1)\n",
    "game_data['loser_pc'] = game_data.apply(lambda row: row['loser_id'] + '/' + row['loser_char'], axis=1)\n",
    "\n",
    "# What week number this is, for example.\n",
    "game_data['end_index'] = game_data['end'].apply(lambda x: math.ceil((x - initial_date) / interval))\n",
    "game_data['end_date'] = game_data['end_index'].apply(lambda x: initial_date + x*interval)\n",
    "game_data['end_date_copy'] = game_data['end_index'].apply(lambda x: initial_date + x*interval) # To deal with include_groups=True deprecation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually compute the dates (index) without loading a dataframe for reference.\n",
    "dates = []\n",
    "\n",
    "date = initial_date\n",
    "while date <= end_date:\n",
    "    dates.append(date)\n",
    "    date += interval\n",
    "\n",
    "pc_combos = list(set(list(game_data['winner_pc']) + list(game_data['loser_pc'])))\n",
    "\n",
    "# Count the number of times a character has been used.\n",
    "# Columns will be player/character (or rather, player/melee/character)\n",
    "# Initialize everything to zero, initially.\n",
    "character_usage_df = pd.DataFrame(0, index=dates, columns=pc_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_row(df):\n",
    "    date = df.iloc[0]['end_date_copy']\n",
    "\n",
    "    pc_series = pd.concat([df['winner_pc'], df['loser_pc']], ignore_index=True)\n",
    "\n",
    "    # Deprecation-proofing\n",
    "    temp = character_usage_df.loc[date].copy()\n",
    "    temp.update(pc_series.value_counts())\n",
    "    \n",
    "    character_usage_df.loc[date] = temp\n",
    "    \n",
    "game_data.groupby('end_date').apply(update_row, include_groups=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_usage_df = character_usage_df.cumsum()\n",
    "character_usage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newest possible date to get data from.\n",
    "sets_df['start_index'] = sets_df['start'].apply(lambda x: int((x - initial_date) / interval))\n",
    "sets_df['start_date'] = sets_df['start_index'].apply(lambda x: initial_date + x * interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No point in recomputing every single time.\n",
    "p1c_final_index = ['p1' + '_' + x + '_count' for x in all_characters.index]\n",
    "p2c_final_index = ['p2' + '_' + x + '_count' for x in all_characters.index]\n",
    "\n",
    "features_pc = p1c_final_index + p2c_final_index\n",
    "\n",
    "def get_char_count(row):\n",
    "    # We'll rename the index entries later for these.\n",
    "    p1c_series = pd.Series(0, index=[row['p1_id'] + '/' + x for x in all_characters.index])\n",
    "    p2c_series = pd.Series(0, index=[row['p2_id'] + '/' + x for x in all_characters.index])\n",
    "\n",
    "    p1c_series.update(character_usage_df.loc[row['start_date']])\n",
    "    p2c_series.update(character_usage_df.loc[row['start_date']])\n",
    "\n",
    "    p1c_series.index = p1c_final_index\n",
    "    p2c_series.index = p2c_final_index\n",
    "\n",
    "    return pd.concat([p1c_series, p2c_series])\n",
    "\n",
    "sets_df = pd.concat([sets_df, sets_df.apply(get_char_count, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute most-used characters for each individual player (mains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_char_usage_df = sets_df[p1c_final_index]\n",
    "p2_char_usage_df = sets_df[p2c_final_index]\n",
    "\n",
    "p1_char_usage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mains(row, player):\n",
    "    # Compute three mains and their usages\n",
    "    index = ['m1', 'm2', 'm1_usage', 'm2_usage']\n",
    "    index = [player + '_' + x for x in index]\n",
    "\n",
    "    row_sorted = row.sort_values(ascending=False)\n",
    "\n",
    "    mains = []\n",
    "\n",
    "    # First two entries are the actual mains\n",
    "    for i in range(0,2):\n",
    "        mains.append(row_sorted.index[i].removeprefix(player + '_').removesuffix('_count'))\n",
    "\n",
    "    # Next two are how many times they have been played\n",
    "    for i in range(0,2):\n",
    "        mains.append(row_sorted.iloc[i])\n",
    "\n",
    "    return pd.Series(mains, index=index)\n",
    "\n",
    "\n",
    "p1_mains_df = p1_char_usage_df.apply(lambda row: get_mains(row, 'p1'), axis=1)\n",
    "p2_mains_df = p2_char_usage_df.apply(lambda row: get_mains(row, 'p2'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary workspace for computing individual matchup elos\n",
    "player_mains_df = pd.concat([sets_df[['p1_id', 'p2_id', 'start']], p1_mains_df, p2_mains_df], axis=1)\n",
    "\n",
    "# TODO: Compute this properly!\n",
    "start_date = datetime.datetime(2015,1,1)\n",
    "interval = datetime.timedelta(weeks=1)\n",
    "\n",
    "# Newest possible date to pull matchup info from\n",
    "player_mains_df['start_index'] = player_mains_df['start'].apply(lambda x: int((x - start_date) / interval))\n",
    "player_mains_df['newest_date'] = player_mains_df['start_index'].apply(lambda x: start_date + x*interval)\n",
    "\n",
    "player_mains_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mains = ['p1_m1_usage', 'p1_m2_usage', 'p2_m1_usage', 'p2_m2_usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## player/char/char elos and updates, alt2 variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt2_df = pd.read_pickle(data_path + 'char_vs_char_player_rankings_weekly_alt2.pkl')\n",
    "\n",
    "# How many times each column was updated so far\n",
    "# Keep everything float to not change the data type\n",
    "alt2_updates_df = alt2_df.copy()\n",
    "alt2_updates_df.iloc[0] = 0.0\n",
    "alt2_updates_df.iloc[1:] = (alt2_updates_df.iloc[1:].reset_index(drop=True) != alt2_updates_df.iloc[:-1].reset_index(drop=True)).astype(float).values\n",
    "alt2_updates_df = alt2_updates_df.cumsum()\n",
    "\n",
    "start_date = alt2_df.index[0]\n",
    "interval = alt2_df.index[1] - alt2_df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pull number of times this specific matchup happened as well?\n",
    "\n",
    "matchups = []\n",
    "\n",
    "# Compute the actual matchups first.\n",
    "# Then obtain the elos.\n",
    "\n",
    "for i in [1,2]: # player_i\n",
    "    for j in [1,2]: # main_j\n",
    "        for k in [1,2]: # opponent_main_k\n",
    "            player_num = i\n",
    "            opponent_num = 3-i # swap 1 and 2\n",
    "\n",
    "            player = player_mains_df['p' + str(player_num) + '_id']\n",
    "            player_main = player_mains_df['p' + str(player_num) + '_m' + str(j)]\n",
    "            opponent_main = player_mains_df['p' + str(opponent_num) + '_m' + str(k)]\n",
    "\n",
    "            col_name = 'p' + str(player_num) + '/m' + str(j) + '/m' + str(k)\n",
    "\n",
    "            matchups.append(col_name)\n",
    "            player_mains_df[col_name] = player + '/' + player_main + '/' + opponent_main\n",
    "\n",
    "player_mains_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell in particular likes to crash if you interrupt it.\n",
    "#       Do NOT interrupt it.\n",
    "\n",
    "def get_entry(row, matchup, df, default_value):\n",
    "    if row[matchup] not in df.columns:\n",
    "        return default_value\n",
    "    else:\n",
    "        return df.at[row['newest_date'], row[matchup]]\n",
    "\n",
    "# p1/m1/m1, etc...\n",
    "for matchup in tqdm.tqdm(matchups):\n",
    "    player_mains_df[matchup + '_elo'] = player_mains_df[['newest_date', matchup]].apply(lambda row: get_entry(row, matchup, alt2_df, 1500.0), axis=1)\n",
    "    features_mains.append(matchup + '_elo')\n",
    "\n",
    "    player_mains_df[matchup + '_updates'] = player_mains_df[['newest_date', matchup]].apply(lambda row: get_entry(row, matchup, alt2_updates_df, 0.0), axis=1)\n",
    "    features_mains.append(matchup + '_updates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save memory\n",
    "del alt2_df\n",
    "del alt2_updates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_mains_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## player/char/char elos and updates, alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = pd.read_pickle(data_path + 'char_vs_char_player_rankings_weekly_alt.pkl')\n",
    "\n",
    "# How many times each column was updated so far\n",
    "# Keep everything float to not change the data type\n",
    "alt_updates_df = alt_df.copy()\n",
    "alt_updates_df.iloc[0] = 0.0\n",
    "alt_updates_df.iloc[1:] = (alt_updates_df.iloc[1:].reset_index(drop=True) != alt_updates_df.iloc[:-1].reset_index(drop=True)).astype(float).values\n",
    "alt_updates_df = alt_updates_df.cumsum()\n",
    "\n",
    "start_date = alt_df.index[0]\n",
    "interval = alt_df.index[1] - alt_df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pull number of times this specific matchup happened as well?\n",
    "\n",
    "matchups = []\n",
    "reference_matchups = [] # player/same_char/same_char to compare against\n",
    "\n",
    "# Compute the actual matchups first.\n",
    "# Then obtain the elos.\n",
    "\n",
    "for i in [1,2]: # player_i\n",
    "    for j in [1,2]: # main_j\n",
    "        for k in [1,2]: # opponent_main_k\n",
    "            player_num = i\n",
    "            opponent_num = 3-i # swap 1 and 2\n",
    "\n",
    "            player = player_mains_df['p' + str(player_num) + '_id']\n",
    "            player_main = player_mains_df['p' + str(player_num) + '_m' + str(j)]\n",
    "            opponent_main = player_mains_df['p' + str(opponent_num) + '_m' + str(k)]\n",
    "\n",
    "            col_name = 'p' + str(player_num) + '/m' + str(j) + '/m' + str(k)\n",
    "\n",
    "            matchups.append(col_name)\n",
    "            player_mains_df[col_name] = player + '/' + player_main + '/' + opponent_main\n",
    "\n",
    "# opponent/omain/omain as a reference for player/pmain/omain\n",
    "for i in [1,2]: # player_i\n",
    "    for j in [1,2]: # main_j\n",
    "        opponent_num = 3-i # swap 1 and 2\n",
    "\n",
    "        opponent = player_mains_df['p' + str(opponent_num) + '_id']\n",
    "        opponent_main = player_mains_df['p' + str(opponent_num) + '_m' + str(k)]\n",
    "\n",
    "        col_name = 'p' + str(opponent_num) + '/m' + str(j) + '/m' + str(j)\n",
    "        \n",
    "        reference_matchups.append(col_name)\n",
    "        player_mains_df[col_name + '_ref'] = opponent + '/' + opponent_main + '/' + opponent_main \n",
    "\n",
    "player_mains_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell in particular likes to crash if you interrupt it.\n",
    "#       Do NOT interrupt it.\n",
    "\n",
    "def get_entry(row, matchup, df, default_value):\n",
    "    if row[matchup] not in df.columns:\n",
    "        return default_value\n",
    "    else:\n",
    "        return df.at[row['newest_date'], row[matchup]]\n",
    "\n",
    "# p1/m1/m1, etc...\n",
    "for matchup in tqdm.tqdm(matchups):\n",
    "    player_mains_df[matchup + '_alt_elo'] = player_mains_df[['newest_date', matchup]].apply(lambda row: get_entry(row, matchup, alt_df, 1500.0), axis=1)\n",
    "    features_mains.append(matchup + '_alt_elo')\n",
    "\n",
    "    player_mains_df[matchup + '_alt_updates'] = player_mains_df[['newest_date', matchup]].apply(lambda row: get_entry(row, matchup, alt_updates_df, 0.0), axis=1)\n",
    "    features_mains.append(matchup + '_alt_updates')\n",
    "\n",
    "# p1/same_m1/same_m1, etc...\n",
    "for reference_matchup in tqdm.tqdm(reference_matchups):\n",
    "    player_mains_df[reference_matchup + '_alt_ref_elo'] = player_mains_df[['newest_date', reference_matchup + '_ref']].apply(lambda row: get_entry(row, reference_matchup + '_ref', alt_df, 1500.0), axis=1)\n",
    "    features_mains.append(reference_matchup + '_alt_ref_elo')\n",
    "\n",
    "    player_mains_df[reference_matchup + '_alt_ref_updates'] = player_mains_df[['newest_date', reference_matchup + '_ref']].apply(lambda row: get_entry(row, reference_matchup + '_ref', alt_updates_df, 0.0), axis=1)\n",
    "    features_mains.append(reference_matchup + '_alt_ref_updates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save memory\n",
    "del alt_df\n",
    "del alt_updates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_mains_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## player/char \"global\" elos (also called \"alt3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_rankings_df = pd.read_pickle(data_path + 'player_char_overall_rankings_weekly.pkl')\n",
    "pc_rds_df = pd.read_pickle(data_path + 'player_char_overall_rds_weekly.pkl')\n",
    "\n",
    "pc_rankings_updates_df = pc_rankings_df.copy()\n",
    "pc_rankings_updates_df.iloc[0] = 0.0\n",
    "pc_rankings_updates_df.iloc[1:] = (pc_rankings_df.iloc[1:].reset_index(drop=True) != pc_rankings_df.iloc[:-1].reset_index(drop=True)).astype(float).values\n",
    "pc_rankings_updates_df = pc_rankings_updates_df.cumsum()\n",
    "\n",
    "pc_rankings_updates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_combos = []\n",
    "\n",
    "# Compute the actual matchups first.\n",
    "# Then obtain the elos.\n",
    "\n",
    "for i in [1,2]: # player_i\n",
    "    for j in [1,2]: # main_j\n",
    "        player_num = i\n",
    "\n",
    "        player = player_mains_df['p' + str(player_num) + '_id']\n",
    "        player_main = player_mains_df['p' + str(player_num) + '_m' + str(j)]\n",
    "\n",
    "        col_name = 'p' + str(player_num) + '/m' + str(j)\n",
    "\n",
    "        pc_combos.append(col_name)\n",
    "        player_mains_df[col_name] = player + '/' + player_main\n",
    "\n",
    "player_mains_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry(row, matchup, df, default_value):\n",
    "    if row[matchup] not in df.columns:\n",
    "        return default_value\n",
    "    else:\n",
    "        return df.at[row['newest_date'], row[matchup]]\n",
    "\n",
    "# p1/m1/m1, etc...\n",
    "for pc_combo in tqdm.tqdm(pc_combos):\n",
    "    player_mains_df[pc_combo + '_elo'] = player_mains_df[['newest_date', pc_combo]].apply(lambda row: get_entry(row, pc_combo, pc_rankings_df, 1500.0), axis=1)\n",
    "    features_mains.append(pc_combo + '_elo')\n",
    "\n",
    "    player_mains_df[pc_combo + '_rd'] = player_mains_df[['newest_date', pc_combo]].apply(lambda row: get_entry(row, pc_combo, pc_rds_df, 350.0), axis=1)\n",
    "    features_mains.append(pc_combo + '_rd')\n",
    "\n",
    "    player_mains_df[pc_combo + '_updates'] = player_mains_df[['newest_date', pc_combo]].apply(lambda row: get_entry(row, pc_combo, pc_rankings_updates_df, 0.0), axis=1)\n",
    "    features_mains.append(pc_combo + '_updates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_mains_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save memory\n",
    "del pc_rankings_df\n",
    "del pc_rankings_updates_df\n",
    "del pc_rds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate final dataset from above features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.concat([sets_df[features_elo_rd], player_mains_df[features_mains], sets_df[[output]]], axis=1)\n",
    "dataset_df = dataset_df.astype(float)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.to_pickle(data_path + 'dataset_mini.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
